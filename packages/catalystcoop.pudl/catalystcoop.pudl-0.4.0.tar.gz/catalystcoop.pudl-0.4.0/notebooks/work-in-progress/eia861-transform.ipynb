{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catalyst Cooperative Jupyter Notebook Template\n",
    "This notebook lays out a standard format and some best practices for creating interactive / exploratory notebooks which can be relatively easily shared between different PUDL users, and turned into reusable Python modules for integration into our underlying Python packages.\n",
    "\n",
    "## Begin with a narrative outline\n",
    "Each notebook should start with a brief explanation (in Markdown) explaining the purpose of the analysis, and outlining the different stages / steps which are taken to accomplish the analysis.\n",
    "As the analysis develops, you can add new sections or details to this section.\n",
    "\n",
    "## Notebooks should be runnable\n",
    "Insofar as it's possible, another PUDL user who has cloned the repository that the notebook is part of should be able to update their `pudl-dev` conda environment, open the notebook, and run all cells successfully.\n",
    "If there are required data or other prerequisites that the notebook cannot manage on its own -- like a file that needs to be downloaded by hand and placed in a particular location -- those steps should be laid out clearly at the beginning of the notebook.\n",
    "\n",
    "## Avoid Troublesome Elements\n",
    "\n",
    "### Don't hardcode passwords or access tokens\n",
    "Most of our work is done in public Github repositories.\n",
    "No authentication information should ever appear in a notebook.\n",
    "These values can be stored in environment variables on your local computer.\n",
    "\n",
    "### Do not hardocde values, especially late in the notebook\n",
    "If the analysis depends on particular choices of input values, those should be called out explicitly at the beginning of the notebook.\n",
    "(NB: We should explore ways to parameterize notebooks, [papermill](https://papermill.readthedocs.io/en/latest/) is one tool that does this).\n",
    "\n",
    "### Don't hardcode absolute file paths\n",
    "If anyone is going to be able to use the notebook, the files it uses will need to be stored somewhere that makes sense on both your and other computers.\n",
    "We assume that anyone using this template has the PUDL package installed, and has a local PUDL data management environment set up.\n",
    "  * Input data that needs to be stored on disk and accessed via a shared location should be saved under `<PUDL_IN>/data/local/<data_source>/`.\n",
    "  * Intermediate saved data products (e.g. a pickled result of a computationally intensive process) and results should be saved to a location relative to the notebook, and within the directory hierarchy of the repository that the notebook is part of.\n",
    "  \n",
    "### Don't require avoidable long-running computations\n",
    "Consider persisting to disk the results of computations that take more than a few minutes, if the outputs are small enough to be checked into the repository and shared with other users.\n",
    "Only require the expensive computation to be run if this pre-computed output is not available.\n",
    "\n",
    "### Don't litter\n",
    "Don't leave lots of additional code laying around, even commented out, \"just in case\" you want to look at it again later.\n",
    "Notebooks need to be relatively linear in the end, even though the thought processes and exploratory analyses that generate them may not be.\n",
    "Once you have a working analysis, either prune those branches, or encapsulate them as options within functions.\n",
    "\n",
    "### Don't load unneccesary libraries\n",
    "Only import libraries which are required by the notebook, to avoid unnecessary dependencies.\n",
    "If your analysis requires a new library that isn't yet part of the shared `pudl-dev` environment, add it to the `devtools/environment.yml` file so that others will get it when they update their environment.\n",
    "\n",
    "## Related Resources:\n",
    "Lots of these guidelines are taken directly from Emily Riederer's post: [RMarkdown Driven Development](https://emilyriederer.netlify.app/post/rmarkdown-driven-development/).\n",
    "For more in depth explanation of the motivations behind this layout, do go check it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "* Because it's very likely that you will be editing the PUDL Python packages or your own local module under development while working in the notebook, use the %autoreload magic with autoreload level 2 to ensure that any changes you've made in those files are always reflected in the code that's running in the notebook.\n",
    "* Put all import statements at the top of the notebook, so everyone can see what its dependencies are up front, and so that if an import is going to fail, it fails early, before the rest of the notebook is run.\n",
    "* Try to avoid importing individual functions and classes from deep within packages, as it may not be clear to other users where those elements came from, later in the notebook, and also to minimize the chance of namespace collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "# 3rd party libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sqlalchemy as sa\n",
    "from functools import reduce\n",
    "\n",
    "# Local libraries\n",
    "import pudl\n",
    "import pudl.constants as pc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure Display Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.figsize'] = (10,4)\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Python Logging facilities\n",
    "* Using a logger from the beginning will make the transition into the PUDL package easier.\n",
    "* Creating a logging handler here will also allow you to see the logging output coming from PUDL and other underlying packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger=logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "handler = logging.StreamHandler(stream=sys.stdout)\n",
    "formatter = logging.Formatter('%(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.handlers = [handler]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions\n",
    "In many cases, the eventual product of a notebook analysis is going to be the creation of new, reusable functions that are integrated into the underlying PUDL code. You should begin the process of accumulating and organizing those functions as soon as you notice repeated patterns in your code.\n",
    "* Functions should be used to encapsulate any potentially reusable code.\n",
    "* Functions should be defined immediately after the imports, to avoid accidental dependence on zombie variables that are defined further down in the code\n",
    "* While they may evolve over time, having brief docstrings explaining what they do will help others understand them.\n",
    "* If there's a particular type of plot or visualization that is made repeatedly in the notebook, it's good to parameterize and functionalize it here too, so that as you refine the presentation of the data and results, those improvements can be made in a single place, and shown uniformly throughout the notebook.\n",
    "* As these functions mature and become more general purpose tools, you will probably want to start migrating them into their own local module, under a `src` directory in the same directory as the notebook. You will want to import this module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "# pudl_in = pathlib.Path(pudl_settings['pudl_in'])\n",
    "# ds = pudl.workspace.datastore.Datastore(pudl_in, sandbox=True)\n",
    "# eia861_raw_dfs = pudl.extract.eia861.Extractor(ds).extract([2019])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy EIA 861 ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_etl_eia(eia_inputs, pudl_settings):\n",
    "    \"\"\"\n",
    "    This is a dummy function that runs the first part of the EIA ETL\n",
    "    process -- everything up until the entity harvesting begins. For\n",
    "    use in this notebook only.\n",
    "\n",
    "    \"\"\"\n",
    "    eia860_tables = eia_inputs[\"eia860_tables\"]\n",
    "    eia860_years = eia_inputs[\"eia860_years\"]\n",
    "    eia861_tables = eia_inputs[\"eia861_tables\"]\n",
    "    eia861_years = eia_inputs[\"eia861_years\"]\n",
    "    eia923_tables = eia_inputs[\"eia923_tables\"]\n",
    "    eia923_years = eia_inputs[\"eia923_years\"]\n",
    "\n",
    "    # generate CSVs for the static EIA tables, return the list of tables\n",
    "    #static_tables = _load_static_tables_eia(datapkg_dir)\n",
    "    \n",
    "    # For new Datastore args\n",
    "    pudl_in = pathlib.Path(pudl_settings['pudl_in'])\n",
    "    ds = pudl.workspace.datastore.Datastore(pudl_in, sandbox=True)\n",
    "    \n",
    "    # Extract EIA forms 923, 860\n",
    "    eia860_raw_dfs = pudl.extract.eia860.Extractor(ds).extract(eia860_years)\n",
    "    eia861_raw_dfs = pudl.extract.eia861.Extractor(ds).extract(eia861_years)\n",
    "    eia923_raw_dfs = pudl.extract.eia923.Extractor(ds).extract(eia923_years)\n",
    "\n",
    "    # Transform EIA forms 860, 861, 923\n",
    "    eia860_transformed_dfs = pudl.transform.eia860.transform(eia860_raw_dfs, eia860_tables=eia860_tables)\n",
    "    eia861_transformed_dfs = pudl.transform.eia861.transform(eia861_raw_dfs, eia861_tables=eia861_tables)\n",
    "    eia923_transformed_dfs = pudl.transform.eia923.transform(eia923_raw_dfs, eia923_tables=eia923_tables)\n",
    "\n",
    "    # create an eia transformed dfs dictionary\n",
    "    eia_transformed_dfs = eia860_transformed_dfs.copy()\n",
    "    eia_transformed_dfs.update(eia861_transformed_dfs.copy())\n",
    "    eia_transformed_dfs.update(eia923_transformed_dfs.copy())\n",
    "\n",
    "    # convert types..\n",
    "    eia_transformed_dfs = pudl.helpers.convert_dfs_dict_dtypes(eia_transformed_dfs, 'eia')\n",
    "\n",
    "    return eia860_raw_dfs, eia861_raw_dfs, eia923_raw_dfs, eia_transformed_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Notebook Parameters\n",
    "If there are overarching parameters which determine the nature of the analysis -- which US states to look at, which utilities are of interest, a particular start and end date -- state those clearly at the beginning of the analysis, so that they can be referred to by the rest of the notebook and easily changed if need be.\n",
    "* If the analysis depends on local (non-PUDL managed) datasets, define the paths to those data here.\n",
    "* If there are external URLs or other resource locations that will be accessed, define those here as well.\n",
    "* This is also where you should create your `pudl_settings` dictionary and connections to your local PUDL databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pudl_in': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR',\n",
       " 'data_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/data',\n",
       " 'settings_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/settings',\n",
       " 'pudl_out': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR',\n",
       " 'sqlite_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite',\n",
       " 'parquet_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/parquet',\n",
       " 'datapkg_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/datapkg',\n",
       " 'notebook_dir': '/Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/notebook',\n",
       " 'ferc1_db': 'sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/ferc1.sqlite',\n",
       " 'pudl_db': 'sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/pudl.sqlite'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/ferc1.sqlite)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Engine(sqlite:////Users/aesharpe/Desktop/Work/Catalyst_Coop/PUDL_DIR/sqlite/pudl.sqlite)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EIA861_YEARS = list(range(2001, 2020))\n",
    "EIA923_YEARS = list(range(2010, 2012))\n",
    "pudl_settings = pudl.workspace.setup.get_defaults()\n",
    "display(pudl_settings)\n",
    "\n",
    "ferc1_engine = sa.create_engine(pudl_settings['ferc1_db'])\n",
    "display(ferc1_engine)\n",
    "\n",
    "pudl_engine = sa.create_engine(pudl_settings['pudl_db'])\n",
    "display(pudl_engine)\n",
    "\n",
    "pudl_out = pudl.output.pudltabl.PudlTabl(pudl_engine) # freq='monthly'/'annual' (maybe other abr.)\n",
    "\n",
    "\n",
    "# Is there other external data you need to pull in?\n",
    "# If so, put it in a (relatively) standard place on the filesystem.\n",
    "my_new_data_url = \"https://mynewdata.website.gov/path/to/new/data.csv\"\n",
    "my_new_datadir = pathlib.Path(pudl_settings[\"data_dir\"]) / \"local/new_data_source\"\n",
    "\n",
    "# Store API keys and other secrets in environment variables\n",
    "# and read them in here, if needed:\n",
    "# API_KEY_EIA = os.environ[\"API_KEY_EIA \"]\n",
    "# API_KEY_FRED = os.environ[\"API_KEY_FRED \"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "* Given the above parameters and functions, it should now be possible to load the required data into local variables for further wrangling, analysis, and visualization.\n",
    "* If the data is not yet present on the machine of the person running the notebook, this step should also acquire the data from its original source, and place it in the appropriate location under `<PUDL_IN>/data/local/`.\n",
    "* If there are steps which have to be done manually here, put them first so that they fail first if the user hasn't read the instructions, and they can fix the situation before a bunch of other work gets done. Try to minimize the amount of work in the filesystem that has to be done manually though.\n",
    "* If this process takes a little while, don't be shy about producing `logging` output.\n",
    "* Using the `%%time` cell magic can also help users understand which pieces of work / data acquisition are hard:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EIA 861 (2010-2018)\n",
    "* Not yet fully integrated into PUDL\n",
    "* Post-transform harvesting process isn't compatible w/ EIA 861 structure\n",
    "* Only getting the `sales_eia861`, `balancing_authority_eia861`, and `service_territory_eia861` tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Already Transformed EIA 861 DataFrames"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No years given. Not extracting eia860 spreadsheet data.\n",
      "Extracting eia861 spreadsheet data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aesharpe/Desktop/Work/Catalyst_Coop/pudl/src/pudl/extract/eia861.py:39: UserWarning: Integration of EIA 861 into PUDL is still experimental and incomplete.\n",
      "The data has not yet been validated, and the structure may change.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "WARNING *** OLE2 inconsistency: SSCS size is 0 but SSAT size is non-zero\n",
      "No years given. Not extracting eia923 spreadsheet data.\n",
      "No raw EIA 860 dataframes found. Not transforming EIA 860.\n",
      "Transforming raw EIA 861 DataFrames for service_territory_eia861 concatenated across all years.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-50cb8f059a01>\u001b[0m in \u001b[0;36mtest_etl_eia\u001b[0;34m(eia_inputs, pudl_settings)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Transform EIA forms 860, 861, 923\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0meia860_transformed_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpudl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meia860\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meia860_raw_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meia860_tables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meia860_tables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0meia861_transformed_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpudl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meia861\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meia861_raw_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meia861_tables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meia861_tables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0meia923_transformed_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpudl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meia923\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meia923_raw_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meia923_tables\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meia923_tables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Work/Catalyst_Coop/pudl/src/pudl/transform/eia861.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(raw_dfs, eia861_tables)\u001b[0m\n\u001b[1;32m   2383\u001b[0m                     f\"concatenated across all years.\")\n\u001b[1;32m   2384\u001b[0m         \u001b[0mtfr_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_early_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2385\u001b[0;31m         \u001b[0mtfr_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfr_funcs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfr_dfs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2387\u001b[0m     \u001b[0;31m# This is more like harvesting stuff, and should probably be relocated:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Work/Catalyst_Coop/pudl/src/pudl/transform/eia861.py\u001b[0m in \u001b[0;36mservice_territory\u001b[0;34m(tfr_dfs)\u001b[0m\n\u001b[1;32m    834\u001b[0m     transformed_df = (\n\u001b[1;32m    835\u001b[0m         \u001b[0;31m# Ensure that we have the canonical US Census county names:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 836\u001b[0;31m         pudl.helpers.clean_eia_counties(\n\u001b[0m\u001b[1;32m    837\u001b[0m             \u001b[0mtype_compatible_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m             fixes=EIA_FIPS_COUNTY_FIXES)\n",
      "\u001b[0;32m~/Desktop/Work/Catalyst_Coop/pudl/src/pudl/helpers.py\u001b[0m in \u001b[0;36mclean_eia_counties\u001b[0;34m(df, fixes, state_col, county_col)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfixes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitertuples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mstate_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mcounty_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcounty_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meia_county\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate_mask\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mcounty_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcounty_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfips_county\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pudl-dev/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pudl-dev/lib/python3.8/site-packages/pandas/core/ops/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_numpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pudl-dev/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomp_method_OBJECT_ARRAY\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/pudl-dev/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscalar_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eia_inputs = {\n",
    "    \"eia860_years\": [],\n",
    "    \"eia860_tables\": pudl.constants.pudl_tables[\"eia860\"],\n",
    "    \"eia861_years\": EIA861_YEARS,\n",
    "    \"eia861_tables\": pudl.constants.pudl_tables[\"eia861\"],\n",
    "    \"eia923_years\": [],\n",
    "    \"eia923_tables\": pudl.constants.pudl_tables[\"eia923\"],\n",
    "}\n",
    "eia860_raw_dfs, eia861_raw_dfs, eia923_raw_dfs, eia_transformed_dfs = test_etl_eia(eia_inputs=eia_inputs, pudl_settings=pudl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check Data\n",
    "If there's any validation that can be done on the data which you've loaded to flag if/when it is inappropriate for the analysis that follows, do it here. If you find the data is unusable, use `assert` statements or `raise` Exceptions to stop the notebook from proceeding, and indicate what the problem is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "    * compare totals col - result: total_capacity col is more accurate but need total_capacity_mw for conversion of pct to       mw\n",
    "    * ensure that col breakdown is acurate:\n",
    "        * that all columns are accounted for\n",
    "        * that the \"tech\" cols are actually components that sum to the total\n",
    "        * check on the wierd extra cols that are the \"half\" components.\n",
    "    * deal with / record duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMI Data Wrangling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_cols = [\n",
    "    'utility_id_eia',\n",
    "    'state',\n",
    "    'report_date',\n",
    "]\n",
    "idx_ba = util_cols + ['balancing_authority_code_eia']\n",
    "idx_nr = util_cols + ['nerc_region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of 2019 unnamed col for new data\n",
    "for df_name, df in eia_transformed_dfs.items():\n",
    "    if 'unnamed_0' in df.columns:\n",
    "        eia_transformed_dfs[df_name] = (\n",
    "            df.drop('unnamed_0', axis=1)\n",
    "        )\n",
    "\n",
    "# Fix reliability col to say standard\n",
    "eia_transformed_dfs['reliability_eia861'] = (\n",
    "    eia_transformed_dfs['reliability_eia861'].rename(columns={'standard': 'standards_class'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this kernel to reset the dict\n",
    "table_dict = {\n",
    "    'advanced_metering_infrastructure_eia861': eia_transformed_dfs['advanced_metering_infrastructure_eia861'].copy(),\n",
    "     #'balancing_authority_assn_eia861',\n",
    "     #'balancing_authority_eia861': eia_transformed_dfs['balancing_authority_eia861'].copy(),\n",
    "     'demand_response_eia861': eia_transformed_dfs['demand_response_eia861'].copy(),\n",
    "     'demand_response_water_heater_eia861': eia_transformed_dfs['demand_response_water_heater_eia861'].copy(),\n",
    "     'demand_side_management_ee_dr_eia861': eia_transformed_dfs['demand_side_management_ee_dr_eia861'].copy(),\n",
    "     'demand_side_management_misc_eia861': eia_transformed_dfs['demand_side_management_misc_eia861'].copy(),\n",
    "     'demand_side_management_sales_eia861': eia_transformed_dfs['demand_side_management_sales_eia861'].copy(),\n",
    "     'distributed_generation_fuel_eia861': eia_transformed_dfs['distributed_generation_fuel_eia861'].copy(),\n",
    "     'distributed_generation_misc_eia861': eia_transformed_dfs['distributed_generation_misc_eia861'].copy(),\n",
    "     'distributed_generation_tech_eia861': eia_transformed_dfs['distributed_generation_tech_eia861'].copy(),\n",
    "     'distribution_systems_eia861': eia_transformed_dfs['distribution_systems_eia861'].copy(),\n",
    "     'dynamic_pricing_eia861': eia_transformed_dfs['dynamic_pricing_eia861'].copy(),\n",
    "     'energy_efficiency_eia861': eia_transformed_dfs['energy_efficiency_eia861'].copy(),\n",
    "     'green_pricing_eia861': eia_transformed_dfs['green_pricing_eia861'].copy(),\n",
    "     'mergers_eia861': eia_transformed_dfs['mergers_eia861'].copy(),\n",
    "     'net_metering_customer_fuel_class_eia861': eia_transformed_dfs['net_metering_customer_fuel_class_eia861'].copy(),\n",
    "     'net_metering_misc_eia861': eia_transformed_dfs['net_metering_misc_eia861'].copy(),\n",
    "     'non_net_metering_customer_fuel_class_eia861': eia_transformed_dfs['non_net_metering_customer_fuel_class_eia861'].copy(),\n",
    "     'non_net_metering_misc_eia861': eia_transformed_dfs['non_net_metering_misc_eia861'].copy(),\n",
    "     'operational_data_misc_eia861': eia_transformed_dfs['operational_data_misc_eia861'].copy(),\n",
    "     'operational_data_revenue_eia861': eia_transformed_dfs['operational_data_revenue_eia861'].copy(),\n",
    "     'reliability_eia861': eia_transformed_dfs['reliability_eia861'].copy(),\n",
    "     'sales_eia861': eia_transformed_dfs['sales_eia861'].copy(),\n",
    "     'service_territory_eia861': eia_transformed_dfs['service_territory_eia861'].copy(),\n",
    "     #'utility_assn_eia861',\n",
    "     'utility_data_misc_eia861': eia_transformed_dfs['utility_data_misc_eia861'].copy(),\n",
    "     'utility_data_nerc_eia861': eia_transformed_dfs['utility_data_nerc_eia861'].copy(),\n",
    "     'utility_data_rto_eia861': eia_transformed_dfs['utility_data_rto_eia861'].copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpeel_list = [\n",
    "    'advanced_metering_infrastructure_eia861',\n",
    "     #'balancing_authority_assn_eia861',\n",
    "     #'balancing_authority_eia861',\n",
    "     'demand_response_eia861',\n",
    "     'demand_response_water_heater_eia861',\n",
    "     'demand_side_management_ee_dr_eia861',\n",
    "     'demand_side_management_misc_eia861',\n",
    "     'demand_side_management_sales_eia861',\n",
    "     'distributed_generation_fuel_eia861',\n",
    "     'distributed_generation_misc_eia861',\n",
    "     'distributed_generation_tech_eia861',\n",
    "     'distribution_systems_eia861',\n",
    "     'dynamic_pricing_eia861',\n",
    "     'energy_efficiency_eia861',\n",
    "     'green_pricing_eia861',\n",
    "     'mergers_eia861',\n",
    "     'net_metering_customer_fuel_class_eia861',\n",
    "     'net_metering_customer_fuel_class_eia861', # because two classes\n",
    "     'net_metering_misc_eia861',\n",
    "     'non_net_metering_customer_fuel_class_eia861',\n",
    "     'non_net_metering_customer_fuel_class_eia861', # because two classes \n",
    "     'non_net_metering_misc_eia861',\n",
    "     'operational_data_misc_eia861',\n",
    "     'operational_data_revenue_eia861',\n",
    "     'reliability_eia861',\n",
    "     'sales_eia861',\n",
    "     'service_territory_eia861',\n",
    "     #'utility_assn_eia861',\n",
    "     'utility_data_misc_eia861',\n",
    "     'utility_data_nerc_eia861',\n",
    "     'utility_data_rto_eia861'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "moniker_dict = {\n",
    "    'advanced_metering_infrastructure_eia861': 'AMI',\n",
    "     #'balancing_authority_assn_eia861',\n",
    "     #'balancing_authority_eia861',\n",
    "     'demand_response_eia861': 'DR',\n",
    "     'demand_response_water_heater_eia861': 'DR',\n",
    "     'demand_side_management_ee_dr_eia861': 'DSM',\n",
    "     'demand_side_management_misc_eia861': 'DSM',\n",
    "     'demand_side_management_sales_eia861': 'DSM',\n",
    "     'distributed_generation_fuel_eia861': 'DG',\n",
    "     'distributed_generation_misc_eia861': 'DG',\n",
    "     'distributed_generation_tech_eia861': 'DG',\n",
    "     'distribution_systems_eia861': 'DS',\n",
    "     'dynamic_pricing_eia861': 'DP',\n",
    "     'energy_efficiency_eia861': 'EE',\n",
    "     'green_pricing_eia861': 'GP',\n",
    "     'mergers_eia861': 'M',\n",
    "     'net_metering_customer_fuel_class_eia861': 'NM',\n",
    "     'net_metering_misc_eia861': 'NM',\n",
    "     'non_net_metering_customer_fuel_class_eia861': 'NNM',\n",
    "     'non_net_metering_misc_eia861': 'NNM',\n",
    "     'operational_data_misc_eia861': 'OD',\n",
    "     'operational_data_revenue_eia861': 'OD',\n",
    "     'reliability_eia861': 'R',\n",
    "     'sales_eia861': 'S',\n",
    "     'service_territory_eia861': 'ST',\n",
    "     #'utility_assn_eia861',\n",
    "     'utility_data_misc_eia861': 'UD',\n",
    "     'utility_data_nerc_eia861': 'UD',\n",
    "     'utility_data_rto_eia861': 'UD',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpeel(df, df_name, class_name):\n",
    "    \"\"\"Make single class name column into suffix for columns - tall-to-wide reformatting\"\"\"\n",
    "    logger.info(f'unpeeling {class_name} from {df_name} table')\n",
    "    # Include utility_id_eia in qualitative col grab (for index)\n",
    "    string_df = (\n",
    "        df[['utility_id_eia']]\n",
    "        .join(df.select_dtypes(exclude=['int64', 'float']))\n",
    "    )\n",
    "\n",
    "    class_name = class_name\n",
    "    qual_cols = list(string_df.columns)\n",
    "    qual_cols.remove(class_name)\n",
    "\n",
    "    wide_df = (\n",
    "        df.set_index(qual_cols)\n",
    "        .pivot(columns=class_name)\n",
    "    )\n",
    "    old_cols = list(wide_df.columns.values)\n",
    "    wide_df.columns= list(map('_'.join, [col[::-1] for col in old_cols]))\n",
    "    #wide_df.columns = list(map('_'.join, wide_df.columns.values))\n",
    "    wide_df = wide_df.reset_index()\n",
    "    return wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_unpeel(df_name):\n",
    "    \"\"\"Run unpeel function on tables that have a class column.\"\"\"\n",
    "    df = table_dict[df_name].copy()\n",
    "    \n",
    "    # Get rid of categorical columns\n",
    "    for col in df:\n",
    "        if 'category' in df[col].dtype.name:\n",
    "            df[col] = df[col].astype('string')\n",
    "\n",
    "    # Only unpeel if there is a class column.\n",
    "    class_names = [col for col in df if 'class' in col]\n",
    "    if len(class_names) > 0:\n",
    "        wide_df = unpeel(df, df_name, class_names[0])\n",
    "    else:\n",
    "        wide_df = df\n",
    "    \n",
    "    return wide_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupby_utils(df, df_name, util_cols):\n",
    "    \"\"\"Group EIA861 tables at the utility-level\n",
    "    \n",
    "    Some of the qualitative columns may present an aggregation challenge when\n",
    "    grouping at the utility level (nerc_region and ba_code, specifically). To\n",
    "    account for all of these values we'll first look to see if there are any\n",
    "    instances where there are duplicate values (same utility/state/year, diff\n",
    "    nerc region or ba code). If there are, we'll combine them into a single row\n",
    "    EX: SERC and MISC to SERC, MISC. We single out the rows that have duplicates\n",
    "    rather than running this on the whole dataframe to save time.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Set N/A state values to UNK to prevent issues in the .transform() func\n",
    "    df['state'] = df['state'].fillna('UNK')\n",
    "    df = df.set_index(util_cols)\n",
    "    \n",
    "    # Separate the df columns into dtypes\n",
    "    num_df = df.select_dtypes(include=['int64', 'float']).reset_index()\n",
    "    qual_df = df.select_dtypes(exclude=['int64', 'float']).reset_index()\n",
    "    \n",
    "    # See whether any of the columns are duplicated at the utility-state-date level\n",
    "    qual_df['dup'] = qual_df.duplicated(subset=util_cols, keep=False)\n",
    "    \n",
    "    # Divide into duplicated and non-duplicated\n",
    "    dup_df = qual_df[qual_df['dup']==True]\n",
    "    non_dup_df = qual_df[qual_df['dup']==False]\n",
    "    \n",
    "    if dup_df.empty:\n",
    "        logger.info(f'{df_name} has no duplicates')\n",
    "        return df.reset_index()\n",
    "    else:\n",
    "        logger.info(f'{df_name}')\n",
    "        # Combine those that are duplicated into VAL1, VAL2 units\n",
    "        dup_transformed = dup_df.groupby(util_cols).transform(lambda x: ' ,'.join(x.unique()))\n",
    "        dup_grouped = (\n",
    "            dup_df[util_cols]\n",
    "            .drop_duplicates()\n",
    "            .join(dup_transformed)\n",
    "            .groupby(util_cols)\n",
    "            .first()\n",
    "            .reset_index()\n",
    "        )\n",
    "        # Grab the first value for non-duplicated values\n",
    "        non_dup_grouped = non_dup_df.groupby(util_cols).first().reset_index()\n",
    "\n",
    "        # Combine newly grouped duplicates and non duplicates\n",
    "        qual_grouped = dup_grouped.append(non_dup_grouped, ignore_index=True)\n",
    "\n",
    "        # Sum numeric columns\n",
    "        num_grouped = num_df.groupby(util_cols).sum(min_count=1)\n",
    "\n",
    "        # Merge numeric and qualitative dataframes back together\n",
    "        merge_df = pd.merge(num_grouped, qual_grouped, on=util_cols).drop('dup', axis=1)\n",
    "        \n",
    "        return merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mega_merge(table_dict):\n",
    "    \"\"\"Merge all the EIA 861 tables together\"\"\"\n",
    "    # Get the list of eia861 tables and merge them together. Add numeric suffixes to columns that repeat.\n",
    "    #table_list = list(table_dict.values())\n",
    "    merge_df = pd.DataFrame(columns=util_cols)\n",
    "    #num = 0\n",
    "    for df_name, df in table_dict.items():\n",
    "        logger.info(f'merging {df_name}')\n",
    "        moniker = moniker_dict[df_name]\n",
    "        df = df.set_index(util_cols)\n",
    "        df.columns = df.columns.map(lambda x: str(x) + f'_{moniker}_')\n",
    "        merge_df = pd.merge(merge_df, df, on=util_cols, how='outer')\n",
    "        #num = num+1\n",
    "    \n",
    "    return merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpeel_group_merge():\n",
    "    \"\"\"Re-widen all tables, groupby utility, merge into one mega table.\"\"\"\n",
    "    # Go through list of tables and widen. Use unpeel_list because \n",
    "    # the non/net_metering tables have to be run twice.\n",
    "    for df_name in unpeel_list:\n",
    "        wide_df = check_and_unpeel(df_name)\n",
    "        table_dict[df_name] = wide_df\n",
    "    \n",
    "    # Group each of the widened tables by utility/state/date\n",
    "    for df_name, df in table_dict.items():\n",
    "        wide_df = df.copy()\n",
    "        util_df = groupby_utils(wide_df, df_name, util_cols)\n",
    "        table_dict[df_name] = util_df\n",
    "        \n",
    "    # Merge wide, grouped tables together into one \"mega\" dataframe\n",
    "    mega_df = mega_merge(table_dict)\n",
    "    \n",
    "    return mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_common_cols(df, col_name):\n",
    "    \"\"\"Turn repeat columns into one column with all values.\"\"\"\n",
    "    col_list = [col for col in df if col_name in col]\n",
    "    col_df = df.set_index(util_cols)[col_list]\n",
    "    temp_df = col_df.fillna('UNK')\n",
    "    temp_df = temp_df.eq(temp_df.iloc[:, 0], axis=0)\n",
    "    col_df['bool'] = temp_df.eq(temp_df.iloc[:, 0], axis=0).all(1)\n",
    "    col_df_false = col_df[col_df['bool']==False].copy()\n",
    "    col_df_false = col_df_false.astype('object')\n",
    "    col_df_false.fillna(np.nan)\n",
    "    col_df_false[col_name] = (\n",
    "        col_df_false[col_df_false.columns[:-1]]\n",
    "        .apply(lambda x: ', '.join(x.dropna().unique()), axis=1)\n",
    "    )\n",
    "    df = df.drop(col_list, axis=1)\n",
    "    df = pd.merge(df, col_df_false[[col_name]], on=util_cols, how='outer')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_over_common_cols(mega_df):\n",
    "    \n",
    "    common_cols = [\n",
    "        'balancing_authority_code_eia',\n",
    "        'utility_name_eia',\n",
    "        'nerc_region',\n",
    "        'entity_type',\n",
    "    ]\n",
    "    # get rid of short form cols\n",
    "    logger.info('removing short form columns')\n",
    "    drop_list = []\n",
    "    for col in mega_df:\n",
    "        if 'short_form' in col:\n",
    "            drop_list.append(col)\n",
    "    mega_df = mega_df.drop(drop_list, axis=1)\n",
    "            \n",
    "    # Compare duplicate columns in the mega table\n",
    "    for col in common_cols:\n",
    "        logger.info(f'comparing column values for {col}')\n",
    "        mega_df = compare_common_cols(mega_df, col)\n",
    "    \n",
    "    return mega_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unpeeling customer_class from advanced_metering_infrastructure_eia861 table\n",
      "unpeeling customer_class from demand_response_eia861 table\n",
      "unpeeling customer_class from demand_side_management_ee_dr_eia861 table\n",
      "unpeeling fuel_class from distributed_generation_fuel_eia861 table\n",
      "unpeeling tech_class from distributed_generation_tech_eia861 table\n",
      "unpeeling customer_class from dynamic_pricing_eia861 table\n",
      "unpeeling customer_class from energy_efficiency_eia861 table\n",
      "unpeeling customer_class from green_pricing_eia861 table\n",
      "unpeeling customer_class from net_metering_customer_fuel_class_eia861 table\n",
      "unpeeling tech_class from net_metering_customer_fuel_class_eia861 table\n",
      "unpeeling customer_class from non_net_metering_customer_fuel_class_eia861 table\n",
      "unpeeling tech_class from non_net_metering_customer_fuel_class_eia861 table\n",
      "unpeeling revenue_class from operational_data_revenue_eia861 table\n",
      "unpeeling standards_class from reliability_eia861 table\n",
      "unpeeling customer_class from sales_eia861 table\n",
      "advanced_metering_infrastructure_eia861\n",
      "demand_response_eia861\n",
      "demand_response_water_heater_eia861\n",
      "demand_side_management_ee_dr_eia861 has no duplicates\n",
      "demand_side_management_misc_eia861 has no duplicates\n",
      "demand_side_management_sales_eia861 has no duplicates\n",
      "distributed_generation_fuel_eia861 has no duplicates\n",
      "distributed_generation_misc_eia861 has no duplicates\n",
      "distributed_generation_tech_eia861 has no duplicates\n",
      "distribution_systems_eia861 has no duplicates\n",
      "dynamic_pricing_eia861\n",
      "energy_efficiency_eia861\n",
      "green_pricing_eia861 has no duplicates\n",
      "mergers_eia861 has no duplicates\n",
      "net_metering_customer_fuel_class_eia861\n",
      "net_metering_misc_eia861\n",
      "non_net_metering_customer_fuel_class_eia861\n",
      "non_net_metering_misc_eia861\n",
      "operational_data_misc_eia861 has no duplicates\n",
      "operational_data_revenue_eia861 has no duplicates\n",
      "reliability_eia861\n",
      "sales_eia861\n",
      "service_territory_eia861\n",
      "utility_data_misc_eia861 has no duplicates\n",
      "utility_data_nerc_eia861\n",
      "utility_data_rto_eia861\n",
      "merging advanced_metering_infrastructure_eia861\n",
      "merging demand_response_eia861\n",
      "merging demand_response_water_heater_eia861\n",
      "merging demand_side_management_ee_dr_eia861\n",
      "merging demand_side_management_misc_eia861\n",
      "merging demand_side_management_sales_eia861\n",
      "merging distributed_generation_fuel_eia861\n",
      "merging distributed_generation_misc_eia861\n",
      "merging distributed_generation_tech_eia861\n",
      "merging distribution_systems_eia861\n",
      "merging dynamic_pricing_eia861\n",
      "merging energy_efficiency_eia861\n",
      "merging green_pricing_eia861\n",
      "merging mergers_eia861\n",
      "merging net_metering_customer_fuel_class_eia861\n",
      "merging net_metering_misc_eia861\n",
      "merging non_net_metering_customer_fuel_class_eia861\n",
      "merging non_net_metering_misc_eia861\n",
      "merging operational_data_misc_eia861\n",
      "merging operational_data_revenue_eia861\n",
      "merging reliability_eia861\n",
      "merging sales_eia861\n",
      "merging service_territory_eia861\n",
      "merging utility_data_misc_eia861\n",
      "merging utility_data_nerc_eia861\n",
      "merging utility_data_rto_eia861\n"
     ]
    }
   ],
   "source": [
    "mega_df = unpeel_group_merge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removing short form columns\n",
      "comparing column values for balancing_authority_code_eia\n",
      "comparing column values for utility_name_eia\n",
      "comparing column values for nerc_region\n",
      "comparing column values for entity_type\n"
     ]
    }
   ],
   "source": [
    "final_df = loop_over_common_cols(mega_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_excel('mega_eia861.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(table_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_val_dict = {\n",
    "    '_AMI_': ['advanced_metering_infrastructure_eia861', []],\n",
    "    '_DG_': ['distributed_generation_eia861', ['capacity_mw']],\n",
    "    '_DP_': ['dynamic_pricing_eia861', []],\n",
    "    '_DR_': ['demand_response_eia861', ['cost']],\n",
    "    '_DS_': ['distribution_systems_eia861', []],\n",
    "    '_DSM_': ['demand_side_management_eia861', ['cost', 'payment']],\n",
    "    '_EE_': ['energy_efficiency_eia861', []],\n",
    "    '_GP_': ['green_pricing_eia861', []],\n",
    "    '_M_': ['mergers_eia861', []],\n",
    "    '_NM_': ['net_metering_eia861', []],\n",
    "    '_NNM_': ['non_net_metering_eia861', []],\n",
    "    '_OD_': ['operational_data_eia861', []],\n",
    "    '_R_': ['reliability_eia861', []],\n",
    "    '_S_': ['sales_eia861', []],\n",
    "    '_ST_': ['service_territory_eia861', []],\n",
    "    '_UD_': ['utility_data_eia861', []],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = final_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_df.set_index(util_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split mega data into OG eia table chunks in prep for comparison \n",
    "monikers = list(d_val_dict.keys())#list(set(moniker_dict.values()))\n",
    "monikers.sort()\n",
    "\n",
    "by_eia_table_dict = {}\n",
    "for moniker in monikers:\n",
    "    moniker_cols = [col for col in val_df if moniker in col]\n",
    "    non_moniker_cols = [col.strip(f'_{moniker}_') for col in moniker_cols]\n",
    "    moniker_df = val_df[moniker_cols]\n",
    "    moniker_df.columns = non_moniker_cols\n",
    "    by_eia_table_dict[moniker] = moniker_df.reset_index()\n",
    "    \n",
    "# delete cols with only null values -- if you uncomment this, then there will be some\n",
    "# cases where a reported column also has all nulls (as opposed to made up cols from the\n",
    "# re-widening process)\n",
    "for name, table in by_eia_table_dict.items():\n",
    "    null_cols = []\n",
    "    for col in table:\n",
    "        if table[col].dtype == 'float' or table[col].dtype == 'int':\n",
    "            if table[col].isnull().all():\n",
    "                null_cols.append(col)\n",
    "    by_eia_table_dict[name] = table.drop(null_cols, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep raw data for comparison\n",
    "raw_dfs_dict = eia861_raw_dfs.copy()\n",
    "\n",
    "for df_name, df in raw_dfs_dict.items():\n",
    "    df = pudl.helpers.fix_eia_na(df)\n",
    "    df = pudl.helpers.convert_to_date(df)\n",
    "    raw_dfs_dict[df_name] = df\n",
    "    \n",
    "raw_dfs_dict = pudl.helpers.convert_dfs_dict_dtypes(raw_dfs_dict, 'eia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = raw_dfs_dict['operational_data_eia861'].copy()\n",
    "test2 = test[test['utility_id_eia'].isna()]\n",
    "test2.to_excel('OD_NA.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse order of customer_class and tech_class in raw net/non_net metering tables\n",
    "\n",
    "tc = pudl.constants.TECH_CLASSES\n",
    "cc = pudl.constants.CUSTOMER_CLASSES\n",
    "\n",
    "def swap_col_order(df_name):\n",
    "    raw_order_cols = raw_dfs_dict[df_name].columns.tolist()\n",
    "    #test = ['commercial_chp_cogen_customers', '']\n",
    "    new_order_cols = []\n",
    "    for col in raw_order_cols:\n",
    "        for c in cc: \n",
    "            if c in col:\n",
    "                for t in tc:\n",
    "                    if t in col:\n",
    "                        col = col.replace(f'{c}_{t}_', f'{t}_{c}_')\n",
    "        new_order_cols.append(col)\n",
    "        \n",
    "    raw_dfs_dict[df_name].columns = new_order_cols\n",
    "\n",
    "swap_col_order('net_metering_eia861')\n",
    "swap_col_order('non_net_metering_eia861')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapt raw tables to account for data cleaning and manipulation\n",
    "\n",
    "dr_df = raw_dfs_dict['demand_response_eia861'].copy()\n",
    "raw_dfs_dict['demand_response_eia861'] = (\n",
    "    dr_df.drop_duplicates(subset=util_cols+['balancing_authority_code_eia'])\n",
    ")\n",
    "dsm_df = raw_dfs_dict['demand_side_management_eia861'].copy()\n",
    "raw_dfs_dict['demand_side_management_eia861'] = (\n",
    "    dsm_df.loc[dsm_df['utility_id_eia'] != 88888].copy()\n",
    ")\n",
    "nm_df = raw_dfs_dict['net_metering_eia861'].copy()\n",
    "raw_dfs_dict['net_metering_eia861'] = (\n",
    "    nm_df.loc[nm_df['utility_id_eia'] != 99999].copy()\n",
    ")\n",
    "nnm_df = raw_dfs_dict['non_net_metering_eia861'].copy()\n",
    "raw_dfs_dict['non_net_metering_eia861'] = (\n",
    "    nnm_df.loc[nnm_df['utility_id_eia'] != 99999].copy()\n",
    ")\n",
    "od_df = raw_dfs_dict['operational_data_eia861'].copy()\n",
    "raw_dfs_dict['operational_data_eia861'] = (\n",
    "    od_df.loc[od_df['utility_id_eia'] != 88888].copy()\n",
    ") #NULLS!\n",
    "r_df = raw_dfs_dict['reliability_eia861'].copy()\n",
    "raw_dfs_dict['reliability_eia861'] = (\n",
    "    r_df.drop_duplicates(subset=util_cols)\n",
    ")\n",
    "s_df = raw_dfs_dict['sales_eia861'].copy()\n",
    "raw_dfs_dict['sales_eia861'] = (\n",
    "    s_df.drop_duplicates(subset=util_cols + ['balancing_authority_code_eia'])\n",
    ")\n",
    "s_df = raw_dfs_dict['sales_eia861'].copy()\n",
    "raw_dfs_dict['sales_eia861'] = (\n",
    "    s_df.loc[s_df['utility_id_eia'] != 88888].copy()\n",
    ")\n",
    "s_df = raw_dfs_dict['sales_eia861'].copy()\n",
    "raw_dfs_dict['sales_eia861'] = (\n",
    "    s_df.loc[s_df['utility_id_eia'] != 99999].copy()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>consumed_by_facility_mwh</th>\n",
       "      <th>consumed_by_respondent_without_charge_mwh</th>\n",
       "      <th>credits_or_adjustments_revenue</th>\n",
       "      <th>data_observed</th>\n",
       "      <th>delivery_customers_revenue</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>exchange_energy_delivered_mwh</th>\n",
       "      <th>exchange_energy_received_mwh</th>\n",
       "      <th>furnished_without_charge_mwh</th>\n",
       "      <th>nerc_region</th>\n",
       "      <th>net_generation_mwh</th>\n",
       "      <th>net_power_exchanged_mwh</th>\n",
       "      <th>net_wheeled_power_mwh</th>\n",
       "      <th>other_revenue</th>\n",
       "      <th>retail_sales_mwh</th>\n",
       "      <th>retail_sales_revenue</th>\n",
       "      <th>sales_for_resale_mwh</th>\n",
       "      <th>sales_for_resale_revenue</th>\n",
       "      <th>short_form</th>\n",
       "      <th>state</th>\n",
       "      <th>summer_peak_demand_mw</th>\n",
       "      <th>total_disposition_mwh</th>\n",
       "      <th>total_energy_losses_mwh</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>total_sources_mwh</th>\n",
       "      <th>transmission_by_other_losses_mwh</th>\n",
       "      <th>transmission_revenue</th>\n",
       "      <th>unbundled_revenue</th>\n",
       "      <th>utility_id_eia</th>\n",
       "      <th>utility_name_eia</th>\n",
       "      <th>wheeled_power_delivered_mwh</th>\n",
       "      <th>wheeled_power_received_mwh</th>\n",
       "      <th>wholesale_power_purchases_mwh</th>\n",
       "      <th>winter_peak_demand_mw</th>\n",
       "      <th>report_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [consumed_by_facility_mwh, consumed_by_respondent_without_charge_mwh, credits_or_adjustments_revenue, data_observed, delivery_customers_revenue, entity_type, exchange_energy_delivered_mwh, exchange_energy_received_mwh, furnished_without_charge_mwh, nerc_region, net_generation_mwh, net_power_exchanged_mwh, net_wheeled_power_mwh, other_revenue, retail_sales_mwh, retail_sales_revenue, sales_for_resale_mwh, sales_for_resale_revenue, short_form , state, summer_peak_demand_mw, total_disposition_mwh, total_energy_losses_mwh, total_revenue, total_sources_mwh, transmission_by_other_losses_mwh, transmission_revenue, unbundled_revenue, utility_id_eia, utility_name_eia, wheeled_power_delivered_mwh, wheeled_power_received_mwh, wholesale_power_purchases_mwh, winter_peak_demand_mw, report_date]\n",
       "Index: []"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WHY IS THIS BLANK AFTER RUNNING THE ABOVE??????? \n",
    "test = raw_dfs_dict['operational_data_eia861'].copy()\n",
    "test['utility_id_eia'] = test.utility_id_eia.astype('float')\n",
    "#test.loc[test['utility_id_eia'].isna()]\n",
    "#test[['utility_id_eia']].sort_values('utility_id_eia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_against_raw_numeric(df, raw_df, df_name_and_exceptions):\n",
    "    \"\"\"Compare numeric columns against their raw counterpart data.\"\"\"\n",
    "    logger.info('')\n",
    "    logger.info(f'checking columns for {df_name_and_exceptions[0]} table')\n",
    "    \n",
    "    num_df = df.select_dtypes(include=['int64', 'float']).set_index('utility_id_eia')\n",
    "    raw_num_df = raw_df.select_dtypes(include=['int64', 'float']).set_index('utility_id_eia')\n",
    "    \n",
    "    not_in_raw = [col for col in num_df if col not in raw_num_df]\n",
    "    not_in_transformed = [col for col in raw_num_df if col not in num_df]\n",
    "    not_in_transformed = [col for col in not_in_transformed if 'total' not in col] #exclude total cols\n",
    "    in_both = [col for col in num_df if col in raw_num_df]\n",
    "    for exception in df_name_and_exceptions[1]:\n",
    "        in_both = [col for col in in_both if exception not in col]\n",
    "    \n",
    "    logger.info(f'     columns not in the raw_df: {not_in_raw}')\n",
    "    logger.info(f'     columns not in the transformed_df {not_in_transformed}')\n",
    "    \n",
    "    # Check whether the raw column total is the same as the transformed column total\n",
    "    for col in in_both:\n",
    "        new_sum = round(num_df[col].sum(skipna=True), 0)\n",
    "        raw_sum = round(raw_num_df[col].sum(skipna=True), 0)\n",
    "        if new_sum != raw_sum:\n",
    "            if raw_sum != round((new_sum/1000), 0):\n",
    "                print(f'     sum miss-match for col: {col}')\n",
    "                print(f'     new_sum: {new_sum}, raw_sum: {raw_sum}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "checking columns for advanced_metering_infrastructure_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for distributed_generation_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df ['backup_capacity_pct', 'combustion_turbine_capacity_pct', 'distributed_generation_owned_capacity_pct', 'hydro_capacity_pct', 'internal_combustion_capacity_pct', 'other_capacity_pct', 'steam_capacity_pct', 'wind_capacity_pct']\n",
      "\n",
      "checking columns for dynamic_pricing_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for demand_response_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for distribution_systems_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for demand_side_management_eia861 table\n",
      "     columns not in the raw_df: ['total_price_responsiveness_customers', 'other_time_responsiveness_customers', 'total_time_responsiveness_customers']\n",
      "     columns not in the transformed_df ['demand_side_management']\n",
      "\n",
      "checking columns for energy_efficiency_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df ['other_incremental_peak_reduction_mw']\n",
      "\n",
      "checking columns for green_pricing_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for mergers_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for net_metering_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for non_net_metering_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for operational_data_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for reliability_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for sales_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df ['other_customers', 'other_sales_mwh', 'other_sales_revenue']\n",
      "\n",
      "checking columns for service_territory_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n",
      "\n",
      "checking columns for utility_data_eia861 table\n",
      "     columns not in the raw_df: []\n",
      "     columns not in the transformed_df []\n"
     ]
    }
   ],
   "source": [
    "for moniker, df_name_and_exceptions in d_val_dict.items():\n",
    "    check_against_raw_numeric(\n",
    "        by_eia_table_dict[moniker],\n",
    "        raw_dfs_dict[df_name_and_exceptions[0]],\n",
    "        df_name_and_exceptions\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation of data transformations:\n",
    "\n",
    "**DG**: change pct into mw - sums will differ\n",
    "\n",
    "**DR**: cost cols thousands to ones, drop duplicates\n",
    "\n",
    "**DSM**: cost / payment cols thousands to one, removed 88888 utilities\n",
    "\n",
    "**NM**: removed 99999 utilities, extra colums from reconstruction that are all nan. can delete but don't impact sum.\n",
    "\n",
    "**NNM**: removed 99999 utilitiesremoved 99999 utilities, *had to fix issue with capacity_mw merge deleting y vs. x*\n",
    "\n",
    "**OD**: removed 88888 utilities, **removed utilities with NA for eia_id**\n",
    "\n",
    "**R**: dropped duplicates\n",
    "\n",
    "**S**: removed 99999 and 88888 utilities, dropped duplicates, revenue cols thousands to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>consumed_by_facility_mwh</th>\n",
       "      <th>consumed_by_respondent_without_charge_mwh</th>\n",
       "      <th>credits_or_adjustments_revenue</th>\n",
       "      <th>data_observed</th>\n",
       "      <th>delivery_customers_revenue</th>\n",
       "      <th>entity_type</th>\n",
       "      <th>exchange_energy_delivered_mwh</th>\n",
       "      <th>exchange_energy_received_mwh</th>\n",
       "      <th>furnished_without_charge_mwh</th>\n",
       "      <th>nerc_region</th>\n",
       "      <th>net_generation_mwh</th>\n",
       "      <th>net_power_exchanged_mwh</th>\n",
       "      <th>net_wheeled_power_mwh</th>\n",
       "      <th>other_revenue</th>\n",
       "      <th>retail_sales_mwh</th>\n",
       "      <th>retail_sales_revenue</th>\n",
       "      <th>sales_for_resale_mwh</th>\n",
       "      <th>sales_for_resale_revenue</th>\n",
       "      <th>short_form</th>\n",
       "      <th>state</th>\n",
       "      <th>summer_peak_demand_mw</th>\n",
       "      <th>total_disposition_mwh</th>\n",
       "      <th>total_energy_losses_mwh</th>\n",
       "      <th>total_revenue</th>\n",
       "      <th>total_sources_mwh</th>\n",
       "      <th>transmission_by_other_losses_mwh</th>\n",
       "      <th>transmission_revenue</th>\n",
       "      <th>unbundled_revenue</th>\n",
       "      <th>utility_id_eia</th>\n",
       "      <th>utility_name_eia</th>\n",
       "      <th>wheeled_power_delivered_mwh</th>\n",
       "      <th>wheeled_power_received_mwh</th>\n",
       "      <th>wholesale_power_purchases_mwh</th>\n",
       "      <th>winter_peak_demand_mw</th>\n",
       "      <th>report_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [index, consumed_by_facility_mwh, consumed_by_respondent_without_charge_mwh, credits_or_adjustments_revenue, data_observed, delivery_customers_revenue, entity_type, exchange_energy_delivered_mwh, exchange_energy_received_mwh, furnished_without_charge_mwh, nerc_region, net_generation_mwh, net_power_exchanged_mwh, net_wheeled_power_mwh, other_revenue, retail_sales_mwh, retail_sales_revenue, sales_for_resale_mwh, sales_for_resale_revenue, short_form , state, summer_peak_demand_mw, total_disposition_mwh, total_energy_losses_mwh, total_revenue, total_sources_mwh, transmission_by_other_losses_mwh, transmission_revenue, unbundled_revenue, utility_id_eia, utility_name_eia, wheeled_power_delivered_mwh, wheeled_power_received_mwh, wholesale_power_purchases_mwh, winter_peak_demand_mw, report_date]\n",
       "Index: []"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = raw_dfs_dict['operational_data_eia861']\n",
    "test = test.reset_index()\n",
    "test[test['utility_id_eia'].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Data Wrangling\n",
    "Once all of the data is loaded and looks like it's in good shape, do any initial wrangling that's specific to this particular analysis. This should mostly make use of the higher level functions which were defined above. If this step takes a while, don't be shy about producing `logging` outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = eia_transformed_dfs['demand_side_management_misc_eia861']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11940\n",
      "11940\n"
     ]
    }
   ],
   "source": [
    "test['dup'] = test.duplicated(subset=['utility_id_eia', 'state', 'report_date'])\n",
    "test.sort_values('dup', ascending=False)\n",
    "\n",
    "print(len(test.groupby(['utility_id_eia', 'state', 'report_date'])))\n",
    "print(len(test.groupby(['utility_id_eia', 'state', 'report_date', 'nerc_region'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Validation Test with Pandera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>utility_id_eia</th>\n",
       "      <th>state</th>\n",
       "      <th>balancing_authority_code_eia</th>\n",
       "      <th>report_date</th>\n",
       "      <th>short_form</th>\n",
       "      <th>utility_name_eia</th>\n",
       "      <th>customer_class</th>\n",
       "      <th>advanced_metering_infrastructure</th>\n",
       "      <th>automated_meter_reading</th>\n",
       "      <th>daily_digital_access_customers</th>\n",
       "      <th>direct_load_control_customers</th>\n",
       "      <th>energy_served_ami_mwh</th>\n",
       "      <th>home_area_network</th>\n",
       "      <th>non_amr_ami</th>\n",
       "      <th>total_meters</th>\n",
       "      <th>summ</th>\n",
       "      <th>same</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [utility_id_eia, state, balancing_authority_code_eia, report_date, short_form, utility_name_eia, customer_class, advanced_metering_infrastructure, automated_meter_reading, daily_digital_access_customers, direct_load_control_customers, energy_served_ami_mwh, home_area_network, non_amr_ami, total_meters, summ, same]\n",
       "Index: []"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Zscore not a good measure because utilities are not all uniform in size.\n",
    "\n",
    "df = eia_transformed_dfs['advanced_metering_infrastructure_eia861'].copy()\n",
    "df['advanced_metering_infrastructure'] = df['advanced_metering_infrastructure'].fillna(0)\n",
    "df['automated_meter_reading'] = df['automated_meter_reading'].fillna(0)\n",
    "df['non_amr_ami'] = df['non_amr_ami'].fillna(0)\n",
    "df['total_meters'] = df['total_meters'].fillna(0)\n",
    "\n",
    "df = df.assign(\n",
    "    summ=lambda x: (\n",
    "        x.advanced_metering_infrastructure \n",
    "        + x.automated_meter_reading \n",
    "        + x.non_amr_ami),\n",
    "    same=lambda x: x.summ == x.total_meters\n",
    ")\n",
    "\n",
    "df[(df['same']==False) & (df['total_meters']!= 0)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
