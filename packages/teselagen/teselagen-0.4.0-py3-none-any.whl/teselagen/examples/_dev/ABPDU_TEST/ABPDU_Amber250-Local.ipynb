{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teselagen version:  0.4.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import platform\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta, date, time\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 5)\n",
    "from tqdm import tqdm\n",
    "import teselagen\n",
    "from teselagen.api import TeselaGenClient\n",
    "\n",
    "print(\"teselagen version: \", teselagen.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Process\n",
    "Here we perform a series of steps that will get the data originally stored in the Amber250 excel file into CSV files formatted for TEST imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse Amber250 Data Export Excel File\n",
    "\n",
    "The Amber250 Excel file has multiple sheets with the measurements of every Assay Subject (HT1 to HT12)\n",
    "\n",
    "Here we are going to read each sheet and create a CSV file for each of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HT1\n",
      "HT2\n",
      "HT3\n",
      "HT4\n",
      "HT5\n",
      "HT6\n",
      "HT7\n",
      "HT8\n",
      "HT9\n",
      "HT10\n",
      "HT11\n",
      "HT12\n"
     ]
    }
   ],
   "source": [
    "# Read the excel spreadsheet file (here we use pandas)\n",
    "excel_filepath = \"./data/ABFr_3HP_ambr_01.xlsx\"\n",
    "# 'subjects_data' it's a dictionary we'll be using to store data split by the different subjects (HT1 to HT12).\n",
    "subjects_data = {}\n",
    "reader = pd.ExcelFile(excel_filepath, engine=\"openpyxl\")\n",
    "for subject_name in reader.sheet_names:\n",
    "    print(subject_name)\n",
    "    subject_df = pd.read_excel(excel_filepath, sheet_name=subject_name, engine=\"openpyxl\")\n",
    "    # We set a Subject ID Index called Reactor\n",
    "    subject_df.insert(0, \"Reactor\", subject_name)\n",
    "    subjects_data[subject_name] = subject_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HT1 (51199, 31)\n",
      "HT2 (51519, 31)\n",
      "HT3 (51520, 31)\n",
      "HT4 (51536, 31)\n",
      "HT5 (51547, 31)\n",
      "HT6 (51557, 31)\n",
      "HT7 (51571, 31)\n",
      "HT8 (51584, 31)\n",
      "HT9 (51583, 31)\n",
      "HT10 (51602, 31)\n",
      "HT11 (51620, 31)\n",
      "HT12 (51607, 31)\n"
     ]
    }
   ],
   "source": [
    "# Display the different dataframes and their shape stored in our subjects_data dictionary and an example of one of them\n",
    "for subject_name, subject_df in subjects_data.items():\n",
    "    print(subject_name, subject_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the data into a format suitable for a TEST import. \n",
    "\n",
    "Define the measurement units: TEST data import require measurements to have a unit associated to its values. The Amber250 spreadsheet however does not export this information, so we are manually going to define the units based on offline documents ABPDU has shared.\n",
    "\n",
    "Process the data: TEST data import supports data with multiple measurements, however in this case each measurement comes with its own independent time column.\n",
    "\n",
    "So we'll have to split each measurement into its own file and import them separately into the assay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define each measurement's units. Units are very important when importing scientific data into TEST.\n",
    "measurement_units = {\n",
    "    \"Time\": \"hrs\",\n",
    "    \"Acid volume pumped\": \"mL\",\n",
    "    \"Air flow\": \"lpm\",\n",
    "    \"Antifoam volume pumped\": \"mL\",\n",
    "    \"Base volume pumped\": \"mL\",\n",
    "    \"CER\": \"dimensionless\",\n",
    "    \"DO\": \"dimensionless\",\n",
    "    \"Feed#1 volume pumped\": \"mL\",\n",
    "    \"OUR\": \"dimensionless\",\n",
    "    \"pH\": \"dimensionless\",\n",
    "    \"RQ\": \"dimensionless\",\n",
    "    \"Stir speed\": \"rpm\",\n",
    "    \"Temperature\": \"C\",\n",
    "    \"Volume\": \"L\",\n",
    "    \"Volume - sampled\": \"L\",\n",
    "    \"Volume of inocula\": \"L\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Build a data decimation or downsampling function\n",
    "We will use this function to perform data decimation for each of our measurement tables.\n",
    "\n",
    "This is not mandatory but it will make the import times faster and database storage lighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimate_data(dataframe: pd.DataFrame, sampling_period: str = None,  max_datapoints: int = 100, with_datetime: bool = False, verbose: bool = False) -> pd.DataFrame:\n",
    "    \n",
    "    # Create a date time column for decimation with pandas resample. We only have \"hour\" information, thus a dummy date will be used.\n",
    "    dataframe[\"DateTime\"] = dataframe.Time.apply(lambda hours: datetime.combine(date.today(), time()) + timedelta(hours=hours))\n",
    "    # Compute the time span in minutes.\n",
    "    minutes_span=(dataframe[\"DateTime\"].max()-dataframe[\"DateTime\"].min()).total_seconds() / 60\n",
    "\n",
    "    # Compute the sampling period automatially based on the number of datapoints wanted (defaulted to 200).\n",
    "    if minutes_span > max_datapoints and sampling_period is None:\n",
    "        minutes_step = math.floor(minutes_span / max_datapoints)\n",
    "        dataframe.set_index(\"DateTime\", inplace=True)\n",
    "        dataframe=dataframe.resample(rule=f\"{minutes_step}Min\").first().dropna().reset_index(drop=not with_datetime)\n",
    "        if verbose:\n",
    "            print(\"minutes_span\", minutes_span)\n",
    "            print(\"minutes_step\", minutes_step)\n",
    "            print(\"datapoints left\", dataframe.shape[0])\n",
    "    elif sampling_period is not None: \n",
    "        dataframe.set_index(\"DateTime\", inplace=True)\n",
    "        dataframe=dataframe.resample(rule=sampling_period).first().dropna().reset_index(drop=not with_datetime)\n",
    "    else:\n",
    "        dataframe.set_index(\"DateTime\", inplace=True)\n",
    "        dataframe.dropna().reset_index(drop=not with_datetime)\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a data table (dataframe) for each measurement constructed with the measurement data from all the subjects.\n",
    "The result will be one table per measurement column, with the following columns: Reactor, Time, Time Unit, \"MeasurementName\", \"MeasurementName\" Unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Every measurement in the data comes with its own Time column which in turn has its own time values,\n",
    "# this prevents us from importing the different measurements together at once because they are not associated or aligned against a unique reference dimension.\n",
    "# Fortunately we can split them into different one measurement tabular files and import them separately.\n",
    "\n",
    "# Here we store an auxiliary list of all the columns in the data. (all subjects have the same columns so we'll just get them from the first one)\n",
    "column_names = list(subjects_data.values())[0].columns.values\n",
    "# 'measurements_data' is a dictionary mapping the different measurement names to their dataframes.\n",
    "measurements_data = {}\n",
    "measurements_metadata = {}\n",
    "# Loop through each measurement in the data\n",
    "for measurement_number, measurement_name in enumerate(column_names[slice(2, None, 2)]):\n",
    "    # if measurement_name == \"Base volume pumped\": break\n",
    "    measurement_df = None\n",
    "    # Loop through the data of every subject\n",
    "    for subject_name, subject_df in subjects_data.items():\n",
    "        # if subject_name == \"HT1\": continue\n",
    "        # elif subject_name == \"HT3\": break\n",
    "        # Get the current measurement values\n",
    "        measurement_values = subject_df[measurement_name]\n",
    "        # Get the current timepoint for the above vslues\n",
    "        measurement_timepoints = subject_df[column_names[2 * (measurement_number + 1) - 1]]\n",
    "        \n",
    "        # This skips any measurement with no values (e.g., 'Antifoam volume pumped').\n",
    "        if(measurement_values.dropna().shape[0] == 0): continue\n",
    "        \n",
    "        # Here each measurement result table is being created.\n",
    "        subject_measurement_df = pd.DataFrame({\n",
    "            \"Reactor\": subject_df[\"Reactor\"],\n",
    "            \"Time\": measurement_timepoints,\n",
    "            \"Time units\": measurement_units[\"Time\"],\n",
    "            measurement_name: measurement_values,\n",
    "            f\"{measurement_name} units\": measurement_units[measurement_name]\n",
    "        })\n",
    "        # measurements_data.append(measurement_df)\n",
    "        # Drops any rows with NaN values.\n",
    "        subject_measurement_df.dropna(inplace=True)\n",
    "        \n",
    "        # Decimate data automatically based on the measurement time span.\n",
    "        subject_measurement_df = decimate_data(dataframe=subject_measurement_df)\n",
    "                \n",
    "        if measurement_df is None:\n",
    "            measurement_df = subject_measurement_df\n",
    "        else:\n",
    "            measurement_df = measurement_df.append(subject_measurement_df)\n",
    "    \n",
    "    if measurement_df is not None: measurements_data[measurement_name] = measurement_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acid volume pumped (170, 5)\n",
      "Air flow (1200, 5)\n",
      "Base volume pumped (204, 5)\n",
      "CER (1212, 5)\n",
      "DO (1212, 5)\n",
      "Feed#1 volume pumped (105, 5)\n",
      "OUR (1217, 5)\n",
      "pH (1188, 5)\n",
      "RQ (1217, 5)\n",
      "Stir speed (1212, 5)\n",
      "Temperature (846, 5)\n",
      "Volume (1200, 5)\n",
      "Volume - sampled (180, 5)\n",
      "Volume of inocula (12, 5)\n"
     ]
    }
   ],
   "source": [
    "# Display the different dataframes and their shape stored in our subjects_data dictionary and an example of one of them\n",
    "for measurement, measurement_df in measurements_data.items():\n",
    "    print(measurement, measurement_df.shape)\n",
    "# measurements_data['DO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we have each subject's measurements separated, we proceed to generate a CSV file for each of these dataframes.\n",
    "# We are going to use these CSVs later on once we call TEST API.\n",
    "\n",
    "# Here we export these dataframes as CSV files into the directories created for the subjects subject (HT1 to HT12).\n",
    "measurements_data_dir = Path(\"./data/measurements\")\n",
    "Path(measurements_data_dir).mkdir(parents=True, exist_ok=True)\n",
    "# Loop through the data of every subject\n",
    "for measurement_name, measurement_data in measurements_data.items():\n",
    "    measurement_data_file = measurements_data_dir / Path(f\"{measurement_name}.csv\")\n",
    "    # Creates the directory in case is does not exist.\n",
    "    measurement_data.to_csv(measurement_data_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect to the TeselaGen Client\n",
    "\n",
    "Here we are going to import the TeselaGenClient from the 'teselagen' Python package.\n",
    "\n",
    "We are going to create a client instance and call its 'login' method with valid credentials.\n",
    "\n",
    "Then we are going to select the Laboratory we are going to want to work in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Accepted\n"
     ]
    }
   ],
   "source": [
    "#host_url=\"http://host.docker.internal:3000\"\n",
    "host_url=\"https://pr-app-7840.teselagen.net/\"\n",
    "client = TeselaGenClient(host_url=host_url, module_name=\"test\")\n",
    "client.login(\n",
    "    username='test@teselagen.com',\n",
    "    # apiKey='' # Get your API Key from the TeselaGen web Application at: Settings > API Key > Generate CLI API Key\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Laboratory/Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1', 'name': 'The Test Lab'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Lab: The Test Lab\n"
     ]
    }
   ],
   "source": [
    "## Fetch My Laboratories\n",
    "display(client.get_laboratories())\n",
    "\n",
    "## Select a Laboratory\n",
    "client.select_laboratory(lab_name='The Test Lab')\n",
    "#client.unselect_laboratory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Laboratory Environment\n",
    "### Before importing data, we first prepare our new Laboratory environment.\n",
    "\n",
    "1) Create TEST **data types** according to the multiomics files. These are used to map the different data file headers.\n",
    "   The different data type records we are going to create are:\n",
    "   \n",
    "      b. Mesurement Target (names and decribes each measurement)\n",
    "      c. Assay Subject Class (class or category of the subject under evaluation (i.e., the bioreactor) ) \n",
    "      d. Reference Dimension (Time dimension)\n",
    "      e. Unit (different used units across measurements)\n",
    "\n",
    "2) Create an experiment, this will be the scope of our files and assay measurements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Creating Data Types\n",
    "Here we are going to create all the necessary data type records needed according to the Ambr250 file headers.\n",
    "In TEST, data type records are strictly related to the mapping and understanding of each column of tabular data. There are different **types** of data types (refer to [Metadata Documentation](https://docs.teselagen.com/en/articles/4508837-test-data-mapping)).\n",
    "\n",
    "One way of understanding TEST data type records is that these are used to **map** (i.e., give meaning) to columns in tabular data, much like tabular headers do but in a more structured and organized manner.\n",
    "\n",
    "The following Notebook cells show how to create these data type records. For each record created, an ID will be returned. These IDs will be particularly important when creating the different **mappers** (array of structured headers) used to import the tabular data into TEST data lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load: Unit Dimensions\n",
    "Units are very important when when getting your data structured. TEST units are composed of the **unit name** (or symbol) and associated with a physical **unit dimension** (e.g., Time, Volume, Pressure, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elapsed Time': '1',\n",
       " 'Volume': '2',\n",
       " 'Volumetric Flow Rate': '9',\n",
       " 'Rotational Speed': '7',\n",
       " 'Temperature': '8',\n",
       " 'Dimensionless': '6'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit dimensions of each of the above units.\n",
    "colum_unit_dimensions = [\n",
    "    {'name':'Elapsed Time'},\n",
    "    {'name':'Volume'},\n",
    "    {'name':'Volumetric Flow Rate'}, \n",
    "    {'name':'Rotational Speed'}, \n",
    "    {'name':'Temperature'},\n",
    "    {'name':'Dimensionless'} # Dummy dimension for dimensionless measurements (s.a., OD)\n",
    "]\n",
    "unitDimensions = client.test.create_metadata(metadataType=\"unitDimension\", metadataRecord=colum_unit_dimensions)\n",
    "unitDimensionNameToId = {unitDimension['name']: unitDimension['id'] for unitDimension in unitDimensions}\n",
    "unitDimensionNameToId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load: Unit Scales\n",
    "Unit scales are another useful feature of TEST Units. They provide a way to group different units together in order to convert measurement from one to another interchangeably when doing data analysis.\n",
    "\n",
    "These however are not strictly required and can be omitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elapsed time scale': '7',\n",
       " 'Volume scale': '8',\n",
       " 'Volumetric flow rate scale': '9',\n",
       " 'Rotational Speed scale': '10',\n",
       " 'Temperature scale': '11',\n",
       " 'Dimensionless scale': '12'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit scales for the above unit dimensions.\n",
    "colum_unit_scales = [\n",
    "    {'name':'Elapsed time scale', 'unitDimensionId': unitDimensionNameToId['Elapsed Time']},\n",
    "    {'name':'Volume scale', 'unitDimensionId': unitDimensionNameToId['Volume']},\n",
    "    {'name':'Volumetric flow rate scale', 'unitDimensionId': unitDimensionNameToId['Volumetric Flow Rate']},\n",
    "    {'name':'Rotational Speed scale', 'unitDimensionId': unitDimensionNameToId['Rotational Speed']},\n",
    "    {'name':'Temperature scale', 'unitDimensionId': unitDimensionNameToId['Temperature']},\n",
    "    {'name':'Dimensionless scale', 'unitDimensionId': unitDimensionNameToId['Dimensionless']}\n",
    "]\n",
    "unitScales = client.test.create_metadata(metadataType='unitScale', metadataRecord=colum_unit_scales)\n",
    "unitScaleNameToId = {unitScale['name']: unitScale['id'] for unitScale in unitScales}\n",
    "unitScaleNameToId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load: Units\n",
    "As explained above, these are the unit names or symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hrs', 'mL', 'lpm', 'mL', 'mL', 'dimensionless', 'dimensionless', 'mL', 'dimensionless', 'dimensionless', 'dimensionless', 'rpm', 'C', 'L', 'L', 'L']\n"
     ]
    }
   ],
   "source": [
    "print(list(measurement_units.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hrs': '1',\n",
       " 'mL': '18',\n",
       " 'lpm': '15',\n",
       " 'rpm': '14',\n",
       " 'C': '16',\n",
       " 'dimensionless': '17'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unit names (aliases) for each data column.\n",
    "column_units = [\n",
    "    {'name':'hrs', 'unitScaleId': unitScaleNameToId['Elapsed time scale']},\n",
    "    {'name':'mL', 'unitScaleId': unitScaleNameToId['Volume scale']},\n",
    "    {'name':'lpm', 'unitScaleId': unitScaleNameToId['Volumetric flow rate scale']},\n",
    "    {'name':'rpm', 'unitScaleId': unitScaleNameToId['Rotational Speed scale']},\n",
    "    {'name':'C', 'unitScaleId': unitScaleNameToId['Temperature scale']},\n",
    "    {'name':'dimensionless', 'unitScaleId': unitScaleNameToId['Dimensionless scale']},\n",
    "]\n",
    "units = client.test.create_metadata(metadataType='unit', metadataRecord=column_units)\n",
    "unitNameToId = {unit['name']: unit['id'] for unit in units}\n",
    "unitNameToId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load: Assay Subject Class\n",
    "TEST **Assay Subject Class** type is of great importance. This supports the classification of the subjects under evaluation or under measure. Every measurement needs to be linked to an Assay Subject. It allows consistency,traceability and organization for you data.\n",
    "\n",
    "within the scope of your **TEST Laboratory**, you can be studying different types of subjects (e.g., Bacterial Strains, Enzyme mutants, Bioreactor Runs, etc.). Assay Subject Class provides a way to classify them or group them accordingly.\n",
    "\n",
    "Here we are just going to create a single Assay Subject Class called **Run**, which will be the class of al Ambr250 Runs, from HT1 to HT12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Reactor': '12'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assay Subject corresponds to the \"ID\" of the \"object\" under evaluation.\n",
    "assay_subject_class=[{'name':'Reactor', 'description': 'Ambr250 Reactor'}]\n",
    "assaySubjectClasses = client.test.create_metadata(metadataType='assaySubjectClass', metadataRecord=assay_subject_class)\n",
    "assaySubjectClassNameToId = {assaySubjectClass['name']: assaySubjectClass['id'] for assaySubjectClass in assaySubjectClasses}\n",
    "assaySubjectClassNameToId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load: Reference Dimension\n",
    "Measurements are usually taken against a reference dimension. The most common being **Time**. TEST Reference Dimension is just that, you will hardly ever need to create a new Reference Dimension but this allows to associated each of your measurements against a reference.\n",
    "\n",
    "Think it as a 2D plot, where your measurement corresponds to the Y-Axis, and your reference to the X-Axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Elapsed Time': '1'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference Dimensions correspond to an independent variable against which your measurements are taken.\n",
    "reference_dimensions=[\n",
    "    {'name':'Elapsed Time', 'description': 'Time represented as elapsed hours.', \"unitDimensionId\":1}\n",
    "]\n",
    "referenceDimensions = client.test.create_metadata(metadataType='referenceDimension', metadataRecord=reference_dimensions)\n",
    "referenceDimensionNameToId = {referenceDimension['name']: referenceDimension['id'] for referenceDimension in referenceDimensions}\n",
    "referenceDimensionNameToId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load: Measurement Targets\n",
    "Usually organizational data becomes messy or difficult to align across the many scientist recording their own measurements into sometimes losely-structured spreadsheets with non-standardized measurement column names. \n",
    "\n",
    "This last TEST data type tackles just that, and each measurement needs to be mapped to a **Measurement Target**. This way every piece of data that gets imported into TEST Data Lake, will be structured and easily comprehensible by the organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Acid volume pumped': '53',\n",
       " 'Air flow': '50',\n",
       " 'Antifoam volume pumped': '45',\n",
       " 'Base volume pumped': '47',\n",
       " 'CER': '49',\n",
       " 'DO': '46',\n",
       " 'Feed#1 volume pumped': '48',\n",
       " 'OUR': '51',\n",
       " 'pH': '59',\n",
       " 'RQ': '52',\n",
       " 'Stir speed': '58',\n",
       " 'Temperature': '56',\n",
       " 'Volume': '57',\n",
       " 'Volume - sampled': '55',\n",
       " 'Volume of inocula': '54'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These correspond to the \"Targets\" being measured in this data. \n",
    "# Each of them are assigned a 'name'\n",
    "# and also a 'description' that can help clarify what the measurement target is to the organization.\n",
    "measurement_targets = [{\"name\": measurement_name, \"description\": \"edit\"} for measurement_name in column_names[slice(2,None,2)]]\n",
    "measurementTargets = client.test.create_metadata(metadataType='measurementTarget', metadataRecord=measurement_targets)\n",
    "measurementTargetNameToId = {measurementTarget['name']: measurementTarget['id'] for measurementTarget in measurementTargets}\n",
    "measurementTargetNameToId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Experiment (Study)\n",
    "Experiments are part of TEST organizational hierarchy. These live under a Laboratory scope and can be used to group many **Assays** together.\n",
    "\n",
    "For this data, we are going to create an Experiment where we're going to store all our assay files and data we built above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '24', 'name': 'Ambr250'}\n"
     ]
    }
   ],
   "source": [
    "## This will create a new Experiment. The output will give as the Experiment ID that we'll be using later.\n",
    "experiment_name=\"Ambr250\"\n",
    "experiment = client.test.create_experiment(experiment_name=experiment_name)\n",
    "print(experiment)\n",
    "experiment_id = experiment['id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST Data Import\n",
    "Now that the Laboratory has been prepared, we are ok to begin the data import process.\n",
    "\n",
    "We are going to import the data we split into assays into TEST Data Lake.\n",
    "\n",
    "1) Create **mapper** objects for each measurement results. These will map the **metadata** we created with the columns in the data.\n",
    "\n",
    "2) Import the data using the stored CSV along with the **mapper** object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapper Object\n",
    "Mapper Objects are always hand in hand with tabular data when importing into TEST Data Lake. These mappers are used to assign a TEST data type (like the ones we created above) to each of the data columns. This way data in TEST Data Lake is always kept structured, comprehensible, traceable and organized.\n",
    "\n",
    "A mapper object is simply a list or an array of a few key/value properties:\n",
    "  - name: this is the name of the column\n",
    "  - class: this is the data type class (e.g., assaySubjectClass, measurementTarget or unit).\n",
    "  - subClass: this is the data tybe subclass (e.g., \"Reactor\" for assaySubjectClasses, or \"Elapsed Time\" for referenceDimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since all of the assays have the same tabular format, each .\n",
    "mapper_objects = {}\n",
    "for measurement_name in measurements_data.keys():\n",
    "    mapper_object = [\n",
    "        {\n",
    "            \"name\": \"Reactor\", # Name of the column containing the Run Names (HT1 to HT12) \n",
    "            \"class\": \"assaySubjectClass\", # TEST Assay Subject Class data type.\n",
    "            \"subClass\": assaySubjectClassNameToId[\"Reactor\"] # ID of the \"Reactor\" data type record.\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Time\", # Name of the column containing the measurement times.\n",
    "            \"class\": \"referenceDimension\", # TEST Reference Dimension data type.\n",
    "            \"subClass\": referenceDimensionNameToId['Elapsed Time'] # ID of the referenceDimension data type record.\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"Time units\", # Name of the column containing the unit of time used for each measurement.\n",
    "            \"class\": \"d-unit\",\n",
    "            # ID of the referenceDimension metadata record.\n",
    "            # This is in order to assign this \"Unit\" column to the Time column measurements.\n",
    "            \"subClass\": referenceDimensionNameToId['Elapsed Time']\n",
    "        },\n",
    "        {\n",
    "            \"name\": measurement_name,\n",
    "            \"class\": \"measurementTarget\",\n",
    "            \"subClass\": measurementTargetNameToId[measurement_name]\n",
    "        },\n",
    "        {\n",
    "            \"name\": f\"{measurement_name} units\",\n",
    "            \"class\": \"unit\",\n",
    "            \"subClass\": measurementTargetNameToId[measurement_name]\n",
    "        }\n",
    "    ]\n",
    "    mapper_objects[measurement_name] = mapper_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'Reactor', 'class': 'assaySubjectClass', 'subClass': '12'},\n",
       " {'name': 'Time', 'class': 'referenceDimension', 'subClass': '1'},\n",
       " {'name': 'Time units', 'class': 'd-unit', 'subClass': '1'},\n",
       " {'name': 'Volume of inocula', 'class': 'measurementTarget', 'subClass': '54'},\n",
       " {'name': 'Volume of inocula units', 'class': 'unit', 'subClass': '54'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's visualize how the Mapper objects look like.\n",
    "display(mapper_objects[measurement_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the mapper objects together with the measurement files to import its data into TEST Data Lake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import ID=25 processing Acid volume pumped data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=26 processing Air flow data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=27 processing Base volume pumped data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=28 processing CER data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=29 processing DO data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=30 processing Feed#1 volume pumped data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=31 processing OUR data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=32 processing pH data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=33 processing RQ data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=34 processing Stir speed data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=35 processing Temperature data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=36 processing Volume data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=37 processing Volume - sampled data file into 'Amber250 Assay 4' assay.\n",
      "Import ID=38 processing Volume of inocula data file into 'Amber250 Assay 4' assay.\n"
     ]
    }
   ],
   "source": [
    "# This will loop though the measurement CSVs and submit an import proecss for each of those into the same Assay.\n",
    "# Each of these submissions returns back an 'importId' that we can use to query the state/progress of the import process.\n",
    "# Acid volume pumped (170, 5)\n",
    "# Air flow (1200, 5)\n",
    "# Base volume pumped (204, 5)\n",
    "# CER (1212, 5)\n",
    "# DO (1212, 5)\n",
    "# Feed#1 volume pumped (105, 5)\n",
    "# OUR (1217, 5)\n",
    "# pH (1188, 5)\n",
    "# RQ (1217, 5)\n",
    "# Stir speed (1212, 5)\n",
    "# Temperature (846, 5)\n",
    "# Volume (1200, 5)\n",
    "# Volume - sampled (180, 5)\n",
    "# Volume of inocula (12, 5)\n",
    "import_responses = []\n",
    "measurements_dir = Path(\"./data/measurements\")\n",
    "assay_name = \"Amber250 Assay 4\"\n",
    "for measurement_name in measurements_data.keys():\n",
    "    # if measurement_name not in ['Volume - sampled']:\n",
    "    #     continue\n",
    "    assay_result_filepath = measurements_dir / Path(f\"{measurement_name}.csv\")\n",
    "    # assay_name = f\"{measurement_name} Assay\"\n",
    "    import_response = client.test.import_assay_results(\n",
    "        filepath=assay_result_filepath,\n",
    "        assay_name=assay_name,\n",
    "        experiment_id=experiment_id,\n",
    "        mapper=mapper_objects[measurement_name]\n",
    "    )\n",
    "    print(f\"Import ID={import_response['importId']} processing {measurement_name} data file into '{assay_name}' assay.\")\n",
    "    import_responses.append(import_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'url': 'https://pr-app-7840.teselagen.net//test/cli-api/assays/results/import/38',\n",
       " 'status': True,\n",
       " 'content': {'importId': '38',\n",
       "  'assayId': '23',\n",
       "  'status': {'code': 'FINISHED', 'description': 'Importer job finished'},\n",
       "  'message': None}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we show how we can query the state/progress of an import process for one of the submitted ones above.\n",
    "import_process = client.test.get_assay_results_import_status(importId=38)\n",
    "display(import_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '1',\n",
       "  'name': 'Example Assay',\n",
       "  'experiment': {'id': '1', 'name': 'Example Experiment'}},\n",
       " {'id': '17',\n",
       "  'name': 'Wild Type Optical Density',\n",
       "  'experiment': {'id': '22', 'name': 'Multiomics data for WT Strain'}},\n",
       " {'id': '18',\n",
       "  'name': 'Wild Type External Metabolites',\n",
       "  'experiment': {'id': '22', 'name': 'Multiomics data for WT Strain'}},\n",
       " {'id': '19',\n",
       "  'name': 'Wild Type Transcriptomics',\n",
       "  'experiment': {'id': '22', 'name': 'Multiomics data for WT Strain'}},\n",
       " {'id': '20',\n",
       "  'name': 'Wild Type Proteomics',\n",
       "  'experiment': {'id': '22', 'name': 'Multiomics data for WT Strain'}},\n",
       " {'id': '21',\n",
       "  'name': 'Wild Type Metabolomics',\n",
       "  'experiment': {'id': '22', 'name': 'Multiomics data for WT Strain'}},\n",
       " {'id': '22',\n",
       "  'name': 'Isoprenol Production',\n",
       "  'experiment': {'id': '23', 'name': 'Multiomics BE strains data'}},\n",
       " {'id': '23',\n",
       "  'name': 'Amber250 Assay 4',\n",
       "  'experiment': {'id': '24', 'name': 'Ambr250'}}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(client.test.get_assays())\n",
    "12*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the 'page_size' argument for pagination is advised (default page_size=200).\n",
      "Using the 'page_number' argument for pagination is advised (default page_number=1).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assayId</th>\n",
       "      <th>fileId</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>Subject ID Subject Name Subject Class  Ela...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23</td>\n",
       "      <td>38</td>\n",
       "      <td>Subject ID Subject Name Subject Class  Elap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   assayId fileId                                               data\n",
       "0       23     30      Subject ID Subject Name Subject Class  Ela...\n",
       "1       23     38     Subject ID Subject Name Subject Class  Elap..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_with_subject_data=client.test.get_assay_results(assay_id=23, as_dataframe=True, with_subject_data=True)\n",
    "pd.DataFrame(results_with_subject_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results_with_subject_data[3][\"data\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
