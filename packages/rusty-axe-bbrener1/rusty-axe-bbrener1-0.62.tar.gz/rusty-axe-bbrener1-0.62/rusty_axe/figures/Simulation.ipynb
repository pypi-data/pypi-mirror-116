{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulations\n",
    "\n",
    "In this notebook we wish to run a simulation demonstrating some of the basic claims we make regarding the random \n",
    "forest. \n",
    "\n",
    "The key claims we would like to demonstrate are thus:\n",
    "\n",
    "- A dataset can have heirarchal behavior\n",
    "    - an RF will identify such hierarchal structure \n",
    "    - an RF will capture local changes in covariance etc\n",
    "    \n",
    "    - A PCA CANNOT capture some of the effects that we will identify as local in distinct PCs.\n",
    "\n",
    "- When a dataset undergoes changes in population prevalence, we identify this as a shift in factor values\n",
    "\n",
    "- When a dataset undergoes a change in population behavior we identify this as a shift in predictive power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reflect a hierarchal structure with meaningful local behavior, we will need several features that have different means among different clusters, but importantly also interact with each other, especially in different ways within different clusters. \n",
    "\n",
    "Let's operate on 10 features total. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On The Basis of Component Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import multivariate_normal,norm,beta\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import scale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will generate the macro-structure. We will generate an eigenvector that applies globally, has a \n",
    "# multivariate normal set of loadings and a bimodal normal distribution of values\n",
    "\n",
    "def generate(noise_multiplier=1):\n",
    "\n",
    "    global_noise = [\n",
    "        1,1,1\n",
    "    ]\n",
    "\n",
    "    factor_1 = [\n",
    "        2,0,0\n",
    "    ]\n",
    "    \n",
    "    factor_2 = [\n",
    "        0,1,0\n",
    "    ]\n",
    "\n",
    "    factor_3 = [\n",
    "        0,0,1\n",
    "    ]\n",
    "\n",
    "    factors = np.vstack([factor_1,factor_2,factor_3])\n",
    "\n",
    "    \n",
    "    noise = multivariate_normal(global_noise,np.identity(3)*noise_multiplier).rvs(10000)    \n",
    "\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(factors,aspect='auto',interpolation='none')\n",
    "    plt.show()\n",
    "    \n",
    "    loadings_1 = norm().rvs(10000) \n",
    "    loadings_2 = norm().rvs(2500)\n",
    "    loadings_3 = norm().rvs(2500)\n",
    "#     loadings_2 = norm().rvs(2000)\n",
    "#     loadings_3 = norm().rvs(2000)\n",
    "#     loadings_1 = beta(1,1).rvs(10000) + 3\n",
    "#     loadings_2 = beta(.5,.5).rvs(2500) + 1\n",
    "#     loadings_3 = beta(.5,.5).rvs(2500) + 1\n",
    "#     loadings_2 = beta(1,1).rvs(2000) + 2\n",
    "#     loadings_3 = beta(1,1).rvs(2000) + 2\n",
    "\n",
    "\n",
    "    loadings_1 = loadings_1[np.argsort(loadings_1)]\n",
    "    loadings_2 = loadings_2[np.argsort(loadings_2)]\n",
    "    loadings_3 = loadings_3[np.argsort(loadings_3)]\n",
    "\n",
    "    combined_loadings = np.zeros((10000,3))\n",
    "    combined_loadings[:,0] = loadings_1\n",
    "    combined_loadings[5000:7500,1] = loadings_2\n",
    "    combined_loadings[7500:,2] = loadings_3\n",
    "#     combined_loadings[:2000,1] = loadings_2\n",
    "#     combined_loadings[-2000:,2] = loadings_3\n",
    "    \n",
    "    \n",
    "    coordinates = np.dot(combined_loadings,factors) + (noise * noise_multiplier)\n",
    "    \n",
    "    return (combined_loadings,factors,coordinates)\n",
    "\n",
    "loadings,factors,coordinates = generate(noise_multiplier=.3)\n",
    "\n",
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "\n",
    "# sample_agglomeration = dendrogram(linkage(coordinates, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "\n",
    "# loadings = loadings[sample_agglomeration]\n",
    "# coordinates = coordinates[sample_agglomeration]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the data we have generated. \n",
    "# We should have two broad clusters which are easily distinguished \n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Example Loadings\")\n",
    "plt.imshow(loadings,aspect='auto',interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Example Factor Values\")\n",
    "plt.imshow(factors,aspect='auto',interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Example Feature Values\")\n",
    "plt.imshow(coordinates,aspect='auto',interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(coordinates[:,1],coordinates[:,2],s=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage,dendrogram\n",
    "\n",
    "# sample_agglomeration = dendrogram(linkage(coordinates, metric='cosine', method='average'), no_plot=True)['leaves']\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Feature Values\")\n",
    "plt.imshow(coordinates,aspect='auto',interpolation='none',cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"True Loadings\")\n",
    "plt.imshow(loadings,aspect='auto',interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Factors\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(coordinates[sample_agglomeration],aspect='auto',interpolation='none')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we will produce an embedding of the newly generated dataset for \n",
    "# easier visualization. \n",
    "\n",
    "t_coordinates = TSNE().fit_transform(coordinates)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"TSNE Embedded Simulated Samples\")\n",
    "plt.scatter(*t_coordinates.T)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# First we can visualize the true factor values in order to understand \n",
    "# which clusters are which\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"True Factor 1 Scores\")\n",
    "plt.scatter(*t_coordinates.T,c=loadings[:,0],cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"True Factor 2 Scores\")\n",
    "plt.scatter(*t_coordinates.T,c=loadings[:,1],cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"True Factor 3 Scores\")\n",
    "plt.scatter(*t_coordinates.T,c=loadings[:,2],cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can perform a PCA analysis to see if we can recover the \n",
    "# global and local factors accurately. \n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "model = PCA().fit(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we observe that PCA DOES explain most of the variance present in the datset,\n",
    "# however it does so after using 4 components.\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"PC Explanatory Power (Ratio)\")\n",
    "plt.plot(model.explained_variance_ratio_)\n",
    "plt.xlabel(\"PCs\")\n",
    "plt.ylabel(\"Variance Fraction Explained\")\n",
    "plt.show()\n",
    "\n",
    "model.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, we would be interested to see if it's possible to understand reover local \n",
    "# feature relationships in the PC loadings. After all, in an ideal case, we would recover\n",
    "# our loadings exactly.\n",
    "\n",
    "print(model.components_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we see that the loadings of the PCs discovered do NOT contain a negative association \n",
    "# between 2 and the 6-7 pair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pct = model.transform(coordinates)\n",
    "pct.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can observe directly that while the PCs recover the overall struture somewhat correctly,\n",
    "# they make inappropriately global inferences about individual PCs.\n",
    "\n",
    "# This occurs despite the fact that it is in principle perfectly possible to represent the \n",
    "# data structure corretly using 3 PCs with non-standardly distributed scores. \n",
    "# (As this was the generative process)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"PC1 Scores\")\n",
    "plt.scatter(*t_coordinates.T,c=pct[:,0],cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"PC2 Scores\")\n",
    "plt.scatter(*t_coordinates.T,c=pct[:,1],cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"PC3 Scores\")\n",
    "plt.scatter(*t_coordinates.T,c=pct[:,2],cmap='bwr')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abmax= np.max(np.abs(pct))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Principal Component Loadings\")\n",
    "plt.imshow(pct[:,:3],interpolation='none',aspect='auto',cmap='bwr',vmin=-abmax,vmax=abmax)\n",
    "plt.xlabel(\"PCs\")\n",
    "plt.ylabel(\"Samples\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        print((i,j))\n",
    "        plt.subplot(3,3,(i*3)+(j+1))\n",
    "        plt.title(f\"PC {i}, Factor {j}\")\n",
    "        plt.scatter(loadings[:,i],pct[:,j],s=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(loadings[:,1],pct[:,1])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(true_factor_scores[:,1],pct[:,2])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(true_factor_scores[:,1],pct[:,3])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(true_factor_scores[:,2],pct[:,1])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(true_factor_scores[:,2],pct[:,2])\n",
    "# plt.show()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(true_factor_scores[:,2],pct[:,3])\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PCA(n_components=3).fit(coordinates)\n",
    "pct = model.transform(coordinates)\n",
    "\n",
    "recovered = model.inverse_transform(pct)\n",
    "\n",
    "null_residuals = coordinates - np.mean(coordinates,axis=0)\n",
    "recovered_residuals = coordinates - recovered\n",
    "\n",
    "null_error = np.sum(np.power(null_residuals,2))\n",
    "recovered_error = np.sum(np.power(recovered_residuals,2))\n",
    "\n",
    "print(null_error)\n",
    "print(recovered_error)\n",
    "\n",
    "recovered_error / null_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can examine whether a Random Forest can capture the structure that eluded a PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import tree_reader as tr \n",
    "import lumberjack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train a relatively shallow forest, since the dataset is not complex. \n",
    "# We have relatively few features and would like a reliable structure, so will bootstrap a \n",
    "# large number of features per node (80% bootstrap)\n",
    "\n",
    "# The rest of the parameters aren't deeply important, and so are left without comment. \n",
    "\n",
    "forest = lumberjack.fit(\n",
    "    coordinates,\n",
    "    trees=300,\n",
    "    ifs=8,\n",
    "    ofs=8,\n",
    "    braids=1,\n",
    "    ss=100,\n",
    "    leaves=10,\n",
    "    depth=2,\n",
    "    norm='l1',\n",
    "    sfr=.5,\n",
    "#     reduce_input='true',\n",
    "#     reduce_output='true',\n",
    "    reduce_input='false',\n",
    "    reduce_output='false',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest.tsne_coordinates = t_coordinates\n",
    "forest.reset_split_clusters()\n",
    "forest.interpret_splits(mode='additive_mean',metric='cosine',depth=3,resolution=1,pca=False,k=200,relatives=True)\n",
    "forest.maximum_spanning_tree(mode='samples')\n",
    "# forest.most_likely_tree(depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# forest.html_tree_summary(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The resulting tree is available at:\n",
    "\n",
    "# https://bx.bio.jhu.edu/track-hubs/bc/sc_summary/simulation/tree_template.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(forest.factor_matrix()[:,1:],cmap='bwr',vmin=-1,vmax=1,aspect='auto',interpolation='none')\n",
    "plt.xticks(np.arange(len(forest.split_clusters[:-1])),np.arange(1,len(forest.split_clusters)))\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "\n",
    "reordered = [1,5,3,4,2,6]\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Forest Factor Loadings\")\n",
    "plt.imshow(forest.factor_matrix()[:,reordered],cmap='bwr',vmin=-1,vmax=1,aspect='auto',interpolation='none')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "factor_matrix = forest.factor_matrix()\n",
    "f_d = factor_matrix.shape[1]\n",
    "l_d = loadings.shape[1]\n",
    "\n",
    "plt.figure(figsize=(10,20))\n",
    "for i in range(f_d):\n",
    "    for j in range(l_d):\n",
    "        print((i,j))\n",
    "        plt.subplot(f_d,l_d,(i*3)+(j+1))\n",
    "        plt.title(f\"Forest Factor {i}, True Factor {j}\")\n",
    "        plt.scatter(loadings[:,j],factor_matrix[:,i],s=1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we would like to range over a variety of noise levels\n",
    "\n",
    "# noise_range = np.zeros((5,10000,10))\n",
    "# true_range = np.zeros((5,10000,3))\n",
    "\n",
    "# for i,noise in enumerate(range(-2,3)):\n",
    "    \n",
    "#     true_factor_scores = np.zeros((10000,3))\n",
    "#     coordinates = np.zeros((10000,10))\n",
    "\n",
    "#     true_factor_scores,coordinates = generate_global(\n",
    "#         true_factor_scores,\n",
    "#         coordinates,\n",
    "#         noise_multiplier=2**noise,\n",
    "#     )\n",
    "\n",
    "#     true_factor_scores,coordinates = generate_local(\n",
    "#         true_factor_scores,\n",
    "#         coordinates,\n",
    "#     )\n",
    "\n",
    "\n",
    "\n",
    "#     plt.figure()\n",
    "#     plt.title(\"Individual Feature Values\")\n",
    "#     plt.imshow(coordinates,aspect='auto',interpolation='none')\n",
    "#     plt.colorbar()\n",
    "#     plt.xlabel(\"Features\")\n",
    "#     plt.ylabel(\"Samples\")\n",
    "#     plt.show()\n",
    "    \n",
    "#     noise_range[i] = coordinates\n",
    "#     true_range[i] = true_factor_scores\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"Individual Feature Values\")\n",
    "# plt.imshow(noise_range.reshape((50000,10)),aspect='auto',interpolation='none')\n",
    "# plt.colorbar()\n",
    "# plt.xlabel(\"Features\")\n",
    "# plt.ylabel(\"Samples\")\n",
    "# plt.show()\n",
    "\n",
    "    \n",
    "# t_coordinates = TSNE().fit_transform(noise_range.reshape((50000,10)))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.title(\"TSNE Embedded Simulated Samples\")\n",
    "# plt.scatter(*t_coordinates.T)\n",
    "# plt.show()\n",
    "\n",
    "# for i in range(5):\n",
    "#     plt.figure()\n",
    "#     plt.title(\"TSNE Embedded Simulated Samples\")\n",
    "#     plt.scatter(*t_coordinates[i*10000:(i+1)*10000].T,c=np.arange(10000),cmap='rainbow')\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "# for i in range(5):\n",
    "    \n",
    "#     forest = lumberjack.fit(\n",
    "#         noise_range[i],\n",
    "#         trees=300,\n",
    "#         ifs=8,\n",
    "#         ofs=8,\n",
    "#         braids=1,\n",
    "#         ss=1000,\n",
    "#         leaves=10,\n",
    "#         depth=4,\n",
    "#         norm='l1',\n",
    "#         sfr=0,\n",
    "#     #     reduce_input='true',\n",
    "#     #     reduce_output='true',\n",
    "#         reduce_input='false',\n",
    "#         reduce_output='false',\n",
    "#     )\n",
    "    \n",
    "#     forest.tsne_coordinates = t_coordinates[i*10000:(i+1)*10000]\n",
    "#     forest.reset_split_clusters()\n",
    "#     forest.interpret_splits(mode='additive_mean',metric='cosine',depth=4,pca=3,k=500,relatives=True)\n",
    "#     forest.maximum_spanning_tree(mode='samples')\n",
    "    \n",
    "#     plt.figure()\n",
    "#     plt.imshow(forest.factor_matrix(),aspect='auto',interpolation='none',cmap='bwr')\n",
    "#     plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1x = np.arange(0,10) - 5\n",
    "v1y = np.arange(0,10)\n",
    "\n",
    "v2x = np.cos(np.arange(0, 2 * np.pi ,.1))\n",
    "v2y = np.sin(np.arange(0, 2 * np.pi ,.1))\n",
    "\n",
    "# v2x = np.arange(0,6)\n",
    "# v2y = (np.arange(0,6) * - 1) + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(v1x,v1y)\n",
    "plt.plot(v2x,v2y)\n",
    "plt.plot\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "transformed = PCA().fit_transform(np.array([np.hstack([v1x,v2x]), np.hstack([v1y,v2y])]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed.shape\n",
    "plt.figure()\n",
    "plt.scatter(*transformed.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

