{
  "00:00": "Hi everybody and welcome to lesson 7! We're going to start by having a look at a kind",
  "00:08": "of regularization called weight decay. And the issue that we came to at the end of the",
  "00:14": "last lesson, is that we were training our simple dot product model with bias, and our",
  "00:22": "loss started going down, and then it started going up again. And so we have a problem that",
  "00:29": "we are overfitting. And remember in this case we're using mean squared error, so try to",
  "00:36": "recall why it is that we don't need a metric here. Because mean squared error is pretty",
  "00:43": "much the thing we care to care about really. Or we could use mean absolute error if we",
  "00:48": "like, but either of those works fine as a loss function. They don't have the problem",
  "00:52": "of big flat areas like accuracy does classification. So what we want to do is to make it less likely",
  "01:00": "that we're going to overfit by doing something we call reducing the capacity of the model.",
  "01:06": "The capacity of the model is basically how much space does it have to find answers. And",
  "01:12": "if it can kind of find any answer anywhere those answers can include basically memorizing",
  "01:18": "the dataset. So, one way to handle this would be to decrease the number of latent factors.",
  "01:26": "But generally speaking, reducing the number of parameters in a model, particularly as",
  "01:33": "we look at more deep learning style models, ends up biasing the models towards very simple,",
  "01:41": "kind of, shapes. So there's a better way to do it. Rather than reducing the number of",
  "01:46": "parameters, instead we try to force the parameters to be smaller unless they're really required",
  "01:54": "to be big. And the way we do that is with weight decay. Weight decay is also known as",
  "02:00": "l2 regularization. They're very slightly different but we can think of them as the same thing.",
  "02:04": "And what we do is we change our loss function and specifically we change the loss function",
  "02:10": "by adding to it, the sum of all the weights squared. In fact all of the parameters squared,",
  "02:18": "really should say. Why do we do that? Well because if that's part of the loss function,",
  "02:24": "then one way to decrease the loss, would be to decrease the weights. One particular weight,",
  "02:30": "or all of the weights, or or something like that. And so when we decrease the weights,",
  "02:36": "if you think about what that would do, then think about, for example, the different possible",
  "02:49": "values of a in y equals ax squared. The larger a is for example a is 50, you get these very",
  "02:57": "narrow peaks. In general big coefficients are going to cause big swings, big changes",
  "03:06": "in the, in the loss for small changes in the parameters. And when you have these, kind",
  "03:12": "of, sharp peaks or valleys, it means that a small change to the parameter, can make",
  "03:21": "a, sorry, small change to the input, can make a big change to the loss. And so, if you have,",
  "03:27": "if you're in that situation, then you can basically fit all the data points close to",
  "03:32": "exactly with a really complex jagged function with sharp changes which exactly tries to",
  "03:38": "sit on each data point rather than finding a nice smooth surface which connects them",
  "03:44": "all together or goes through them all. So if we limit our weights, by adding in the",
  "03:53": "loss function, the sum of the weights squared, then what is going to do is, it's going to",
  "03:59": "fit less well on the training set because we're giving it less room to try anything",
  "04:03": "that it wants to but we're going to hope that it would result in a better loss on the validation",
  "04:09": "set or the test set so that it will generalize better. One way to think about this is that",
  "04:16": "the loss with weight decay is just the loss plus the sum of the parameters squared, times",
  "04:26": "number we pick, a hyper parameter, sometimes it's like .1, 0.01 or 0.001 kind of region.",
  "04:33": "This is basically what loss with weight decay looks like as an equation. But remember, when",
  "04:39": "it actually comes to what's, how was the loss used in",
  "04:42": "stochastic gradient descent it's used by taking its gradient so what's the gradient of this,",
  "04:48": "well if you remember back to when you first learn calculus, it's okay if you don't, the",
  "04:55": "gradient of something squared is just two times that's something we change from parameters",
  "05:01": "to weight which is a bit confusing though this used weight here to keep it consistent",
  "05:07": "maybe parameters is better, so the derivative of weight squared is just 2 * weight, so in",
  "05:14": "other words to add in this term to the gradient we can just add to the gradients weight decay",
  "05:24": "* 2 * weight and since weight decay is just a hyper parameter we can just replace it with",
  "05:30": "weight decay * 2 so that would just give us weight decay times weight though weight decay",
  "05:36": "refers to adding on the to the gradients the weights times some hyperparameter and so that",
  "05:47": "is going to try to create these kind of more shallow, less bumpy, surfaces do that simply",
  "05:58": "when we call fetch will fit one cycle or whatever we can pass in a wd parameter and that's just",
  "06:07": "this number here, so if we pass in point one then the training loss goes from .29 to 0.49",
  "06:16": "that's much worse right because we can't overfit anymore but the valid loss goes from 0.89",
  "06:23": "to 0.82 much better so this is an important thing to remember for those of you that have",
  "06:29": "done a lot of more traditional statistical models is in kind of more traditional statistical",
  "06:35": "models we try to avoid overfitting and we try to increase generalization by decreasing",
  "06:40": "the number of parameters but in a lot of modern machine learning and certainly deep learning",
  "06:48": "we tend to instead use regularization such as weight decay because it gives us more flexibility",
  "06:55": "it lets us use more non-linear functions and still avoid you know still reduces the capacity",
  "07:01": "of the model great so we're down to 0823 this is a good model this is really actually a",
  "07:08": "very good model and so let's dig into actually what's going on here because in our in our",
  "07:17": "architecture remember we basically just had or embedding layers or what's an embedding",
  "07:24": "layer we've described it conceptually but let's write our own and remember we said that",
  "07:29": "an embedding layer is just a computational shortcut for doing a matrix multiplication",
  "07:35": "by a one hot and coded matrix and that, that is actually the same as just indexing into",
  "07:40": "an array so it embedding is just a indexing into an array and so it's nice to be able",
  "07:50": "to create our own versions or things that exist in PyTorch and Fast.ai so let's do that",
  "07:54": "for everybody so if we're going to create our own kind of layer which is pretty cool",
  "08:00": "we need to be aware of something which is normally a layer is basically created by inheriting",
  "08:09": "as we've discussed from module or NN module so for example this is an example here of",
  "08:15": "a module where we've created a class called T that inherits from module and when it's",
  "08:20": "constructed remember that's what dunder init (__init__) does we're just going to sit this",
  "08:24": "is just a dummy little module here we're gonna set self taught a through the number one repeated",
  "08:29": "three times as a tensor, now if you remember back to notebook4 we talked about how the",
  "08:36": "optimizers in PyTorch and Fast.ai rely on being able to grab the parameters attribute",
  "08:42": "to find a list of all the parameters now if you want to be able to optimize self.a you",
  "08:48": "would need to it to appear in parameters but actually there's nothing there why is that",
  "08:54": "that's because PyTorch does not assume that everything that's in a module is something",
  "09:00": "that needs to be learnt the tell us that it's something that needs to be learned you have",
  "09:04": "to wrap it with nn.Parameter so here's exactly the same class but torch.ones which is just",
  "09:10": "a list of three ones In this case. It is wrapped in nn.Parameter().",
  "09:14": "And now if I go T().parameters(), I see I have a parameter: the three ones in it. And",
  "09:23": "that's going to automatically call requires_grad_() for us as well. We haven't had to do that",
  "09:29": "for things like nn.Linear() in the past because PyTorch automatically uses nn.Parameter()",
  "09:35": "internally. So if we have a look at the parameters for something that uses nn.Linear() with no",
  "09:41": "bias layer you'll see again we have here parameter with three things in it. So we want to in",
  "09:52": "general be able to create a parameter, a tensor with a bunch of things in it and generally",
  "09:58": "we want to randomly initialize them. So to randomly initialize we can pass in the size",
  "10:03": "we want, we can initialize a tensor of zeroes of that size, and then randomly generates",
  "10:08": "some normally distributed random numbers with a mean of 0 and a deviation of 0.01. No particular",
  "10:15": "reason I'm picking those numbers just know how this works. So here is something that",
  "10:20": "will give us back our parameters of any size we want. And so now we're going to replace",
  "10:26": "everywhere that used to say embedding, I've got to replace it with create_params. Everything",
  "10:33": "else here is the same in the __init__(). And then the forward() is very very similar to",
  "10:39": "before; as you can see I'm grabbing the zero index column from X, that's my users, and",
  "10:46": "I just look it up as you see in that user_factors array. And the cool thing is I don't have",
  "10:53": "to do anything with gradients myself for this manual embedding layer because PyTorch can",
  "10:57": "figure out the gradients automatically as we've discussed but then I just got the dot",
  "11:02": "product as before, add on the bias as before, do the sigmoid range as before, and so here's",
  "11:07": "a DotProductBias() without any special PyTorch layers and we fit and we get the same result.",
  "11:17": "So I think that is pretty amazingly cool. We've really shown that the embedding layer",
  "11:23": "is nothing fancy, is nothing magic right it's just indexing into an array. So hopefully",
  "11:29": "that removes a bit of the mystery for you. So let's have a look at this model that we've",
  "11:35": "created and we've trained and find out what it's learned. Its\ufffd already useful, we've",
  "11:40": "got something we can make pretty accurate predictions with. But let's find out what",
  "11:45": "those... what the model looks like. Remember when we create \ufffd [Rachel] we have a question.",
  "11:52": "[Jeremy]okay let's take a question before we look at this. [Rachel] what's the advantage",
  "11:56": "of creating our own embedding layer over the stock PyTorch one?. [Jeremy] oh nothing at",
  "12:02": "all. We're just showing that we can. It's great to be able to dig under the surface",
  "12:06": "because at some point you'll want to try doing new things. So a good way to learn to do new",
  "12:11": "things is to be able to replicate things that already exist and you can check you understand",
  "12:16": "how they work. It's also a great way to understand the foundations of what's going on is to actually",
  "12:21": "create in code your own implementation but I wouldn't expect you to use this implementation",
  "12:26": "in practice. But basically it removes all the mystery. So if you remember we've created",
  "12:34": "a learner called learn and to get to the model that's inside it you can always call learn.model",
  "12:41": "and then inside that there's going to be automatically created for it... well sorry not automatically...",
  "12:47": "we've created all these attributes movie_factors movie_bias user_bias and so forth...where",
  "12:53": "we can grab learn.model.movie_bias. And now what I'm going to do is I'm going to sort",
  "13:01": "that vector and I'm going to print out the first five idxs. And so what this is going",
  "13:08": "to do is it's going to print out though the movies with the smallest bias and here they",
  "13:16": "are. What does this mean? Well it kind of means these are the five movies that people",
  "13:23": "really didn't like. But it's more than that. It's not only do people not like them but",
  "13:29": "if we take account of the genre they're in the actors they have you",
  "13:34": "know whatever the latent factors are, people liked them a lot less than they expected so",
  "13:40": "maybe, for example people... this is kind of... I haven't seen any of these movies luckily",
  "13:45": "perhaps this is a sci-fi movie so people who kind of like these sci-fi movies found it",
  "13:53": "so bad they still didn't like it. So we can do the exact opposite which is to sort ascending",
  "14:00": "and here are the top five movies and specifically they're the top five by bias right. So these",
  "14:07": "are the movies that even after you take account of the fact that \ufffd LA Confidential... I",
  "14:11": "have seen all of these ones...So la confidential is a kind of a murder mystery cop movie I",
  "14:17": "guess and people who don't necessarily like that genre ...or I think Guy Pearce was in",
  "14:22": "it so maybe they don't like Guy Pearce very much whatever ...people still like this movie",
  "14:26": "more than they expect. So this is a kind of a nice thing that we can look inside our model",
  "14:33": "and see what it's learned. Now we can look out not only at the bias vector but we can",
  "14:42": "also look at the factors. Now there are 50 factors which is too many to visualize so",
  "14:48": "we could use a technique called pca principal components analysis. The details don't matter",
  "14:52": "but basically they're going to squish those 50 factors down to 3 and then we'll plot the",
  "14:58": "top two as you can see here. And what we see when we plot the top 2 is we can kind of see",
  "15:08": "that the movies have been spread out across a space of some kind of latent vectors. So",
  "15:17": "if you look at the far right there's a whole bunch of kind of big budget actiony things",
  "15:24": "and on the far left there's more like cult kind of things Fargo, Schindler's List, Monty",
  "15:31": "Python. By the same token at the bottom we've got some English Patient, Harry Met Sally",
  "15:40": "so kind of romance drama kind of stuff and at the top we've got action and sci-fi kind",
  "15:49": "of stuff. So you can see even as though we haven't passed in any information about these",
  "15:54": "movies, what we've seen is who likes what, these latent vectors have automatically kind",
  "16:02": "of figured out a space or a way of thinking about these movies based on what kinds of",
  "16:07": "movies people like and what are other kinds of movies they like along with those. That's",
  "16:12": "really interesting to kind of try and visualize what's going on inside your model. Now we",
  "16:23": "don't have to do all this manually. We can actually just say give me a collab_learner",
  "16:29": "using this set of data loaders with this number of factors in this y_range and it does everything",
  "16:35": "we've just seen again about the same number. Okay?Well now you can see... this is nice",
  "16:41": "right... we've actually been able to see right underneath inside the collab_learner part",
  "16:46": "of the FastAI application the collaborative filtering application and we can build it",
  "16:50": "all ourselves from scratch we know how to create the SGD know how to create the embedding",
  "16:56": "layer we know how to create the model the architecture so now you can see you know we've",
  "17:03": "really built up from scratch our own version of this. So if we just type learn.model you",
  "17:11": "can see here the names are a bit more generic this is a user weight item weight user bias",
  "17:16": "item bias but it's basically the same stuff we've seen before and we can replicate the",
  "17:22": "exact analyses we saw before by using this same idea okay? Slightly different order this",
  "17:31": "time because it is a bit random but pretty similar as well. Another interesting thing",
  "17:39": "we can do is we can think about the distance between two movies. So let's grab all the",
  "17:46": "movie factors or just pop them into a variable and then let's pick a movie and then let's",
  "17:57": "find the distance from that movie to every other movie and so one way of thinking",
  "18:08": "about distance is you might recall the Pythagorean formula or the distance on the hypotenuse",
  "18:15": "of a triangle which is also the distance to a point in a Cartesian plane on a chart which",
  "18:22": "is root x squared plus y squared you might know... doesn't matter if you don't... but",
  "18:27": "you can do exactly the same thing for 50 dimensions. It doesn't just work for two dimensions. There's",
  "18:34": "a... that tells you how far away a point is from another point if x and y are actually",
  "18:42": "differences between two movie vectors. So then what gets interesting is you can actually",
  "18:55": "then divide that kind of by the length to make all the lengths the same distance to",
  "19:02": "find the angle between any two movies... and that actually turns out to be a really good",
  "19:06": "way to compare the similarity of two things that's called cosine similarity ...and so",
  "19:11": "the details don't matter you can look them up if you're interested \ufffd.but the basic",
  "19:15": "idea here is to see that we can actually pick a movie and find the movie that is the most",
  "19:21": "similar to it based on these factors. Kind of interesting. [Rachel] we have a question.",
  "19:28": "[Jeremy] alright. [Rachel] what motivated learning at a 50 dimensional embedding and",
  "19:32": "then using pca to reduce to three versus just learning a few dimensions? [Jeremy] because",
  "19:35": "the purpose of this was actually to create a good model though the visualization part",
  "19:43": "is normally kind of the exploration of what's going on in your model and so with 50 latent",
  "19:50": "factors you're going to get a more accurate view. So that's one approach is this dot product",
  "19:58": "version. There's another version we could use which is we could create a set of user_factors",
  "20:08": "and a set of item_factors and the just like before we could look them up. But what we",
  "20:15": "could then do instead of doing a dot product is we could concatenate them together into",
  "20:20": "a tensor that contains both the user and the movie factors next to each other and then",
  "20:28": "we could pass them through a simple little neural network: Linear, ReLU, Linear and then",
  "20:37": "sigmoid_range as before. So importantly here the first linear layer the number of inputs",
  "20:43": "is equal to the number of user_factors plus the number of item_factors and the number",
  "20:49": "of outputs is however many activations we have and which we just default to a hundred",
  "20:59": "here and then the final layer will go from a hundred to one because we're just making",
  "21:02": "one prediction. So we could create or call that collabNN and we can instantiate that",
  "21:08": "to create a model we can create a learner and we can get. It's not going quite as well",
  "21:15": "as before it's not terrible but it's not quite as good as our dot product version. But the",
  "21:21": "interesting thing here is it does give us some more flexibility which is that since",
  "21:25": "we're not doing a dot product we can actually have a different embedding size for each of",
  "21:32": "users versus items and actually FastAI has a simple heuristic if you call get_emb_sz()",
  "21:38": "and pass in your data loaders it will suggest appropriate size embedding matrices for each",
  "21:46": "of your categorical variables, each of your user and item enters. so that's if we pass",
  "22:00": "in *emb that's going to pass in the user tuple and the item tuple which we can then pass",
  "22:09": "to embedding. This is star prefix we learned about in the last class in case you forgot.",
  "22:15": "So this is kind of interesting we can you know we can see here that there's two different",
  "22:23": "architectures we could pick from, it wouldn't be necessarily obviously ahead of time which",
  "22:26": "one's going to work better, I mean in this particular case the simplest one the the dot",
  "22:32": "product one actually turned out to work a bit better which is interesting. This particular",
  "22:36": "version here if you call, \ufffdcollab_learner\ufffd and pass",
  "22:40": "use_nn=True, then what that's going to do is, it's going to use this version, the version",
  "22:46": "with concatenation and the linear layers. So \ufffdcollab_learner\ufffd use_nn=True, again",
  "22:57": "we get about the same result as you'd expect, because it's just a raw cut for this version,",
  "23:04": "and it's interesting actually, we have a look at \ufffdcollab_learner\ufffd, it actually returns",
  "23:10": "an object of type \ufffdembeddingNN\ufffd, and it's kind of cool, if you look inside the fast",
  "23:14": "AI source code or use the double question mark trick to see the source code for embeddingNN,",
  "23:19": "you'll see it's three lines of code. How does that happen? Because we're using this thing",
  "23:25": "called \ufffdTabularModel\ufffd, which we will learn about in a moment, but basically this neural",
  "23:33": "net version of collaborative filtering is literally just a \ufffdTabularModel\ufffd in which",
  "23:38": "we pass no continuous variables and embedding sizes. So we'll see that in a moment. OK,",
  "23:50": "so that is collaborative filtering and again take a look at the further research section",
  "23:55": "in particular, after you finish the questionnaire and because there's some really important",
  "24:00": "next steps you can take to push your knowledge and your skills. So let's now move to Notebook",
  "24:09": "9 Tabular, and we're going to look at tabular modeling and do a deep dive, and let's start",
  "24:16": "by talking about this idea that we were starting to see here, which is embeddings, and specifically",
  "24:25": "let's move beyond just having embeddings for users and items, at embeddings for any kind",
  "24:32": "of categorical variable, but really because we know an embedding is just a lookup into",
  "24:38": "an array, it can handle any kind of discrete categorical data. So things like age are not",
  "24:47": "discrete they\ufffdre continuous numerical data, but something like sex or postcode are categorical",
  "24:54": "variables, they have a certain number of discrete levels. The number of discrete levels they",
  "24:59": "have is called their cardinality. So to have a look at an example of a data set that contains",
  "25:07": "both categorical and continuous variables, we're going to look at the \ufffdRossman sales",
  "25:13": "competition\ufffd that ran on Kaggle a few years ago, and so basically what's going to happen",
  "25:18": "is we're going to see a table that contains information about various stores in Germany,",
  "25:24": "and the goal will be to try and predict how many sales there's going to be, for each day",
  "25:29": "in a couple of weeks period, for each store. One of the interesting things about this competition",
  "25:37": "is that one of the gold medalists used deep learning, and it was one of the earliest known",
  "25:43": "example of a state of the art deep learning tabular model. I mean, this is not long ago",
  "25:49": "2015 or something, but really this idea of creating state of the art tabular models with",
  "25:55": "deep learning has not been very common, and for not very long. You know, interestingly",
  "26:02": "compared to the other gold medalists in this competition, the folks that used deep learning",
  "26:07": "used a lot less feature engineering, and a lot less domain expertise, and so they wrote",
  "26:11": "a paper called \ufffdEntity Embeddings of Categorical Variables\ufffd, in which they basically described",
  "26:18": "the exact thing that you saw in the in Notebook 8, the way you can think of one hot encodings",
  "26:25": "as just being embeddings you concatenate them together and you can put them through a couple",
  "26:30": "of layers, they call them dense layers, we call them linear layers, and create a neural",
  "26:36": "network out of that. So this is really neat, you know, it's, kind of, simple and obvious,",
  "26:43": "in hindsight, trick, and they actually did exactly what we did in the paper, which is",
  "26:48": "to look at the results of the trained embeddings, and so for example they had an embedding matrix",
  "26:56": "for regions in Germany, because there was... What there wasn't really metadata about this,",
  "27:04": "these were just learnt embeddings, just like we learnt embeddings about movies and so",
  "27:08": "then they just created, just like we did before, a chart where they popped each region, according",
  "27:14": "to, I think probably a PCA of their embeddings, and then if you circle the ones that are close",
  "27:21": "to each other, in blue, you'll see that they're actually close to each other in Germany, and",
  "27:27": "ditto for red, and ditto for green, and then here's the brown. So this is, like, pretty",
  "27:35": "amazing, is the way that we can see that it's, kind of, learnt something about what Germany",
  "27:41": "looks like based entirely on the purchasing behavior of people in those states. Something",
  "27:47": "else they did was to look at every store, and they looked at the distance between stores",
  "27:55": "in practice, like how many kilometers away they are, and then they looked at the distance",
  "27:59": "between stores in terms of their embedding distance, just like we saw in the previous",
  "28:05": "notebook, and there was this very strong correlation, that stores that were close to each other",
  "28:11": "physically, ended up having close embeddings as well, even as though the actual location",
  "28:21": "of these stores in physical space was not part of the model. Ditto with days of the",
  "28:27": "week. So the days of the week are another embedding, and the days of the week that were",
  "28:33": "next to each other, ended up next to each other in embedding space, and ditto for months",
  "28:39": "of the year. So pretty fascinating the way, kind of, information about the world ends",
  "28:46": "up captured just by looking at browning embeddings, which as we know are just index lookups into",
  "28:53": "an array. So the way we then combine these categorical variables, with these embeddings,",
  "29:01": "with continuous variables... What was done in both the entity embedding paper, that we",
  "29:08": "just looked at, and then also described in more detail by Google, when they described",
  "29:14": "how their recommendation system in Google Play works. This is from Google's paper. As",
  "29:19": "they have the categorical features, that go through the embeddings, and then there are",
  "29:24": "continuous features, and then all the embedding results, and the continuous features are just",
  "29:28": "concatenated together, into this big concatenated table, that then goes through, in this case,",
  "29:33": "three layers of a neural net, and interestingly they also take the, kind of, collaborative",
  "29:41": "filtering bit, and do the dot product as well and combine the two. So they use both of the",
  "29:46": "tricks we used in the previous notebook and combine them together. So that's the basic",
  "29:54": "idea we're going to be seeing or proving beyond just collaborative filtering, which is just",
  "30:01": "two categorical variables, to as many categorical and as many continuous variables as we like,",
  "30:06": "but before we do that let's take a step back and think about other approaches, because",
  "30:13": "as I mentioned, the idea of deep learning as a, kind of a, best practice for tabular",
  "30:19": "data, it is still pretty new and it's still kind of controversial. It's certainly not",
  "30:25": "always the case but it's the best approach. So when we're not using deep learning, what",
  "30:31": "would we be using? Well what would probably be using is something called an ensemble of",
  "30:36": "decision trees, and the two most popular are Random Forests, and Gradient Boosting Machines",
  "30:43": "or something in the line. So basically between multi-layered neural networks learnt with",
  "30:48": "SGD, and ensembles of decision trees that, kind of, covers the vast majority of approaches",
  "30:55": "that you're likely to see for tabular data, and so we're going to make sure we cover them",
  "30:59": "both, of course. Today in fact. So although deep learning is nearly always clearly superior",
  "31:08": "for stuff like images, and audio, and natural language texts, these two approaches tend",
  "31:15": "to give somewhat similar results a lot of the time for tabular data. So let's take a",
  "31:21": "look. Some\ufffd You know, you really should generally try both and see which works best",
  "31:24": "for you for each problem you look at. Why does the range go from 0 to 5.5, if the",
  "31:35": "maximum is 5? That's a great question. Um, the reason is, if you think about it, for",
  "31:41": "sigmoid it's actually impossible for a sigmoid to get all the way to the top or all the way",
  "31:48": "to the bottom: there's a asymptote, so, no matter how far, how big your X is, it can",
  "31:54": "never quite get to the top, or no matter how small it is, it can never quite get to...so",
  "31:58": "if you want to be able to actually predict a rating of 5, then you need to use something",
  "32:03": "higher than 5 as your maximum. Are embeddings used only for highly cardinal categorical",
  "32:11": "variables, or is this approach general? For low cardinality can one use one-hot encoding?",
  "32:20": "I\ufffdll remind you cardinality is the number of discrete levels in variable, and remember",
  "32:29": "that an embedding is just a computational shortcut for a one-hot encoding, so there's",
  "32:36": "really no reason to use a one-hot encoding because, as long as you have more than two",
  "32:43": "levels, it's always going to be more memory and slower, and give you exactly mathematically",
  "32:48": "the same thing. And if there's just two levels, then it is basically identical. So there isn't",
  "32:54": "really any reason not to use it. Thank you for those great questions. Okay, so, one of",
  "33:07": "the most important things about decision tree ensembles is that at the current state of",
  "33:13": "the technology, they do provide faster and easier ways of interpreting the model. I think",
  "33:18": "that's rapidly improving for deep learning models on tabular data, but that's where we",
  "33:22": "are right now. They also require less hyperparameter tuning, so they're easier to kind of get right",
  "33:28": "the first time, so my first approach analyzing a tabular dataset is always an ensemble of",
  "33:35": "decision trees, and specifically I pretty much always start with a random forest, because",
  "33:39": "it's just so reliable. Yes! Your experience for highly imbalanced datasets such as fraud",
  "33:48": "or medical data, what usually is best out of random forest, XGboost or neural networks?",
  "33:55": "Um, I'm not sure that whether the data is balanced or unbalanced is a key reason for",
  "34:03": "choosing one of those about the others. I would try all of them and see which works",
  "34:06": "best. So the exception to the guideline about starting with decision tree ensembles as your",
  "34:13": "first thing to try, would be if there's some very high cardinality categorical variables,",
  "34:18": "then they can be a bit difficult to get to work really well in decision tree ensembles.",
  "34:25": "Or if there's something like, most importantly if it's like plain text data or image data",
  "34:29": "or audio data or something like that, and then you'll definitely kind of need to use",
  "34:34": "a neural net in there. But you could actually ensemble it with a random forest as we'll",
  "34:38": "see. Okay, so, really we're going to need to understand how decision tree ensembles",
  "34:47": "work. So, PyTorch isn't a great choice for decision tree ensembles: they're really designed",
  "34:54": "for gradient-based methods and random forests and decision tree growing are not really gradient-based",
  "35:01": "methods in the same way. So, instead we're going to use a library called scikit-learn,",
  "35:07": "referred to as sklearn as a module. Scikit-learn does a lot of things, we're only going to",
  "35:16": "touch on a tiny piece of them, what we need to do to train decision trees and random forests.",
  "35:23": "We've already mentioned before Wes McKinney's book, also a great book for understanding",
  "35:29": "more about scikit-learn. So, the dataset for learning about decision tree ensembles is",
  "35:36": "going to be another dataset, it's going to, it's called the Blue Book for Bulldozers dataset,",
  "35:43": "and it's a Kaggle competition. So, Kaggle competitions are fantastic, they are machine",
  "35:51": "learning competitions where you get interesting datasets, you get feedback on whether your",
  "35:56": "approach is any good or not, you can see on a leaderboard what approaches are working",
  "36:00": "best and then you can read blog posts from the winning",
  "36:02": "contestants sharing tips and tricks. It's certainly not a substitute for actual practice",
  "36:14": "doing end-to-end data science projects, but for becoming good at creating predictive models",
  "36:21": "that are predictive it's a really fantastic resource. Highly recommended, and you can",
  "36:27": "also submit to most old competitions to see how you would have gone without having to",
  "36:32": "worry about, you know, the kind of stress of like whether people will be looking at",
  "36:36": "your results, because they're not publicized or published if you do that. There\ufffds a question:",
  "36:44": "can you comment on real time applications of random forests? In my experience they tend",
  "36:50": "to be too slow for real-time use cases, like a recommender system neural network is much",
  "36:56": "faster when run on the right hardware. Let's get to that once we see what they are, shall",
  "37:04": "we? Now you can't just download and untar Kaggle datasets using the untar data thing",
  "37:12": "that we have in fastai, so you actually have to sign up to Kaggle and then follow these",
  "37:16": "instructions for how to download data from Kaggle. Make sure you replace creds here with",
  "37:23": "what it describes; you need to get a special API code and then run this one time to put",
  "37:29": "that up on your server. And now you can use Kaggle to download data using the API. So",
  "37:39": "after we do that we're going to end up with a bunch of, as you see, CSV files. So let's",
  "37:46": "take a look at this data. So the main data, the main table, is trained on CSV -- remember",
  "37:54": "that's comma separated values -- and the training set contains information such as unique identifier",
  "38:00": "of the sale, or the unique identifier of the machine, sale price sale date. So what's going",
  "38:05": "on here is one row of the data represents a sale of a single piece of heavy machinery",
  "38:13": "like a bulldozer at an auction. So it happens at a date, has a price, it's of some particular",
  "38:19": "piece of equipment and so forth. So if we use pandas again to read in the CSV file (let's",
  "38:27": "combine training and valid together) we can then look at the columns to see. There's a",
  "38:33": "lot of columns there and many things which I don't know what the hell they mean, like",
  "38:37": "blade extension and pad type and ride control, but the good news is we're going to show you",
  "38:43": "a way that you don't have to look at every single column and understand what they mean,",
  "38:47": "and random forests are going to help us with that as well. So once again we're going to",
  "38:52": "be seeing this idea that models can actually help us with data understanding and data cleanup.",
  "38:59": "One thing we can look at is ordinal columns, a good place to look at that now. If there's",
  "39:03": "things there that you know are discrete values that have some order like product size it",
  "39:10": "has medium and small and large medium and mini, these should not be in you know alphabetical",
  "39:17": "order or some random order; they should be in this specific order, right? They have a",
  "39:23": "specific ordering so we can use .astype to turn it into a categorical variable, and then",
  "39:32": "we can say set categories ordered equals true, to basically say this is an ordinal column.",
  "39:38": "So it's got discrete values, but we actually want to define what the order of the classes",
  "39:43": "are. We need to choose which is the dependent variable and we do that by looking on Kaggle,",
  "39:51": "and Kaggle will tell us that the thing we're meant to be predicting is sale price. And",
  "39:55": "actually, specifically, they'll tell us the thing we're meant to be predicting is the",
  "39:58": "log of sale price, because root mean squared log error is the -- what we're actually going",
  "40:05": "to be judged on in the competition, where we take the log. So we're going to replace",
  "40:10": "sale price with its log, and that's what we'll be using from now on. So a decision tree ensemble",
  "40:17": "requires decision trees -- well let's start by looking at decision trees. So a decision",
  "40:22": "tree in this case is a, something that asks a series of binary, that is yes or no, questions",
  "40:30": "about data. So such as is somebody less than 30? Yes they are. Are they eating healthily?",
  "40:37": "Yes they are and so okay, then we're going to say they're fit or unfit. So like there's",
  "40:43": "an example of some arbitrary decision tree that somebody might have come up with. It's",
  "40:48": "a series of binary yes and no choices and at the bottom are leaf nodes that make some",
  "40:54": "prediction. Now of course for our bulldozers competition we don't know what binary questions",
  "41:06": "to ask about these things, and in what order, in order to make a prediction about sale price.",
  "41:12": "So we're doing machine learning, so we're going to try and come up with some automated",
  "41:17": "way to create the questions. And there's actually a really simple procedure for doing that.",
  "41:23": "You have a think about it. So if you want to kind of stretch yourself here, have a think",
  "41:28": "about what's an automatic procedure that you can come up with that would automatically",
  "41:33": "build a decision tree, where the final answer would do a, you know, significantly better",
  "41:39": "than random job of estimating the sale price of one of these options. Right so here's the",
  "41:51": "approach that we could use. Loop through each column of the data set. We're going to go",
  "41:56": "through each of (but obviously not sale prices, the dependent variable) sale ID, machine ID,",
  "42:03": "auctioneer, year made etc. And so one of those will be, for example, product size And so",
  "42:10": "then what we're going to do is we're going to loop through each possible value of product",
  "42:18": "size (large, large-medium, medium, etc.) and then we're going to do a split basically like",
  "42:23": "where this comma is. And we're going to say okay let's get all of the options of large",
  "42:28": "equipment and put that into one group, and everything that's smaller than that and put",
  "42:35": "that into another group, and so that's here: split the data into two groups based on whether",
  "42:42": "they're greater than or less than that value. If it's a categorical non-ordinal variable,",
  "42:48": "it'll be just whether it's equal or not equal to that level. And then we're going to find",
  "42:54": "the average sale price for each of the two groups. So for the large group what is the",
  "42:59": "average sale price? For the smaller than large group what was the average sale price, and",
  "43:04": "that will be our model. Our prediction will simply be the average sale price for that",
  "43:10": "group. And so then you can say, well how good is that model? If our model was just to ask",
  "43:15": "a single question with a yes/no answer, put things into two groups and take the average",
  "43:20": "of the group as being our prediction, and we can see how good would that model be? What",
  "43:24": "would be the root mean squared error from that model? And so we can then say all right,",
  "43:30": "how good it would it be if we used large as a split. And then let's try again -- what",
  "43:35": "if we did large/medium as a split? What if we did medium as a split? And so in each case",
  "43:40": "we can find the root mean squared error of that incredibly simple model. And then once",
  "43:45": "we've done that for all of the product size levels, we can go to the next column and look",
  "43:49": "at -- have a look -- UsageBand and do every level of UsageBand. And then state -- your",
  "43:58": "level or state and so forth. And so there'll be some variable and some split level which",
  "44:06": "gives the best root mean squared error of this really, really simple model. And so then",
  "44:11": "I'll say okay that would be our first binary decision:, it gives us two groups and then",
  "44:18": "we're going to take each one of those groups separately and find another single binary",
  "44:24": "decision for each of those two groups using exactly the same procedure. And then we'll",
  "44:30": "have four groups, and then we'll do exactly the same thing again separately for each of",
  "44:34": "those four groups, and so forth. So let's see what that looks like. And, in fact, once",
  "44:44": "we\ufffdve gone through this, you might even want to see if you can implement",
  "44:47": "this algorithm yourself. It's not trivial but it doesn't require any special coding",
  "44:53": "skills so hopefully you can find you will be able to do it. There's a few things we",
  "45:00": "have to do before we can actually create a decision tree in terms of just some basic",
  "45:03": "data munging. One is if we're going to take advantage of dates, we actually want to call",
  "45:12": "fastai\ufffds add_date_part function. And what that does is it creates a whole bunch of different",
  "45:19": "bits of metadata from that data. Sale year, sale month, sale a week, sale day and so forth.",
  "45:27": "The sale date, of itself doesn't have a whole lot of information directly, but we can pull",
  "45:36": "lots of different information out of it. This is an example of something called feature",
  "45:40": "engineering, which is where we take some piece of data and we try to create lots of other",
  "45:47": "pieces of data from it. So is this particular date at the end of a month, or not? At the",
  "45:52": "end of a year or not? And so forth, but that handles dates. There's a bit more cleaning",
  "46:00": "we want to do, and fastai provides some things to make that easier. We can use the TabularPandas",
  "46:07": "class. We create a tabular data set in pandas, and specifically we're going to use two tabular",
  "46:17": "processors, (procs). A tabular processor is basically just to transform, and we\ufffdve seen",
  "46:23": "transforms before, so go back and remind yourself what a transform is. In, like three lines",
  "46:30": "of code it\ufffds actually going to modify the object in place rather than creating a new",
  "46:37": "object and giving it back to you. That's because often these tables of data are kind of really",
  "46:42": "big and we don't want to waste lots of RAM. And it's just gonna run the transform once",
  "46:48": "and save the result rather than doing it lazily when you access it, but for the same reason",
  "46:52": "we just want to make this a lot faster, so just think of them as transforms really. So",
  "46:59": "one of them is called categorify, which is going to replace a column with numeric categories",
  "47:06": "using the same basic idea like a vocab like we've seen before. fill_missing is going to",
  "47:14": "find any columns with missing data and it's going to fill in the missing data with the",
  "47:18": "median of the data and additionally create a new column, a boolean column which is set",
  "47:22": "to True for anything that was missing. These two things are basically enough to get you",
  "47:28": "to a point where most of the time you'll be able to train a model. Now the next thing",
  "47:33": "we need to do is think about our validation set as we discussed in lesson one, a random",
  "47:41": "validation set is not always appropriate, and certainly for something like predicting",
  "47:45": "auction results it almost certainly is not appropriate because we're going to be wanting",
  "47:50": "to use a model in the future, not at some random date in the past. So the way this Kaggle",
  "47:57": "competition was set up was that the test set, the thing that you had to fill in and submit",
  "48:02": "for the competition, was two weeks of data that was after any of the training set. So",
  "48:10": "we should do the same thing for the validation set. We should create something which is where",
  "48:16": "the validation set is the last couple of weeks of data, and so then the training set will",
  "48:24": "only be data before that. So we basically can do that by grabbing everything before",
  "48:29": "October 2011. I'll create our training and validation sets based on that condition, and",
  "48:36": "grabbing those bits. So that's going to spit our training set and validation set by date,",
  "48:45": "not randomly. We're also going to need to tell when you create a TabularPandas object,",
  "48:51": "you're going to be passing in a data frame, your tabular procs, and you also have to say",
  "48:57": "what are your categorical and continuous variables. We can use fastai\ufffds conte_cat_split(), which",
  "49:04": "automatically splits a data frame to continuous and categorical variables for you. So we can",
  "49:10": "just pass those in, tell it what is the dependent variable -- you can have more than one -- and",
  "49:17": "what are the indexes to split into training and validation sets. And this is a Tabular",
  "49:22": "object, so it's got all the information you need about the training set ,the validation",
  "49:26": "set, the categorical and continuous variables, and the dependent variable, and any processes",
  "49:31": "to run. It looks a lot like a Datasets object but has .train and .dot valid properties,",
  "49:41": "and so if we have a look at .show(), we can see the data. But .show() is going to show",
  "49:50": "us the string data, but if we look at .items() you can see internally it's actually stored",
  "49:57": "in these very compact numbers which we can use directly in a model.",
  "50:06": "So fastai has basically got us to a point where our training and validation set is created",
  "50:11": "and the data is in a format ready for modeling. To see how these numbers relate to these strings",
  "50:20": "we can again, just like we saw last week, use the .classes attribute which is a dictionary,",
  "50:25": "it basically tells us the vocab. So this is how we look up, for example that ProductSize",
  "50:31": "category 6 is \ufffdCompact\ufffd. That processing takes a little while to run, so you can go",
  "50:40": "ahead and save the Tabular object, so then you can load it back later without having",
  "50:47": "to rerun all the processing. So that's a nice kind of fast way to quickly get back up and",
  "50:53": "running without having to reprocess your data. So we've done the basic data munging we need.",
  "51:00": "So we can now create a decision tree, and in scikit-learn the decision tree model where",
  "51:04": "the dependent variable is continuous is called a DecisionTreeRegressor. Now let's start by",
  "51:10": "telling it we just want a total of four leaf nodes (we'll see what that means in a moment).",
  "51:18": "In scikit-learn you generally call fit so it looks quite a lot like fastai. And you",
  "51:23": "pass in your independent variables and your dependent variable, and we can grab those",
  "51:28": "straight from our TabularObject training set. It\ufffds train.xs and train.y and we can do",
  "51:34": "the same thing for validation. Question: do you have any thoughts on what",
  "51:41": "data augmentation for tabular data might look like?",
  "51:46": "Answer: I don't have a great sense of data augmentation with tabular data. We'll be seeing",
  "51:59": "later either in this course, or in the next part ,dropout and mixup and stuff like that,",
  "52:05": "which they might be able to do that in later layers in the tabular model. Otherwise I think",
  "52:13": "you'd need to think about the semantics of the data, and think about what are things",
  "52:17": "you could do to change the data without changing the meaning.",
  "52:22": "Question: does fastai distinguish between ordered categories such as low, medium, high,",
  "52:34": "and unordered categories? Answer: that that was that ordinal thing I",
  "52:37": "told you about before and all it really does is it ensures that your classes list has a",
  "52:43": "specific numerical order, and as you'll see that's actually going to turn out to be pretty",
  "52:50": "important for how we train our Random Forest. Okay so we can create a DecisionTreeRegressor,",
  "52:58": "we can fit it, and then we can draw it, using the fastai function draw_tree(), and here",
  "53:05": "is the decision tree we just trained. Behind the scenes this actually used basically the",
  "53:16": "exact process that we described back here. Right so this is where you can try and create",
  "53:23": "your own decision tree implementation if you're interested in stretching yourself. So we're",
  "53:30": "going to use an implementation that already exists, and the best way to understand what",
  "53:33": "it's done is to look at this diagram from top to bottom. So this first step says the",
  "53:41": "initial model it created is a model with no binary splits at all. Specifically it's always",
  "53:48": "going to predict the value 10.1, for every single row. Why is that? Well, because this",
  "53:55": "is the simplest possible model -- to always predict the average of the dependent variable.",
  "54:01": "So this pretty much your baseline for regression. There are 404,710 rows that we're averaging,",
  "54:14": "and the mean squared error (MSE) of this incredibly simple model in which there are no rules at",
  "54:20": "all is just a single average is 0.48. So then, the next most complex model is to take a single",
  "54:30": "column, Coupler_System, and a single binary decision -- is Coupler_System less than or",
  "54:37": "equal to 0.5? There are 360,847 examples where it's True and 43,863 examples where it's False.",
  "54:46": "And now interestingly in the False case (right side) you can see that there are no further",
  "54:55": "binary decisions, so this is called a leaf node -- it's a node where this is as far as",
  "55:00": "you can get. And so if your Coupler_System is not less than or equal to 0.5 (False) then",
  "55:07": "the prediction this model makes for the sale price is 9.21, versus 10.1 if it's True. So",
  "55:14": "you can see it's actually found a very big difference here and that's why it picked this",
  "55:19": "as the first binary split. And so the mean squared error for this section here is 0.12",
  "55:26": "which is far better than we started out at. The left branch, which is Coupler_System less",
  "55:30": "than 0.5, has 360,847 examples in it. Next it does another binary split, this time on",
  "55:36": "YearMade, asking whether this piece of equipment was made before 1991.5; if the answer isTrue,",
  "55:45": "then we get a leaf node and the predicted price is 9.97, with mean squared error 0.37.",
  "55:52": "If the value is False we don't have a leaf node and we have another binary split. And",
  "55:57": "you can see eventually we get down to here Coupler_System True, YearMade False, ProductSize",
  "56:02": "False, mean squared error 0.17. So all of these leaf nodes have MSEs that are smaller",
  "56:10": "than that original baseline model of just taking the mean. So this is how you can grow",
  "56:18": "a decision tree, and we only stopped here because we set max_leaf_nodes=4. And so if",
  "56:26": "we want to keep training it further we can just use a higher number. There's actually",
  "56:33": "a very nice library by Terence Parr called dtree_viz, which can show us exactly the same",
  "56:41": "information, and so here are the same leaf nodes, and you can see the chart of how many",
  "56:51": "are there. This is the split of Coupler_System at 0.5, here are the two groups, you can see",
  "56:56": "the sale price in each of the two groups, and then here's the leaf node. And then the",
  "57:01": "second split was on YearMade. And you can see here something weird\ufffds going on with",
  "57:06": "YearMade -- there's a whole bunch of YearMade = 1000, which is obviously not a sensible",
  "57:11": "year for a bulldozer to be made, so presumably that's some kind of missing value. So looking",
  "57:18": "at a graph representation like this can give us some insights about what's going on in",
  "57:22": "our data, and so maybe we should replace those thousands with 1950, because that's you know",
  "57:31": "obviously a very very early year for a bulldozer, and so we can kind of pick it arbitrarily.",
  "57:37": "It's actually not really going to make any difference to the model that's created, because",
  "57:42": "all we care about is the order, because we're just doing these binary splits. Bit it\ufffdll",
  "57:46": "make it easier to look at as you can see -- here's our YearMade=1950 cluster now, and so now",
  "57:52": "it's much easier to see what's going on in that binary split.",
  "57:57": "So, let's now get rid of max_leaf_nodes, and build a bigger decision tree. And then let's",
  "58:08": "create a couple of little functions -- one to create the root mean squared error, which",
  "58:12": "is just here -- and another one to take a model, some independent variables and a dependent",
  "58:19": "variable, predict from the model on the independent variables, and then take the root mean squared",
  "58:23": "error with the dependent variable. so that's going to be our model\ufffds root mean squared",
  "58:28": "error. So for this decision tree, in which we didn't have a stopping criteria -- allowing",
  "58:33": "as many leaf nodes as you like -- the model\ufffds root mean squared error is zero! So we've",
  "58:39": "just built the perfect model. So this is great news, right? We have built the perfect option",
  "58:48": "trading system. Well, remember that we actually need to check the performance on the validation",
  "58:52": "set. Let's check the MSE with a validation set, and oh it's worse than zero. So our training",
  "59:00": "set has MSE = 0, but our validation set is much worse than zero. Why has that happened?",
  "59:06": "Well one of the things that a random forest in sklearn can do is it can tell you the number",
  "59:11": "of leaf nodes. There are 341,000 leaf nodes with 400,00 data points, so in other words",
  "59:22": "we have nearly as many leaf nodes as data points! Most of our leaf nodes only have a",
  "59:26": "single thing in them, so they're taking an average of a single thing. Clearly this makes",
  "59:30": "no sense at all, so what we should actually do is pick some different stopping criteria.",
  "59:37": "Let's adopt the rule: don't split at a node if that would create a leaf node with less",
  "59:46": "than 25 things in it. And now if we fit, and we look at the root mean squared error for",
  "59:52": "the validation set, it's going to go down from 0.33 to 0.32. So the training sets got",
  "59:59": "worse from 0.2248 but the validation set got better, and now we only have 12,000 leaf nodes",
  "60:07": "so that is much more reasonable. All right so let's take a five-minute break,",
  "60:14": "and then we're going to come back and see how we get the best of both worlds. How are",
  "60:17": "we going to get something which has the kind of flexibility to get",
  "60:27": "really deep trees, but also without overfitting. And the trick will be to use something called",
  "60:33": "bagging. We\ufffdll come back and talk about that in five minutes.",
  "60:38": "Okay welcome back, so we're going to look at how we can get the best of both worlds",
  "60:50": "as we discussed. Let's start by having a look at what we're doing with categorical variables,",
  "60:57": "first of all. So you might notice that previously, with categorical variables, for example in",
  "61:04": "collaborative filtering, we had to think about how many embedding levels we have. For example,",
  "61:14": "if you've used other modeling tools you might have experience with creating dummy variables.",
  "61:19": "With Random Forests, you don't have to. The reason is that as we've seen, all of our categorical",
  "61:31": "variables have been turned into numbers. And so we can perfectly well have decision tree",
  "61:39": "binary decisions which use those particular numbers. Now the numbers might not be ordered",
  "61:48": "in any interesting way, but if there's a particular level which kind of stands out as being important,",
  "61:56": "it only takes two binary splits to split out that level into a single piece. So generally",
  "62:07": "speaking, I don't normally worry too much kind of about encoding categorical variables",
  "62:15": "in a special way, except that as I mentioned, I do try to encode ordinal variables by seeing",
  "62:20": "what the order of the levels is, because often as you would expect sizes, for example, you",
  "62:27": "know medium and small are gonna mean kind of next to each other, and large and extra-large",
  "62:32": "would be next to each other. That's good to have those as similar numbers. Having said",
  "62:38": "that, you can one-hot encode a categorical variable if you want to, using get_dummies",
  "62:46": "in pandas, but there's not a lot of evidence that that actually helps. That has actually",
  "62:52": "been explored in a paper, and so I would say in general for categorical variables, don't",
  "62:58": "worry about it too much, just use what we've shown you.",
  "63:02": "Do you have a question? For ordinal categorical variables, how do you deal with, when they",
  "63:10": "have like n/a or missing values, where do you put that in the order? So in fast.ai,",
  "63:19": "na, missing values always appear as the first item; they\ufffdll always be the 0 index item.",
  "63:26": "And also if you get something in the validation or test set which was a level we haven't seen",
  "63:31": "in training, that will be considered to be that missing or na value as well. All right.",
  "63:38": "So what we're gonna do to try and improve our random forest is we're going to use something",
  "63:45": "called bagging. And this was developed by a retired Berkeley professor named Leo Breiman",
  "63:51": "in 1994. And he did a lot of great work, and perhaps you could see that most of it happened",
  "63:57": "after he retired. His technical report was called bagging predictors and he described",
  "64:04": "how you could create multiple versions of a predictor, so multiple different models.",
  "64:09": "And you could then aggregate them by averaging over the predictions. And specifically, the",
  "64:17": "way he suggested doing this was to create what he called bootstrap replicas. In other",
  "64:23": "words, randomly select different subsets of your data, train a model on that subset and",
  "64:29": "store it away as one of your predictors. And then do it again a bunch of times. And so",
  "64:34": "each of these models is trained on a different random subset of your data. And then you (to",
  "64:41": "predict you) predict on all of those different versions of your model and average them. And",
  "64:48": "it turns out that bagging works really well. So this, the sequence of steps is basically:",
  "64:55": "randomly choose some subset of rows, train a model using that subset, save that model",
  "65:02": "and then return to Step 1. Do that a few times to train a few models. And then to make a",
  "65:08": "prediction, predict with all the models and take the average. That is bagging. And it's",
  "65:15": "very simple but it\ufffds amazingly powerful. And the reason why is that each of these models",
  "65:21": "we've trained, although they are not using all of the data, so they're kind of less accurate",
  "65:29": "than a model that uses all of the data, each of them is \ufffd the errors are not correlated.",
  "65:37": "You know, the errors because they\ufffdre using that smaller subset are not correlated with",
  "65:42": "the errors of the other models. Because they're random subsets. And so when you take the average",
  "65:50": "of a bunch of kind of errors which are not correlated with each other, the average of",
  "65:57": "those errors is zero. So, therefore, the average of the models should give us an accurate prediction",
  "66:06": "of the thing we're actually trying to predict. So as I say here, it\ufffds an amazing result",
  "66:10": "- we can improve the accuracy of nearly any kind of algorithm by training it multiple",
  "66:15": "times on different random subsets of data and then averaging the predictions. So then",
  "66:23": "Breimen in 2001 showed a way to do this specifically for decision trees where not only did he randomly",
  "66:30": "choose a subset of rows for each model. But then for each binary split, he also randomly",
  "66:37": "selected a subset of columns. And this is called the random forest. And it's perhaps",
  "66:44": "the most widely used most practically important machine learning method, and astonishingly",
  "66:48": "simple. To create a random forest regressor you use sklearn\ufffds RandomForestRegressor.",
  "66:57": "If you pass n jobs minus one it will use all of the CPU cores that you have to run as fast",
  "67:03": "as possible. n_estimators says how many trees, how many",
  "67:08": "models to train. max samples says how many rows to use - randomly chosen rows to use",
  "67:15": "in each one. max_features is how many randomly chosen columns to use for each binary split",
  "67:22": "point. min_samples_leaf is the stopping criteria and we'll come back to that. So here's a little",
  "67:31": "function that will create a random first regressor and fit it to some set of independent variables",
  "67:37": "and a dependent variable. So we can give it a few default values and create a random forest",
  "67:46": "and train. And our validation set RMSE is 0.23. If we compare that to what we had before,",
  "67:59": "you, we had 0.32 - so dramatically better by using a random forest. So, so, what's happened",
  "68:17": "when we called RandomForestRegressor is it's just using that decision tree builder that",
  "68:23": "we've already seen but it's building multiple versions with these different random subsets.",
  "68:28": "And for each binary split it does it's also randomly selecting a subset of columns. And",
  "68:35": "then when we create a prediction it is averaging the predictions of each of the trees. And",
  "68:41": "as you can see it's giving a really great result. And one of the amazing things we'll",
  "68:46": "find is that it's gonna be hard for us to improve this very much. You know, the kind",
  "68:51": "of the default starting point tends to turn out to be pretty great. The, the sklearn docs",
  "69:00": "have lots of good information, and one of the things it has is this nice picture that",
  "69:04": "shows as you increase the number of estimators, how does the accuracy improve, error rate",
  "69:12": "improve for different max_features levels. And in general the more trees you add, the",
  "69:20": "more accurate your model. There, the, it's not going to overfit, right, because it's",
  "69:25": "averaging more of these, these week models, more of these models that are trained on subsets",
  "69:34": "of the data. So train as many, use as many estimators as you like. It\ufffds really just",
  "69:40": "a case of how much time do you have, and whether you\ufffdve kind of reached a point where it's",
  "69:43": "not really improving anymore. You can actually get at the underlying decision trees in a",
  "69:49": "model, in a random first model, using estimators_. So with a list comprehension we can call predict",
  "69:56": "on each individual tree. And so here's an array, a Numpy array containing the predictions",
  "70:02": "from each individual tree for each row in our data. So if we take the mean across the",
  "70:11": "zero axis, we\ufffdll get exactly the same number. Because remember, that\ufffds what a random forest",
  "70:20": "does -- it takes the mean of the trees\ufffd predictions. So one cool thing we could do",
  "70:29": "is we could look at the 40 estimators we have and grab the predictions for the first i of",
  "70:40": "those trees and take their mean. And then we can find the root mean squared error. And",
  "70:47": "so in other words, here is the accuracy when you've just got one tree, two trees, three",
  "70:54": "trees, four trees five trees, etc. And you can see \ufffd so it's kind of nice, right. You",
  "70:58": "can, you can actually create your own kind of build your own tools to look inside these",
  "71:05": "things and see what's going on. And so we can see here that as you add more and more",
  "71:09": "trees, the accuracy did indeed keep improving, or the root mean squared error kept improving.",
  "71:14": "Although it, the improvement slowed down after a while. The validation set is worse than",
  "71:26": "the training set and there's a couple of reasons that could have happened. The first reason",
  "71:31": "could be because we're still overfitting, which is not necessarily a problem. This is",
  "71:37": "something we could handle. Or maybe it's because the fact that we're trying to predict the",
  "71:42": "last two weeks is actually a problem, and that the last two weeks are kind of different",
  "71:48": "to the other options in our data set. Maybe something changed over time. So how do we",
  "71:53": "tell which of those two reasons there are, what, what is the reason",
  "71:57": "that our validation set is worse. We can actually find out using a very clever trick called",
  "72:03": "out-of-bag error, oob_error. And we use oob_error for lots of things. You can grab the oob_error,",
  "72:13": "or you can grab the oob predictions from the model with oob_prediction_, and you can grab",
  "72:18": "the RMSE. And you can find that the oob_ error RMSE is 0.21, which is quite a bit better",
  "72:28": "than 0.23. So let me explain what oob_error is. What oob_error is, is, we look at each",
  "72:42": "row of the training set, not the validation set, each row of the training set. And we",
  "72:47": "say, so for row number 1, which trees included row number 1 in the training. And we\ufffdll",
  "72:56": "say, okay, let's not use those for calculating the error because it was part of those trees\ufffd",
  "73:01": "training. I would just calculate the error for that row using the trees where that row",
  "73:08": "was not included in training that tree. Because remember every tree is using only a subset",
  "73:13": "of the data. So we do that for every row - we find the prediction using only the trees that",
  "73:20": "were not used, that, that, that row was not used. And those are the oob predictions but",
  "73:28": "in other words this is like giving us a validation set result without actually needing a validation.",
  "73:35": "But the thing is, it's not with that time offset, it\ufffds not looking at the last two",
  "73:41": "weeks, it's looking at the whole training set. But this basically tells us how much",
  "73:46": "of the error is due to overfitting versus due to being the last couple of weeks. So",
  "73:53": "that's a cool trick, oob_error is something that very quickly kind of gives us a sense",
  "73:57": "of how much we're, we're overfitting. And we don't even need a validation set to do",
  "74:02": "it. But there's our oob_error, so that's telling us a bit about what's going on in our model.",
  "74:07": "Then there's a lot of things we'd like to find out from our model. And I've got five",
  "74:14": "things in particular here which I generally find pretty interesting. Which is, how confident",
  "74:20": "are we about our predictions, there's some particular prediction we're making? Like we",
  "74:27": "can say this is what we think the prediction is, but how confident are we - is that exactly",
  "74:32": "that, or is it just about that, or we really have no idea? And then for particular predicting",
  "74:40": "a particular item, which factors were the most important in that prediction? And how",
  "74:45": "did they influence it? Overall which columns are making the biggest difference? Which ones",
  "74:52": "could be maybe throw away and it wouldn't matter? Which columns are basically redundant",
  "74:59": "with each other (ao we don't really need both of them)? And as we vary some column, how",
  "75:06": "does it change the prediction. So those are the five things that work that I'm interested",
  "75:10": "in figuring out. And we can do all of those things with a random forest. Let's start with",
  "75:16": "the first one. So the first one, we've already seen that we can grab all of the predictions,",
  "75:25": "for all of the trees, and take their mean to get the actual predictions of the model,",
  "75:32": "and then to get the RMSE. But what if instead of saying \ufffdmean\ufffd, we did exactly the same",
  "75:36": "thing, like so, but instead it said \ufffdstandard deviation\ufffd. This is going to tell us for",
  "75:46": "every row in our dataset, how much did the trees vary? And so if our model really had",
  "75:56": "never seen kind of data like this before, it was something where, you know, different",
  "76:02": "trees were giving very different predictions, it might give us a sense that maybe this is",
  "76:08": "something that we're not at all confident about. And as you can see when we look at",
  "76:12": "the standard deviation of the trees for each prediction let's just look at the first five,",
  "76:17": "they vary a lot - 0.2, 0.1, 0.09, nearly 0.3, okay. So this is a really interesting, it's",
  "76:31": "not something that a lot of people talk about, but I think",
  "76:32": "it's a really interesting approach to kind of figuring out whether we might want to be",
  "76:37": "cautious about a particular prediction, because maybe we're not very confident about it. There's",
  "76:44": "one thing we can easily do with the random forest. The next thing, and this is I think",
  "76:50": "the most important thing for me in terms of interpretation, is feature importance. Here's",
  "76:56": "what feature importance looks like. We can call feature importance on a mode. Here's",
  "77:00": "some independent variables. Let's say grab the first ten. This says these are the 10",
  "77:06": "most important features in this random forest. These are the things that are the most strongly",
  "77:12": "driving sale price. Or we could plot them. And so you can see here, there's just a few",
  "77:20": "things that are by far the most important - what year the equipment was made (bulldozer",
  "77:27": "or whatever), how big is it, coupler system (whatever that means) and the product class",
  "77:34": "(whatever that means). And so you can get this by simply looking inside your trained",
  "77:43": "model and grabbing the feature importances attribute. And so here for making it better",
  "77:49": "to print out I'm just sticking that into a data frame and sorting descending by importance.",
  "77:56": "So how is this actually being done? It's, it's actually really neat what scikit-learn",
  "78:03": "does and (and Breiman the inventor of random forests described), is that you can go through",
  "78:09": "each tree and then start at the top of the tree and look at each branch and at each branch",
  "78:15": "see what feature was used the split. Which binary, which, the binary split was based",
  "78:20": "on which column. And then how much better was the model after that split compared to",
  "78:26": "beforehand. And we basically then say, \ufffdOkay that column was responsible for that amount",
  "78:32": "of improvement.\ufffd And so he can add that up across all of the splits, across all of",
  "78:37": "the trees, for each column and then you normalize it so they all add to 1. And that's what gives",
  "78:44": "you these numbers, which we show the first few of them in this table. And the first 30",
  "78:50": "of them here in this chart. So this is something that's fast, and it's easy and it kind of",
  "78:57": "gives us a good sense of like, \ufffdWell maybe the stuff that are less than 0.005, we could",
  "79:05": "remove. So if we did that that would leave us with only 21 columns. So let's try that,",
  "79:14": "let's just, just say, Okay, xs, which are important, the xs which are in this list of",
  "79:20": "ones to keep, do the same for valid. Retrain our random forest and have a look at the result.",
  "79:29": "And basically our accuracy is about the same, but we've gone down from 78 columns to 21",
  "79:38": "columns. So I think this is really important. It's not just about creating the most accurate",
  "79:43": "model you can, but you want to kind of be able to fit it in your head as best as possible.",
  "79:48": "And so 21 columns it's going to be much easier for us to check for any data issues and understand",
  "79:53": "what's going on. And the accuracy is about the same, or the RMSE. So I would say, Okay",
  "80:01": "let's do that, let's just stick with xs important for now on. And so here's this entire set",
  "80:08": "of the 21 features you can see it looks now like YearMade and ProductSize are the two",
  "80:15": "really important things. And then there's a cluster of kind of mainly product related",
  "80:20": "things that are kind of at the next level of importance. One of the tricky things here",
  "80:28": "is that we've got like a ProductClassDesc, modelID secondary desk, model desk, base model,",
  "80:37": "a model descriptor - they all look like there might be similar ways of saying the same thing.",
  "80:43": "So one thing that can help us to interpret the feature importance better and understand",
  "80:47": "better what's happening the model is to remove redundant features",
  "80:56": "So one way to do that is to call fast.ai use cluster_columns, which is basically a thin",
  "81:01": "wrapper for stuff that scikit-learn already provides. And what that's going to do is it's",
  "81:06": "going to find pairs of columns which are very similar. You can see here saleYear and saleElapsed,",
  "81:13": "see how this line is way out to the right whereas machineID and modelID is not at all,",
  "81:19": "it\ufffds way out to the left. But that means that saleYear and saleElapsed are very very",
  "81:24": "similar - when one is lower the other tends to be low and vice versa. Here's a group of",
  "81:29": "three which all seem to be much the same. And then productGroupDesc and productGroup",
  "81:35": "and then fiBaseModel and fiModelDes, but these all seem like things where maybe we could",
  "81:41": "remove one of each of these pairs because they basically seem to be much the same. You",
  "81:48": "know, they're very, you know they're. when one is higher the other is high and vice versa.",
  "81:56": "So let's try removing one of each of these. Now it takes a little while to train a random",
  "82:07": "forest and so for the, just to see whether removing something makes it much worse, we",
  "82:14": "could just do a very fast version. So we could just train something where we only have 50,000",
  "82:19": "rows per tree, train for each tree and we'll just use 40 trees. And let's then just get",
  "82:32": "the oob_score. And so for that fast simple version our oob with our important ex\ufffds",
  "82:43": "is 0.87 7. And here, for oob, a higher number is better. So then let's try going through",
  "82:51": "each of the things we thought we might not need and try dropping them, then getting the",
  "82:58": "oob_error for our exes with that one column removed. And so compared to 877, most of them",
  "83:07": "don't seem to hurt very much. saleEapsed hurt quite a bit, alright. So for each of those",
  "83:14": "groups, let's go and see which one of the ones seems like we could remove it. Here's",
  "83:21": "the five I found, remove the whole lot and see what happens. And so the oob went from",
  "83:30": ".877 to .874. So hardly any difference at all, despite the fact we managed to get rid",
  "83:37": "of five of our variables. So let's create something called ex_final, which is the ex",
  "83:45": "is important and then dropping those five. Save them for later, we can always load them",
  "83:53": "back again. And then let's check our random forest using those and again 0.233, 0.234.",
  "84:01": "So we've got about the same thing, but we've got even less columns now. So we're getting",
  "84:08": "a kind of a simpler and simpler model without hurting our accuracy. That is great. So the",
  "84:14": "next thing we said we were interested in learning about is di the columns that are, particularly",
  "84:21": "the columns that are most important, how does, what's the relationship between that column",
  "84:27": "and the dependent variable. Sorry, for example, what's the relationship between ProductSize",
  "84:31": "and sale price. Now the first thing I would do would be just to look at a histogram. So",
  "84:39": "one way to do that is with value_counts in Pandas. And we can see here our different",
  "84:48": "levels of ProductSize. And one thing to note here is actually missing is actually the most",
  "84:57": "common, and then next most is Compact and Small and then Mini is pretty tiny. So we",
  "85:04": "can do the same thing for YearMade. Now for YearMade, we can't just see the, the basic",
  "85:12": "bar chart, here we have a Pandas bar chart. For YearMade we actually need a histogram,",
  "85:18": "which Pandas has stuff like this built in so we can just call histogram. And that 1950,",
  "85:25": "you remember we created it - that's kind of this missing value thing which used to be",
  "85:30": "1000, but most of them seemed to have been well into the \ufffd90s and 2000\ufffds. So let's",
  "85:36": "now look at something called a partial dependence plot. I'll show it to you",
  "85:42": "first. Here is a partial dependence plot of YearMade against PartialDependence. What does",
  "85:55": "this mean? Well, we should focus on the part where we actually have a reasonable amount",
  "86:00": "of data. So at least, well into the 80s, so around here, and so let's look at this bit",
  "86:07": "here. Basically what this says is that as year made increases, the predicted sale price,",
  "86:18": "log sale price, of course also increases, you can see. And the log sale price is increasing",
  "86:25": "linearly on other roughly so roughly then this is actually an exponential relationship",
  "86:31": "between YearMade and sale price. Why do we call it a partial dependence? So we're just",
  "86:38": "plotting a kind of the year against the average sale price. Well no, we're not. We can't do",
  "86:44": "that because a lot of other things change from year to year. Example, maybe more recently",
  "86:52": "people tend to buy bigger bulldozers or more bulldozers with air conditioning, or more",
  "87:01": "expensive models of bulldozers. And we really want to be able to say, like no, just what's",
  "87:05": "the impact of a year and nothing else? And if you think about it from a kind of an inflation",
  "87:10": "point of view, you would expect that older bulldozers would be kind of like, that bulldozers",
  "87:21": "would get a kind of a constant ratio, cheaper the further you go back. Which is what we",
  "87:28": "see. So what we really want to say is, all other things being equal, what happens if",
  "87:35": "only the year changes? And there's a really cool way we can answer that question with",
  "87:41": "a random forest. So how does YearMade impact sale price, all other things being equal?",
  "87:48": "What we can do is we can go into an actual data set and replace every single value in",
  "87:53": "the YearMade column with 1950, and then can calculate the predicted sale price for every",
  "87:59": "single option, and then take the average over all the options. And that's what gives us",
  "88:05": "this value here. And then we can do the same for 1951, 1952, and so forth, until eventually",
  "88:12": "we get to our final year of 2011. So this isolates the effect of only YearMade. So it's",
  "88:23": "a kind of a bit of a curious thing to do but it's actually it's a pretty neat trick, like",
  "88:30": "trying to kind of pull apart and create this partial dependence and say what might be the",
  "88:36": "impact of just changing YearMade. And we can do the same thing for ProductSize. And one",
  "88:44": "of the interesting things if we do it for ProductSize is we see that the lowest value",
  "88:49": "of predicted sale price, log sale price, is #na#, which is a bit of a worry because we",
  "88:59": "kind of want to know \ufffd well that means it's really important. The question of whether",
  "89:02": "or not the product size is labeled is really important. And that is something that I would",
  "89:08": "want to dig into before I actually use this model to find out, Well why is it that sometimes",
  "89:13": "things aren't labeled and what does it mean? You know why is it that, that's actually,",
  "89:17": "that's just important predictor. So that is the partial dependence plot and it's so really",
  "89:25": "clever trick. So we have looked at four of the five questions we said we wanted to answer",
  "89:35": "at the start of this section, so the last one that we want to answer is one here ... we're",
  "89:42": "predicting with a particular row of data what were the most important factors and how did",
  "89:47": "they influence that prediction. This is quite related to the very first thing we saw. So",
  "89:52": "it's like, imagine you were using this auction price model in real life you had something",
  "89:58": "on your tablet and you went into some auction and you looked up what the predicted auction",
  "90:05": "price would be for this lot that's coming up to find out whether it seems like it's",
  "90:11": "being under or overvalued and then you decide what to do about that. So one thing we said",
  "90:17": "we'd be interested to know is like, well are we actually confident in our prediction and",
  "90:22": "then we might be curious to find out like, \ufffdOh, I'm really surprised it was predicting",
  "90:26": "such a high value. Why was it predicting such a",
  "90:28": "high value?\ufffd So to find the answer to that question we can use a module called treeInterpreter.",
  "90:38": "And treeInterpreter, the way it works is that you pass in a single row, so it's like here's",
  "90:46": "the auction that's coming up, here's the model, here's the auctioneer ID, etc., etc., please",
  "90:54": "predict the value - from the random forest, what's the expected sale price. And then what",
  "91:02": "we can do is we can take that one row of data and put it through the first decision tree",
  "91:07": "and we can see what's the first split that's selected. And then based on that split, does",
  "91:12": "it end up increasing or decreasing the predicted price compared to that kind of raw baseline",
  "91:18": "model of just take the average. And then you can do that again at the next split and again",
  "91:23": "at the next split and the next split. So for each split, we see what the increase or decrease",
  "91:28": "in the prediction, that's not right. We see what the increase or decrease in the prediction",
  "91:37": "is (while I'm here) compared to the parent node. And so then do that for every tree,",
  "91:50": "and then add up the total change and importance by split variable. And that allows you to",
  "91:56": "draw something like this. So here's something that's looking at one particular row of data.",
  "92:04": "And overall we start at 0, and so 0 is the initial 10.1. Do you remember this number?",
  "92:14": "10.1 is the average log sale price of the whole data set, they call it the bias. And",
  "92:21": "so if we call that 0, then for this particular row we're looking at, YearMade has a -4.2",
  "92:31": "impact on the prediction. And then ProductSize has a +0.2. Coupler_System has a positive",
  "92:39": "0.046. modelID has a positive 0.127, and so forth, right. And so the red ones are negative",
  "92:48": "and the green ones are positive. And you can see how they all join up, until eventually,",
  "92:53": "overall the prediction is that it's going to be -.122 compared to 10.1, which is equal",
  "93:02": "to 9.98. So this kind of plot is called a waterfall plot and so basically when we say",
  "93:13": "treeInterpreter.predict, it gives us back the prediction, which is the actual number",
  "93:21": "we get back from the random forest, the bias, which is just always this 10.1 for this data",
  "93:27": "set, and then the contributions, which is all of these different values. It\ufffds how",
  "93:33": "much, how important was each factor and here I've used a threshold which means anything",
  "93:42": "that was less than 0.08 all gets thrown into this other category. I think this is a really",
  "93:49": "useful kind of thing to have in production because it can help you answer questions,",
  "93:54": "whether it will be to the customer or for, you know, whoever's using your model, if they're",
  "93:59": "surprised about something \ufffd why is that prediction? So I'm going to show you something",
  "94:08": "really interesting using some synthetic data and I want you to really have a think about",
  "94:15": "why this is happening before I tell you. Pause the video if you're watching the video when",
  "94:21": "I get to that point. Let's start by creating some synthetic data like so. So we're going",
  "94:27": "to grab 40 values evenly spaced between 0 and 20. And then we're just going to create",
  "94:33": "the y equals x line and add some normally distributed random jitter on that. Here's",
  "94:43": "the scatter plot. So here's some data we want to try and predict. If we're going to use",
  "94:49": "a random forest, you know, kind of a bit of an overkill here. Now in this case we only",
  "94:56": "have one independent variable. Scikit-learn expects us to have more than one, so we can",
  "95:06": "use unsqueeze in PyTorch to add, to go from a shape of 40, in other words a",
  "95:12": "vector with 40 elements to a shape of [40,1]. In other words a matrix of 40 rows with one",
  "95:18": "column. But this unsqueeze(1) means add a unit axis here. I don't use unsqueeze very",
  "95:27": "often because I actually generally prefer the index with a special value None. This",
  "95:32": "works in PyTorch and Numpy. And this, the way it works is to say, okay, x_lin (remember",
  "95:38": "that size is a vector of length 40) every row. And then None means insert a unit axis",
  "95:47": "here. So these are two ways of doing the same thing, but this one is a little bit more flexible,",
  "95:53": "so that's what I use more often. But now that we've got the shape that is expected which",
  "95:58": "is a back to tensor and an array with two dimensions or axes we can create a random",
  "96:05": "forest, we can fit it. And let's just use the first 30 data points, and it's so kind",
  "96:10": "of top here. And then let's do a prediction, all right. So let's plot the original data",
  "96:17": "points and then also plot a prediction. And look what happens on the prediction. It acts,",
  "96:22": "it's kind of nice and accurate, and then suddenly what happens? Yes, this is the bit where if",
  "96:28": "you watch in the video I want you to pause and think, \ufffdWhy is this flat?\ufffd So what's",
  "96:35": "going on here? Well remember a random forest is just taking the average of predictions",
  "96:40": "of a bunch of trees. And a tree, the prediction of a tree is just the average of the values",
  "96:47": "in a leaf node. And remember we fitted using a training set containing only the first 30.",
  "96:55": "So none of these appeared in the training set, so the highest we could get would be",
  "97:00": "the average of values that are inside the training set. In other words, there\ufffds this",
  "97:05": "maximum we can get to. So random forests cannot extrapolate outside of the bounds of the data",
  "97:13": "that they have as a training set. This is going to be a huge problem for things like",
  "97:17": "time series prediction where there's like an underlying trend, for instance. But really",
  "97:22": "it's a more general issue than just time variables. It's going to be hard for, random forests,",
  "97:30": "or impossible, often, for random forests to just extrapolate outside the types of data",
  "97:33": "that it's seen, in a general sense. So we need to make sure that our validation set",
  "97:39": "does not contain out of domain data. So how do we find out of domain data? So we might",
  "97:48": "not even know if our test set is distributed in the same way as our training data. So,",
  "97:54": "if they're from two different time periods, how do you kind of tell how they vary, right?",
  "97:59": "Or if it's a Kaggle competition, how do you tell if the test set and the training set",
  "98:04": "which Kaggle gives you have some underlying differences? There\ufffds actually a cool trick",
  "98:10": "you can do -- which is you can create a column called is_valid contains 0 for everything",
  "98:17": "in the training set and 1 for everything in the validations set. It's concatenating all",
  "98:26": "of the independent variables together. So it's so so it's concatenating the independent",
  "98:30": "variables for both the training and validation set together. So this is our independent variable",
  "98:35": "and this becomes our dependent variable. And we're going to create a random forest not",
  "98:41": "for predicting price, but a random forest that predicts is this row from the validation",
  "98:48": "set or the training set. If the validation set and the training set are from kind of",
  "98:54": "the same distribution, if they're not different, then this random forest should basically have",
  "98:59": "zero predictive power. If it has any predictive power, then it means that our training and",
  "99:06": "validation sets are different and to find out the source of that difference we can use",
  "99:11": "feature importance. And so you can see here that the difference between the validation",
  "99:20": "set and the training set is not surprisingly saleElapsed.",
  "99:27": "So that's the number of days since I think like 1970 or something so it's basically the",
  "99:32": "date. Oh, yes, of course, you can predict whether something is in the validation set",
  "99:36": "or the training set by looking at the date, because that's actually how to find them.",
  "99:40": "That makes sense. This is interesting, SalesID. So it looks like the sales ID is not some",
  "99:46": "random identifier, but it increases over time. And ditto for MachineID. And then there's",
  "99:54": "some other smaller ones here. That kind of makes sense. So I guess for something like",
  "99:58": "ModelDesk. I guess there are certain models that were only made in later years for instance",
  "100:06": "can see these top three columns are a bit of an issue. So then we could say, \ufffdLike,",
  "100:12": "okay, what happens if we look at each one of those columns, those first three and remove",
  "100:18": "them, and then see how it changes our RMSE on our sales price model on the validation",
  "100:34": "set. We start from 0.232, and removing salesID actually makes it a bit better; saleElapsed",
  "100:42": "makes it a bit worse; machineID, about the same. But we can probably remove salesID and",
  "100:48": "machine ID without losing any accuracy. And yep, it's actually a slight improvement. But",
  "100:54": "most importantly it's going to be more resilient over time, right. Because we're trying to",
  "100:58": "remove the time-related features. Another thing to note is that since it seems that,",
  "101:07": "you know, this kind of saleElapsed issue that maybe it's making a big difference is maybe",
  "101:13": "looking at the saleYear distribution (this is the histogram), most of the sales are in",
  "101:19": "the last few years anyway. Now what happens if we only include the most recent few years.",
  "101:25": "So let's just include everything out to 2004. So that is xs_filt. And if I trained on that",
  "101:34": "subset, then my accuracy goes, improves a bit more, from .231 to .230. Oh, so that's",
  "101:43": "interesting, right. We're actually using less data, less rows, and getting a slightly better",
  "101:49": "result because the more recent data is more representative. So that's about as far as",
  "101:55": "we can get with our random forest. But what I will say is this \ufffd this issue of extrapolation",
  "102:04": "would not happen with a neural net, would it? Because a neural net is using the kind",
  "102:10": "of the underlying layers, linear layers, and so linear layers can absolutely extrapolate.",
  "102:16": "So the obvious thing to think then at this point, is well maybe would a neural net there",
  "102:22": "a better job of this? That's gonna be the thing next, after this question. A question",
  "102:27": "first. How do, how does feature importance relate to correlation? Feature importance",
  "102:37": "doesn't particularly relate to a correlation. Correlation is a concept for linear models,",
  "102:43": "and this is not a linear model. So remember, feature importance is calculated by looking",
  "102:48": "at the improvement in accuracy as you go down each tree and you go down each binary spit.",
  "103:00": "If you're used to linear regression, then I guess correlation sometimes can be used",
  "103:06": "as a measure of feature importance, but this is a much more kind of direct version. It\ufffd's",
  "103:14": "taking account of these nonlinearities and interactions, as well, so it's a much more",
  "103:20": "flexible and reliable measure than feature importance. Any more questions? So do the",
  "103:31": "same thing with a neural network. I'm going to just copy and paste the same lines of code",
  "103:36": "that I had from before, but this time I call it nn_df_nn. And these are the same lines",
  "103:41": "of code, and I'll grab the same list of columns we had before in the dependent variable to",
  "103:46": "get the same dataframe. Now as we've discussed, for categorical columns we probably want to",
  "103:53": "use embeddings. So to create embeddings we need",
  "103:57": "to know which columns should be treated as categorical variables. And as we discussed",
  "104:01": "we can use cont_cat_split for that. One of the useful things we can pass that is the",
  "104:06": "maximum cardinality. so max_card=9000 means if there's a column with more than 9000 levels,",
  "104:14": "you should treat it as continuous; and if it's got less than 9000 levels, treat it as",
  "104:20": "categorical. So that's you know it's a simple little function that just checks the cardinality",
  "104:24": "and splits them based on how many discrete levels they have. And of course, if their",
  "104:29": "data type, if it's not actually a numeric data type, it has to be categorical. So there's",
  "104:36": "our, there's our split. And then from there what we can do is we can say, \ufffdOh, we've",
  "104:47": "got to be a bit careful of saleElapsed, because actually saleElapsed I think has less than",
  "104:52": "9000 categories, but we definitely don't want to use that as a categorical variable. The",
  "104:56": "whole point was to make it that this is something that we can extrapolate. Certainly anything",
  "105:01": "that's kind of time dependent, or you think that we might see things outside the range",
  "105:06": "of inputs in the training data, we should make them continuous variables. So let's make",
  "105:12": "saleElapsed, put it in continuous neural net and remove it from categorical. So here's",
  "105:22": "the number of unique levels, this is from Pandas for everything in our neural net data",
  "105:26": "set for the categorical variables. And I get a bit nervous when I see these really high",
  "105:31": "numbers, so I don't want to have too many things with like lots and lots of categories.",
  "105:35": "The reason I don't want lots of things with lots and lots of categories is just they\ufffdve",
  "105:42": "got to take up a lot of parameters. Because you know in an embedding matrix, this is,",
  "105:46": "you know, every one of these is a row in an embedding matrix. In this case, I notice ModelID",
  "105:51": "and modelDesc might be describing something very similar, so I\ufffdd quite like to find",
  "105:56": "out if I could get rid of one. And an easy way to do that would be to use a random forest.",
  "106:01": "So let's try removing the ModelDesc and let's create a random forest, see what happens.",
  "106:12": "And, oh it's actually a tiny bit better and certainly not worse. So that suggests that",
  "106:18": "we can actually get rid of one of these levels, or one of these variables. So let's get rid",
  "106:23": "of that one. So now we can create a tabular Panda's object just like before, but this",
  "106:29": "time we're going to add one more processor which is normalized. And the reason we need",
  "106:35": "normalize \ufffd so normalize is subtract the mean, divide by the standard deviation. We",
  "106:40": "didn't need that for a random forest because for a random forest we're just looking at",
  "106:46": "less than or greater than through our binary splits. So all that matters is the order of",
  "106:51": "things how they're sorted; doesn't matter whether they're super big or super small.",
  "106:54": "But it definitely matters for neural nets because we have these linear layers. So we",
  "107:02": "don't want to have, you know, things with kind of crazy distributions with some super",
  "107:06": "big numbers and super small numbers, because it's not going to work. So it's always a good",
  "107:11": "idea to normalize things. Neural nets, where we can do that in a tabular neural net by",
  "107:18": "using the Normalize tabular proc. So we can do the same thing that we did before with",
  "107:24": "creating our tabularPandas -- tabular object, the neural net. And then we can create data",
  "107:30": "loaders from that with a batch size, and this is a large batch size because tabular models",
  "107:36": "don't generally require nearly as much GPU RAM as a convolutional neural net or something",
  "107:44": "or an RNN or something. Since it\ufffds a regression model, we're going to want our range, so let's",
  "107:52": "find the minimum and maximum of our dependent variable.",
  "107:56": "Then we can now go ahead and create a tabular_learner. But a tabular_learner is going to take our",
  "108:03": "data loaders, our y_range, how many activations do you want in each of the linear layers.",
  "108:11": "And so you can have as many linear layers as you like. How many outputs are there? This",
  "108:18": "is a regression with a single output. And what loss function do you want? We can use",
  "108:25": "lr_find and then we can go ahead and use fit_one_cycle (there's no pre trained model obviously, because",
  "108:32": "this is not something where people have got pre train models for industrial equipment",
  "108:38": "options). We just use fit_one_cycle and train for a minute. And then we can check and our",
  "108:47": "our MSE is 0.226, which, here it was 0.230. But that's amazing, we actually have you know",
  "108:58": "straight away a better result and the random forest. It's a little more fussy, it took",
  "109:03": "something takes a little bit longer. And as you can see, you know, for interesting data",
  "109:09": "sets like this we can get some great results with neural nets. So here's something else",
  "109:19": "we could do -- the random forest and the neural net, they each have their own pros and cons",
  "109:27": "-- some things they\ufffdre good at and some things they're less good. So maybe we can",
  "109:33": "get the best of both worlds and a really easy way to do that is to use ensemble. We've already",
  "109:38": "seen that a random forest is a decision tree ensemble, but now we can put that into another",
  "109:43": "ensemble. You can have an ensemble of the random forest and the neural net. There's",
  "109:48": "lots of super fancy ways you can do that, but a really simple way is to take the average.",
  "109:54": "So sum up the predictions from the two models, divide by two, and use that as a prediction.",
  "110:01": "So that's our ensemble prediction -- it\ufffds just literally the average of the random forest",
  "110:05": "prediction and the neural net prediction. And that gives us .223 (versus .226). So how",
  "110:17": "good is that? Well, it's a little hard to say because unfortunately this competition",
  "110:24": "is old enough that we can't even submit to it and find out how we would have gone on",
  "110:28": "Kaggle, so we don't really know. And so we're relying on our own validations that, but it's",
  "110:34": "quite a bit better than even the first place score on the test set. So if the validation",
  "110:43": "set is, you know, doing a good job then this is a good sign that this is a really really",
  "110:49": "good model. Which wouldn't necessarily be that surprising, because you know in the last",
  "110:56": "few years I guess we've learned a lot about building these kinds of models and we're kind",
  "111:03": "of taking advantage of a lot of the tricks that have, that have appeared in recent years.",
  "111:09": "And, maybe this goes to show that, well I think it certainly goes to show that both",
  "111:14": "random forests and neural nets have a lot to offer and try both and maybe even find",
  "111:24": "both. We've talked about an approach to ensembling called bagging, which is where we train lots",
  "111:34": "of models on different subsets of the data and take the average. Another approach to",
  "111:40": "ensembling, particularly ensembling of trees, is called boosting. And boosting involves",
  "111:47": "training a small model which underfits your data set. Maybe like just have a very small",
  "111:53": "number of if notes then you calculate the predictions using the small model and then",
  "112:01": "you subtract the predictions from the targets. So these are kind of like the errors of your",
  "112:05": "small underfit model, we call them residuals. And then go back to step one but now instead",
  "112:13": "of using the original targets use the residuals to train a small model which underfits your",
  "112:20": "dataset attempting to predict residuals. And do that again, and again, and again .. until",
  "112:27": "you reach some stopping criterion such as the maximum number of trees. Now, that will",
  "112:33": "leave you with a bunch of models which you don't average but which you sum, because each",
  "112:41": "one is creating a model that's based on the residual of the previous one. So we've subtracted",
  "112:47": "the predictions of each new tree from the residuals of the previous tree. But the residuals",
  "112:51": "get smaller and smaller and then to make predictions, we just have to do the opposite, which is",
  "112:56": "to add them all together. So there's lots of variants of this, but you'll see things",
  "113:06": "like GBMs or gradient boosted machines or GBDTs, or gradient boosted decision trees.",
  "113:12": "And there's lots, minor details surround, you know, and they're insignificant details,",
  "113:19": "but the basic idea is, is what I've shown you. Two questions. All right, let's take",
  "113:24": "the questions. Are dropping features in a model as a way to reduce the complexity of",
  "113:29": "the model and thus reduce overfitting - is this better than adding some regularization",
  "113:34": "like weight decay? I didn't claim that we removed columns to avoid overfitting. We removed",
  "113:49": "the columns to simplify things, analyze. And then it should also mean we don't need as",
  "114:00": "many trees, but there's no particular reason to believe that this will regularize. And",
  "114:05": "well the idea of regularization doesn't necessarily make a lot of sense to random forests. And",
  "114:11": "always add more trees. Is there a good heuristic for picking the number of linear layers in",
  "114:19": "the tabular model. Not really. Well, if there is I don't know what it is. I guess two, three",
  "114:32": "hidden layers works pretty well. So you know what I showed, those numbers I showed her",
  "114:39": "pretty good for a large-ish model. At default, it uses 200 and 100, so maybe start with the",
  "114:47": "default and then go up 500 and 250. If that gives an improvement, and like, just keep",
  "114:53": "doubling them while it keeps improving, or you run out of memory. Another thing to note",
  "115:01": "about boosted models is that there's nothing to stop us from overfitting. You add more",
  "115:06": "and more trees. The bagging model, sort of a random forest, it's going to get, going",
  "115:11": "to generalize better and better as your each time you're using your model which is based",
  "115:17": "on a subset of the data. But boosting each model will fit the training set better and",
  "115:25": "better. Eventually overfit, so boosting methods do require hyperparameter tuning and fiddling",
  "115:34": "around with, you know. You certainly have regularization boosting. They're pretty sensitive",
  "115:44": "to their hyperparameters, which is why they're not normally much first go-to. They they often,",
  "115:53": "they more often win Kaggle competition than random forests do. Like they tend to be good",
  "115:58": "at getting that last little bit of performance. But the last thing I'm going to mention is",
  "116:06": "something super-neat which a lot of people don't seem to know exists (it\ufffds a shame,",
  "116:12": "because it\ufffds super-cool) which is something from the entity embeddings paper, the table",
  "116:17": "from it, where what they did was they built a neural network, they got the entity embeddings,",
  "116:24": "ee and then they try to random forest using the entity embedding as predictors, rather",
  "116:35": "than the approach I described with just the raw categorical variables. And the, the error",
  "116:43": "for a random forest went from 0.16 to 0.11, a huge improvement. And a very simple method,",
  "116:52": "KNN, went from .29 to 0.11. Basically all of the methods when they used entity embeddings",
  "116:58": "suddenly improved a lot. The one thing you you should try if you have a look at the further",
  "117:04": "research section after the questionnaire is it asks to try to do this, actually take",
  "117:10": "those entity embeddings that we trained in the neural net, and use them in the random",
  "117:14": "forest. And then maybe try ensembling again and see if you can beat the .223 that we had.",
  "117:25": "This is a really nice idea, it's like, you get you know, all the benefits of boosted",
  "117:31": "decision trees, but all of the nice features of entity embeddings. And so this is something,",
  "117:39": "that not enough people tend to be playing with for some reason. So, overall, you know,",
  "117:49": "random forests are nice and easy to train, you know, they're very resilient, they don't",
  "117:53": "require much pre-processing, they're trained quickly, they don't overfit, you know, they",
  "118:00": "can be a little less accurate, and they can be a bit slow at inference time, because for",
  "118:06": "inference you have to go through every one of those trees. Having said that, a binary",
  "118:11": "tree and be pretty heavily optimized. Though you know it is something you can basically",
  "118:20": "create a totally compiled version of a tree and they can certainly also be done entirely",
  "118:28": "in parallel, so that's something to consider. Gradient boosting machines are also fast to",
  "118:37": "train on the whole but a little more fussy about hyper parameters, you have to be careful",
  "118:43": "about overfitting but get more accurate. Neural nets may be the fussiest to deal with, they've",
  "118:52": "kind of got the least rules of thumb around, or tutorials around saying this is how to",
  "118:58": "do it. It's just a bit a bit newer, a little bit less well understood, but they can give",
  "119:04": "better results in many situations than the other two approaches or at least with an ensemble",
  "119:08": "can improve the other two approaches. So I would always start with a random forest and",
  "119:13": "then see if you can beat it using these. So yeah why don't you now see if you can find",
  "119:21": "a Kaggle competition with tabular data, whether it's running now pr it's a past one and see",
  "119:25": "if you can repeat this process for that and see if you can get in the top 10 percent of",
  "119:31": "the private leaderboard. That would be a really great, stretch goal at this point. Implement",
  "119:38": "the decision tree algorithm yourself, I think that's an important one, to see if you really",
  "119:41": "understand it and then from there create your own random forests from scratch. You might",
  "119:45": "be surprised it's not that hard. And then go and have a look at the tabular model source",
  "119:53": "code, and at this point this is pretty exciting, you should find you pretty much know what",
  "119:59": "all the lines do with two exceptions. And if you don't, you know, dig around and explore",
  "120:05": "an experiment and see if you can figure it out. And with that, we are, I am very excited",
  "120:13": "to say at a point where we've really dug all the way in to the end of these real, valuable,",
  "120:20": "effective, fastai applications and we're understanding what's going on inside them. What should we",
  "120:28": "expect for next week. Oh for next week we will look at NLP and computer vision. And",
  "120:34": "we\ufffdll do the same kind of ideas, delve deep to see what's going on. Thanks everybody,",
  "120:42": "see you next week!"
}