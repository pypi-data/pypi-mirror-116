{
  "00:03": "Welcome back and here is lesson 4 which is where we get deep into the weeds of exactly",
  "00:12": "what is going on when we are training a neural network and we started looking at this in",
  "00:18": "the previous lesson. We were looking at the stochastic gradient descent and so to remind",
  "00:24": "you, we were looking at what Arthur Samuel said. \u201cSuppose we arrange for some automatic",
  "00:31": "means of testing the effectiveness of any current weight assignment ( or we would call",
  "00:36": "it parameter) in terms of actual performance and provide a mechanism for altering the weight",
  "00:42": "assignment so as to maximize that performance. So we could make that entirely automatic and",
  "00:48": "a machine so programmed would learn from its experience\u201d and that was it. So our initial",
  "00:55": "attempt on the MNIST data set was not really based on that. We didn't really have any parameters.",
  "01:04": "So then , last week we tried to figure out how we could parameterize it, how we could",
  "01:10": "create a function that had parameters. And, what we thought we could do would be to have",
  "01:15": "something where that say the probability of being some particular number was expressed",
  "01:21": "in terms of the pixels of that number and some weights, and then we would just multiply",
  "01:27": "them together and add them up. So we looked at how stochastic gradient descent works last",
  "01:38": "week and the basic idea is that we start out by initializing the parameters randomly. We",
  "01:47": "use them to make a prediction using a function, such as this one. We then see how good that",
  "01:57": "prediction is, by measuring using a loss function, we then calculate the gradient which is how",
  "02:02": "much with the loss change if I change one parameter by a little bit, we then use that",
  "02:09": "to make a small step to change each of the parameters by a little bit, and by multiplying",
  "02:16": "the learning rate by the gradient to get a new set of predictions and so we went round",
  "02:20": "and round and round a few times until eventually we decided to stop and so these are the basic",
  "02:27": "seven steps. Then we went through and so we did that for simple quadratic equation, and",
  "02:39": "we had something which looked like this and so by the end we had this nice curve getting",
  "02:48": "closer and closer and closer. So I have a little summary at the start of this section",
  "02:59": "summarizing gradient descent that Silvyan and I have in the notebooks, in the book,",
  "03:04": "of what we just did, so you can review that, and make sure it makes sense to you. So now",
  "03:11": "let's use this to create our MNIST \u201c3\u201d vs. \u201c7\u201d model and so to create a model,",
  "03:20": "we're going to need to create something that we can pass into a function like, let's see",
  "03:27": "where it was, passing to a function like this one. So we need just some pixels that are",
  "03:34": "all lined up and some parameters that are all lined up, and then we're going to sum",
  "03:38": "them up. So our axis are going to be pixels and so in this case because we're just going",
  "03:47": "to multiply each pixel by a parameter and add them up, the effect that they're laid",
  "03:52": "out in a grid is not important so let's reshape those grids and turn them into vectors. The",
  "04:01": "way we reshape things in Pytorch is by using the \u201c.view\u201d method. The view method you",
  "04:08": "can pass to it how large you want each dimension to be and so in this case we want the number",
  "04:18": "of columns to be equal to the total number of pixels in each picture., which is 28 x",
  "04:24": "28. because they're 28 by 28 images. And then the number of rows will be however many rows",
  "04:29": "there are in the data, and so if you just use minus one when you call view, that means,",
  "04:35": "you know, as many as there are in the data, so this will create something of the same,",
  "04:40": "with the same total number of elements that we had before. So we can grab all our 3 we",
  "04:46": "can concatenate them with torch.cat with all of our 7 and then reshape that into a Matrix",
  "04:54": "where each row is one Image with all of the rows and columns of the image all lined up",
  "05:01": "in a single vector. Then we're going to need labels, so that's our X, so we're going to",
  "05:07": "need labels, our labels will be a 1 for each of the 3s and a 0 for each of the 7s so basically,",
  "05:15": "we're creating \u201cis 3 model\u201d. So that's going to create a vector, we actually need",
  "05:25": "it to be a matrix in Pytorch so .unsqueeze will add an additional unit dimension to wherever",
  "05:37": "I've asked for so here in position one, so in other words, this is going to turn up from",
  "05:41": "something which is a vector of 12396 long into a matrix with 12396 rows and one column.",
  "05:52": "That's just what Pytorch expects to see. So now we're going to turn our X, Y into a Dataset",
  "06:01": "and a Dataset is a very specific concept in Pytorch. It's something which we can index",
  "06:08": "into, using square brackets, and when we do so, it's expected to return a tuple. So here",
  "06:19": "if we look at how we're going to create this Dataset and when we index into it, it's going",
  "06:28": "to return a tuple containing our independent variable and a dependent variable, for each",
  "06:36": "particular row, and so to do that we can use the Python zip function, which takes one element",
  "06:44": "of the first thing and combines it with concatenates it with one element of the second thing and",
  "06:50": "then it does that again and again and again and so then if we create a list of those it",
  "06:55": "gives us a, It is as a Dataset!. It gives us a list which when we index into it, it's",
  "07:02": "going to contain one image and one label, and so here you can see why there's my label",
  "07:08": "and my image I won't print out the whole thing, but it's a 784 long vector. So that's a really",
  "07:16": "important concept, a Dataset is something that you can index into, and get back at Tuple.",
  "07:24": "And here I am, this is called destructuring the tuple: which means I'm taking the two",
  "07:27": "parts of the tuple and putting the first part in one variable in the second part in the",
  "07:31": "other variable, which is something we do a lot in Python, it's pretty handy, a lot of",
  "07:35": "other languages support that as well. Repeat the same three steps for a validation set.",
  "07:42": "So we've now got a training Dataset and a validation Dataset. Right! So now we need",
  "07:50": "to initialize our parameters and so to do that, as we've discussed, we just do it randomly.",
  "07:57": "So here's a function. They're given some size, some, some shape if you like. We'll randomly",
  "08:05": "initialize, using a normal random number distribution in PyTorch that's what .Randn does and we",
  "08:13": "can hit shift tab to see how that works, okay!? And it says here that it's going to have a",
  "08:26": "variance of one. So, I probably should NOT call this standard deviation, I probably should",
  "08:29": "have called this: variance actually. So multiply it by the variance - to change its variance",
  "08:35": "to whatever is requested, which will default to one. And then as we talked about, when",
  "08:41": "it comes to calculating our gradients we have to tell PyTorch which things we want gradients",
  "08:48": "for and the way we do that is requires_grad_ . Remember this underscore at the end is a",
  "08:54": "special magic symbol which tells PyTorch that we want this function to actually change the",
  "09:00": "thing that it's referring to. This will change this tensor, Such that it requires radiance.",
  "09:10": "So here's some weights, so our weights are going to need to be 28 by 28 by 1 shape 28",
  "09:18": "by 28 because every pixel is going to need a weight and then 1 because we're going to",
  "09:25": "need it again , so we're going to need to have that unit access to make it into a column.",
  "09:31": "So that's what PyTorch expects. So there's our weights. Now just weights by pixels actually",
  "09:41": "isn't going to be enough because weights by pixels were always equal zero. When the pixels",
  "09:46": "are equal to zero, it has a zero intercept. So we really want something where it's like",
  "09:50": "W * X + B, a line. So the B is we call the bias and So that's just going to be a single",
  "09:58": "number. So let's grab a single number for our bias. So, remember I told you, there's",
  "10:05": "a difference between the parameters and weights. So, actually speaking, so here the weights",
  "10:09": "are the W in this equation, the bias is B in this equation, and the weights and bias",
  "10:17": "together is the parameters of the function they're all the things that we're going to",
  "10:22": "change, they're all the things that have gradients that we're going to update. So there's an",
  "10:27": "important bit of jargon for you: the weights and biases of the model are the parameters.",
  "10:34": "We can... yes question! R: \u201cWhat's the difference between gradient descent and stochastic gradient",
  "10:43": "descent? J: So far we've only done gradient descent and will be doing stochastic gradient",
  "10:49": "descent in a few minutes. We can now create and calculate predictions for one image so",
  "10:55": "we can take an image such as the first one and multiply by the weights only to transpose",
  "11:01": "them to make them line up in terms of the rows and columns and add it up, and add the",
  "11:06": "bias and there is a prediction. We want to do that for every image we could do that with",
  "11:16": "a for loop and that would be really really slow. It wouldn't run on the GPU and it wouldn't",
  "11:22": "run in optimize and see code. So we actually want to use always to do kind of like looping",
  "11:27": "over pixels, looping over images. You always need to try to make sure you're doing that",
  "11:32": "without a Python for loop, in this case doing this calculation for lots of rows and columns",
  "11:41": "is a mathematical operation called matrix multiply, so if you've forgotten your matrix",
  "11:49": "multiplication or maybe never quite got around to it at a high school. It would be a good",
  "11:54": "idea to have a look at Khan Academy or something to learn about what it is, but it's actually",
  "12:00": "I'll give you the quick answer. This is from Wikipedia, if these are two matrices A and",
  "12:06": "B then this element here, 1, 2 in the output is going to be equal to the first bit here",
  "12:15": "times the first bit here, plus the second bit here, times the second bit here. So it's",
  "12:20": "going to be A1,2 * A1,1 + B 2,2 * A 1,2 that's you can see the orange matches the orange.",
  "12:30": "Ditto for over here. This would be equal to B1,3 * A 3,1 + B2,3 * A 3,2 and so forth for",
  "12:39": "every. Here's a great picture of that in action if you look at matrixmultiplication.XYZ another",
  "12:54": "way to think of it is we can kind of flip the second bit over on top and then multiply",
  "13:01": "each bit together and add them up, multiplied each bit together and add them up and you",
  "13:06": "can see always the second one here and ends up in the second spot and the first one ends",
  "13:09": "up in the first spot. And that's what matrix multiplication is. So we can do our, multiply",
  "13:23": "and Add them up by using matrix multiplication and in Python and therefore PyTorch matrix",
  "13:30": "multiplication is the @ sign operator. So when you see @, that means matrix multiply",
  "13:38": "so here is our 20.2336 if I do a matrix multiply of our training set by our weights and then",
  "13:51": "we add the bias and here is our 20.2336 for the first one. And you can see through. It's",
  "13:56": "doing every single one, okay!?. So that's really important is that matrix multiplication",
  "14:03": "gives us an optimized way to do these simple linear functions, whereas we want as many",
  "14:07": "kinds of rows and columns as we want. So this is one of the two fundamental equations of",
  "14:14": "any neural network. Some rows, of data rows and columns of data much use multiply some",
  "14:22": "weights add some bias and the second one which was here in a moment is an activation function.",
  "14:27": "So that is some predictions from our randomly initialized model, so we can check how good",
  "14:36": "our model is and so to do that we can decide that anything greater than 0 we will call",
  "14:45": "a 3 and anything less than 0 we will call a 7. So preds greater than 0.0 tells us whether",
  "14:54": "or not something is predicted to be a 3 or not. Then turn that into a float, so rather",
  "14:59": "than true and false, make it one in zero because it's what our training set contains and then",
  "15:04": "check with our thresholded predictions are equal to our training set and this will return",
  "15:13": "true every time a row is correctly predicted and false otherwise. So if we take all those",
  "15:20": "trues and falses and turn them into floats, that'll be ones and zeroes and then take their",
  "15:24": "mean: It's 0.49, so not surprisingly our randomly initialized model is right about half the",
  "15:31": "time at predicting threes from sevens. I Added one more method here, which is .item() without",
  "15:38": "item This would return a tensor, It's a rank zero tensor. It has no rows. It has no columns",
  "15:45": "it just it's just a number on its own, but I actually wanted to unwrap it to create a",
  "15:51": "normal Python scalar mainly just because I wanted to see them easily see the full set",
  "15:56": "of decimal places and the reason for that is I want to show you how we're going to calculate",
  "16:00": "the derivative on the accuracy. By changing a parameter a tiny bit, so let's take one",
  "16:09": "parameter which will be weights[0] and Multiply it by 1.0001 and so that's going to make it",
  "16:17": "a little bit bigger and then if I calculate how the the accuracy changes based on the",
  "16:26": "change in that weight that will be the gradient of the accuracy with respect to that parameter",
  "16:34": "so I can do that by calculating my new set of predictions and then I can threshold them",
  "16:39": "and then I can check whether they're equal to the training set and then take the meanAnd",
  "16:44": "I get back: exactly the same number so remember that Gradient is equal to Rise over run if",
  "16:57": "you remember back to your calculus or if you'd forgotten your calculus. Hopefully you've",
  "17:01": "reviewed it on Khan Academy so The change in the Y so y_new - y_old which is 0.4912",
  "17:12": "etc minus 0.4912 etc, which is 0 divided by This change will give us 0 so at this point",
  "17:24": "we have a problem our derivative is 0 so we have 0 gradients: Which means our step will",
  "17:32": "be zero which means our prediction will be unchanged.Okay So we have a problem and our",
  "17:43": "problem is that our gradient is zero and with a gradient ofZero we can't take a step and",
  "17:50": "we can't get better predictions. And so Intuitively speaking the reason that our gradient is zero",
  "17:57": "is because when we change a single pixel by a tiny bit we might not ever in any way change",
  "18:05": "an actual prediction to change from a three predicting a three to a seven or Vice versa",
  "18:11": "because we have this we have this threshold. And so in other words our Our accuracy Loss",
  "18:23": "function here is very bumpy. It's like a flat step flat step flat step. So it's got this",
  "18:31": "Zero gradient all over the place. So what we need to do is use something other than",
  "18:36": "accuracy as our loss function So, let's try and create a new function and what this new",
  "18:46": "function is going to do is it's going to Give us a better value. Kind of in much the same",
  "18:55": "way that Accuracy gives a better value. So this is the loss member of small losses better",
  "19:00": "so to give us a lower loss when the accuracy is better, but it won't have a zero gradient.",
  "19:08": "It means that a slightly better prediction needs to have a slightly better lossSo, let's",
  "19:18": "have a look at an example. Let's say our Targets, our labels of like that are three. Oh There's",
  "19:25": "just three rows three images here one zero one, okayAnd we've made some predictions from",
  "19:32": "a neural net and those predictions gave us a point. [0.9, 0.4, 0.2] so now consider:",
  "19:41": "This loss function a loss function: we're going to use torch.where() which is basically",
  "19:46": "the same as This list comprehension it\u2019s basically an if statement so it's going to",
  "19:52": "say for for where target equals one We're going to return 1 minus predictions. So here",
  "19:59": "target is one so it'll be 1 minus 0.9 and Where target is not 1 it'll just be predictions",
  "20:08": "so well these Examples here. The first one target equals 1 will be 1 - 0.9 = 0.1.The",
  "20:20": "next one is target equals 0 so to speak the prediction is just 0.4 And then for the third",
  "20:26": "one, it's a 1 for target So it'll be 1 - prediction, which is 0.8. And so you can see here when",
  "20:35": "the Prediction is correct. Correct. In other words, it's a number, you know It's a high",
  "20:39": "number when the target is 1 and a low number when the target is 0, these numbers are going",
  "20:46": "to be smaller. So the worst one is when we predicted 0.2. So we're pretty we really thought",
  "20:53": "that was actually a zero But it's actually a 1 so we ended up with a 0.8 here because",
  "20:59": "this is 1 minus prediction 1 - 0.2 = 0.8, So we can then take the mean of all of these",
  "21:09": "to calculate a loss. So if you think about it this loss will be the smallest if The predictions",
  "21:19": "are exactly right. So if we did predictions is actually identical to the targets this",
  "21:33": "will be [0., 0., 0.] okay, where else if they were exactly wrong say they were one then",
  "21:44": "it's [1., 1., 1.]. So it's going to be the loss will be better ie smaller when the predictions",
  "21:54": "are closer to the targets. And so here we can now take the mean and when we do we get",
  "22:02": "here 0.433. Let's say we change this last bad one in accurate prediction from 0.2 to",
  "22:16": "0.8 and the loss gets better from 0.43 to 0.23 but this is just this function is torch.where().",
  "22:27": "So this is actually pretty good. This is actually a loss function which pretty closely tracks",
  "22:32": "accuracy was the accuracies better, The loss will be smaller But also it doesn't have these",
  "22:37": "zero gradients because every time we change the prediction the loss changes Because the",
  "22:42": "prediction is literally harder the loss that's pretty neat, isn't it? One problem is this",
  "22:49": "is only going to work. Well, as long as the predictions are between 0 & 1 Otherwise, this",
  "22:54": "one - prediction thing is going to look a bit funny. We should try and find a way to",
  "22:59": "ensure that the predictions are always between zero and one and that's also going to just",
  "23:05": "make a lot more intuitive sense because you know we like to be able to kind of think of",
  "23:08": "these as if they're like probabilities or at least nicely scaled numbers so we need",
  "23:14": "some function that can take our numbers: Have a look, something which can take these big",
  "23:26": "numbers and turn them all into numbers between zero and one and it so happens that we have",
  "23:33": "exactly the right function. It's called the sigmoid functions of the sigmoid function",
  "23:39": "looks like this if you pass in a really small number you get a number very close to zeroIf",
  "23:45": "you pass in a big number you get a number very close to 1 it never gets past one and",
  "23:51": "it never goes smaller than zero and then it's kind of like this smooth curve between and",
  "23:57": "in the middle It looks a lot like the y = x line. This is the definition of the sigmoid",
  "24:04": "function It's 1 over 1 + e to minus x What is exp? exp is just e to the power of something",
  "24:18": "so if we look at e: It's just a number like pi so simply, it's just a number that has",
  "24:27": "a particular value right? So if we go e Squared and we look at It's going to be a tensor,",
  "24:42": "use PyTorch, make it a float:There we go and you can see that these are the same number",
  "24:53": "so that's what torch.exp means. Okay, so you know for me when I see these kinds of interesting",
  "25:04": "functions, I don't worry too much about The definition what I care about is the shape",
  "25:10": "alright So you can have a play around with graphing calculators or whatever to kind of",
  "25:14": "see Why it is that you end up with this shape from this particular equation but for me,",
  "25:20": "I just never think about that it never Really matters to me what's important is this sigmoid",
  "25:27": "shape, which is what we want. It's something that squashes every number to be between naught",
  "25:34": "and 1 So we can change em nest loss to be exactly the same as it was before but first",
  "25:41": "we can make everything into sigmoid First and then use torch,where() so that is a loss",
  "25:50": "function that has all the properties we want. It'sAt something which is going to be have",
  "25:54": "not have any of those nasty Zero gradients and we've ensured that the input to the where()",
  "26:03": "between naught and one SoThe reason we did this is because our our accuracy Was Kind",
  "26:16": "of what we really care about is a good accuracy. We can't use it to get our gradients. Just",
  "26:22": "just to create our steps to improve our parameters So we can change Our our accuracy to another",
  "26:34": "function that is similar in terms of it It's better when the accuracy is better, but it",
  "26:40": "also does not have these zero gradients And so you can see now where why we have a metric",
  "26:46": "and a loss the metric is the thing we actually care about the loss is the thing that's similar",
  "26:51": "to what we care about but has a nicely behaved gradient. Sometimes the thing you care about",
  "27:00": "your metric does have a nicely defined gradient and you can use it directly as a lossFor example,",
  "27:05": "we often use means squared error but for classification unfortunately notSo we need to now use this",
  "27:15": "toTo update the parameters And so there's a couple of ways we could do this one would",
  "27:24": "be to loop through every image, Calculate a prediction for that image and then calculate",
  "27:33": "a loss and then do a step andThen step the other parameters and then do that again for",
  "27:39": "the next image in the next image in the next image. That's going to be really slow. Because",
  "27:45": "we're we're doing a single step for a single image. So that would mean an epoch would take",
  "27:51": "quite a while. We could go much faster, By doing every single image in the data set so",
  "27:59": "a big matrix multiplication It can all be paralyzed on the GPU and then so then we can",
  "28:05": "We could then do a step based on the gradients looking at the entire dataset but now that's",
  "28:14": "going to be like a lot of work to just update the weights once remember sometimes our datasets",
  "28:21": "have Millions or tens of millions of items. So that's probably a bad idea too. So why",
  "28:28": "not compromise? Let's grab a few data items at a timeTo calculate our loss and our step",
  "28:36": "now if we grab a few data items at a time those two data items are called a mini batch",
  "28:40": "and a mini batch just means a few pieces of dataAnd so the size of your mini batch is",
  "28:49": "called not surprisingly the batch size, right? so the bigger the batch size the closer you",
  "28:54": "get to the full size of your data set the longer it's going to take to Calculate a singleSet",
  "29:00": "of losses a single step But the more accurate it's going to be, it's going to be like, the",
  "29:06": "gradients are going to be much closer to the true data set gradients. And then the smaller",
  "29:11": "the batch size the faster each step we'll be able to do, but those steps will represent",
  "29:18": "a smaller number of items and so they won't be such an accurate approximation of the real",
  "29:23": "gradient of the whole dataset. Is there a reason the mean of the loss is calculated",
  "29:31": "over, say, doing a median, since the median is less prone to getting influenced by outliers?",
  "29:36": "In the example you gave, if the third point which was wrongly predicted as an outlier,",
  "29:43": "then the derivative would push the function away while doing SGD, and a median could be",
  "29:48": "better in that case. Honestly, I've never tried using a median. The problem with a median",
  "29:57": "is, it ends up really only caring about one number, which is the number in the middle.",
  "30:03": "So it could end up really pretty much ignoring all of the things at each end, and all it",
  "30:10": "really cares about is the order of things. So my guess is that you would end up with",
  "30:14": "something that is only good at predicting one thing in the middle. But I haven't tried",
  "30:21": "it. It\u2019d be interesting to see. Well, I guess the other thing that would happen with",
  "30:26": "a median is, you would have a lot of zero gradients, I think. Because it's picking the",
  "30:31": "thing in the middle, and you could, you know change your values, and the thing in the middle,",
  "30:36": "well wouldn't be zero gradients, but bumpy gradients. I think in the middle would suddenly",
  "30:40": "jump to being a different item. So it might not behave very well. That's my guess. You",
  "30:48": "should try it. Okay, so how do we ask for a few items at a time? It turns out that Pytorch",
  "30:59": "and Fastai provide something to do that for you. You can pass in any dataset to this class",
  "31:07": "called DataLoader and it will grab a few items from that dataset at a time. You can ask for",
  "31:13": "how many by asking for a batch size, and then, as you can see, it will grab a few items at",
  "31:22": "a time until it's grabbed all of them. So here, I'm saying let's create a collection",
  "31:26": "that just contains all the numbers from nought to 14. Let's pass that into a DataLoader with",
  "31:31": "a batch size of 5, And then, that's going to be something called an iterator, in Python.",
  "31:37": "It's something that you can ask for one more thing from an iterator. If you pass an iterator",
  "31:41": "to list in Python, it returns all of the things from the iterator. So here are my three mini-batches,",
  "31:48": "and you'll see here all the numbers from nought to 15 appear. They appear in a random order,",
  "31:52": "and they appear five at a time. They appear in random order, because shuffle = True. So",
  "31:58": "normally in the training set we ask for things to be shuffled, so it gives us a little bit",
  "32:02": "more randomization. More randomization is good, because it makes it harder for it to",
  "32:09": "learn what the dataset looks like. So that's how our DataLoader is created. Now, remember",
  "32:19": "though, that our datasets actually return tuples, and here I've just got single ints.",
  "32:28": "So let's actually create a tuple. So if we enumerate all the letters of English, then",
  "32:33": "that means that returns (0, \u2018a\u2019), (1, \u2018b\u2019), (2, \u2019c\u2019) etc. Let's make that",
  "32:39": "our dataset. So, if we pass that, to a DataLoader with a batch size of 6, and as you can see,",
  "32:46": "it returns tuples containing 6 of the first things, and the associated 6 of the second",
  "32:56": "things. So this is like our independent variable and this is like our dependent variable. Okay,",
  "33:04": "and so at the end, you know that with the batch size weren't necessarily exactly divided",
  "33:11": "nicely into the full size of the Dataset, you might end up with a smaller batch. So",
  "33:20": "basically then, we already have a Dataset remember, and so we could pass it to a DataLoader",
  "33:27": "and then we can basically say this. An iterator in Python is something that you can actually",
  "33:32": "loop through. So when we say for in DataLoader, it's going to return a tuple. We can de-structure",
  "33:39": "it into the first bit and the second bit, and so that's going to be our x and y. We",
  "33:45": "can calculate our predictions, we can calculate our loss from the predictions and the targets,",
  "33:51": "we can ask it to calculate our gradients and then we can update our parameters just like",
  "33:58": "we did in our toy SGD example for the quadratic equation. So that's re-initialize our weights",
  "34:06": "and bias with the same two lines of code before, let's create the data loader this time from",
  "34:11": "our actual MNIST dataset and create a nice big batch size, so we do plenty of work each",
  "34:16": "time, and just to take a look. Let's just grab the first thing from the \u2018DataLoader\u2019.",
  "34:22": "\u2018first\u2019 is a fast AI function, which just grabs the first thing from an iterator. It's",
  "34:26": "just, it\u2019s useful to look at, you know, kind of an arbitrary mini batch. So here is",
  "34:33": "the shape we're going to have. The first mini batch is 256 rows of 784 long, that's 28 by",
  "34:40": "28. So 256 flattened out images, and 256 labels that are 1. Well, because there's just the",
  "34:48": "number 0 or the number 1, depending on whether as a 3 or a 7. We do the same for the validation",
  "34:54": "set. So here's a validation DataLoader\u2026 And so let's grab a batch here, testing, pass",
  "35:08": "it into, well, why do we do that? We should\u2026 What\u2026 Look\u2026 Yeah, I guess, yeah, actually",
  "35:19": "for our testing, I'm going to just manually grab the first four things just so that we",
  "35:24": "can make sure everything lines up. So... So let's grab just the first four things. We'll",
  "35:28": "call that a batch. Pass it into that linear function, we created earlier. Remember linear,",
  "35:37": "was just, x batch at weights matrix multiply, plus bias. And so that's going to give us",
  "35:54": "four results. That's a prediction of each of those four images. And so then we can calculate",
  "36:00": "the loss, using that loss function we just used, and let's just grab the first four items",
  "36:05": "of the training set, and there's the loss. Okay. And so now we can calculate the gradients,",
  "36:13": "and so the gradients are 784 by 1, so in other words it's a column where every weight as",
  "36:21": "a gradient. It's what's the change in loss for a small change in that parameter, and",
  "36:29": "then the bias as a gradient it\u2019s a single number, because the bias is just a single",
  "36:34": "number. So we can take those three steps and put it in a function. If you pass... If you...",
  "36:42": "This is \u2018calculate gradient\u2019. You pass it an X batch, a Y batch, and some model,",
  "36:47": "then it's going to calculate the predictions, calculate the loss, and do the backward step.",
  "36:53": "And here we see \u2018calculate gradient\u2019, and so we can get the, just to take a look,",
  "36:57": "the mean of the weights gradient, and the bias gradient. And there it is. If I call",
  "37:03": "it a second time, and look. Notice I have not done any step here. This is exactly the",
  "37:10": "same parameters. I get a different value. That\u2019s a concern. You would expect to get",
  "37:16": "the same gradient every time you called it with the same data. Why have the gradients",
  "37:22": "changed? That's because, \u2018loss.backward\u2019 does not just calculate the gradients. It",
  "37:29": "calculates the gradients, and adds them to the existing gradients. The things in the",
  "37:35": "\u2018.grad\u2019 attribute. The reasons for that we'll come to later, but for now the thing",
  "37:41": "to know is just it does that. So actually what we need to do is to call \u2018grad.dot.zero_\u2019.",
  "37:49": "So \u2018dot.zero\u2019 returns a tensor containing zeros, and remember \u2018_\u2019 does it in place",
  "37:56": "so that updates the \u2018weights.grad\u2019 attribute, which is a tensor, to contain zeros. So now",
  "38:03": "if I do that, and call it again, I will get exactly the same number. So here is how you",
  "38:11": "train one epoch with SGD. Loop through the DataLoader, grabbing the X batch, and the",
  "38:17": "Y batch, calculate the gradient, prediction, loss backward. Go through each of the parameters.",
  "38:27": "We're going to be passing those in. So there's going to be the 768 weights, and the one bias,",
  "38:33": "and then, for each of those, update the parameter, to go minus equals gradient times learning",
  "38:40": "rate. That's our Gradient Descent step. And then zero it out for the next time around",
  "38:48": "the loop. I'm not just saying p minus equals. I'm saying \u2018p.data\u2019 minus equals, and",
  "38:56": "the reason for that is that, remember, PyTorch keeps track of all of the calculations we",
  "39:01": "do, so that it can calculate the gradient. Well, I don't want to calculate the gradient",
  "39:07": "of my Gradient Descent step. That's like not part of the model, right? So dot data is a",
  "39:14": "special attribute in Pytorch, where if you write to it, it tells Pytorch not to update",
  "39:22": "the gradients using that calculation. So this is your most basic, standard SGD, stochastic",
  "39:30": "gradient descent loop. So now we can answer that earlier question. The difference between",
  "39:35": "stochastic gradient descent and gradient descent, is that gradient descent does not have this",
  "39:41": "here that loops through each mini-batch. For gradient descent it does it on the whole dataset",
  "39:49": "each time around. So train epoch or gradient descent, would simply not have the for loop",
  "39:56": "at all, but instead it would calculate the gradient for the whole dataset and update",
  "40:01": "the parameters based on the whole dataset, which we never really do in practice. We always",
  "40:05": "use mini-batches of various sizes. Okay, so we can take the function we had before where",
  "40:21": "we compare the predictions to whether that, well that, we used to be comparing the predictions",
  "40:26": "to whether they were greater or less than zero, right? But now that we're doing the",
  "40:29": "sigmoid, remember the sigmoid will squish everything between naught and one. So now",
  "40:34": "we should compare the predictions to whether they're greater than 0.5 or not. If they're",
  "40:38": "greater than 0.5 ,just to look back at our sigmoid function. So zero, what used to be",
  "40:48": "zero, is now, on the sigmoid is 0.5. Okay, so we need to, just to make that slight change",
  "40:54": "to our measure of accuracy. So to calculate the accuracy for some X-batch and some Y-batch,",
  "41:06": "oh this is actually assumed this is actually the predictionsThen we take the sigmoid of",
  "41:12": "the predictions. We compare them to 0.5 to tell us whether it's a 3 or not, we check",
  "41:17": "what the actual target was, to see which ones are correct, and then we take the mean of",
  "41:22": "those, after converting the booleans to floats. So we can check that accuracy. Let's take",
  "41:29": "our batch, put it through our simple linear model, compare it to the four items of the",
  "41:35": "training set, and there's the accuracy. So if we do that for every batch in the validation",
  "41:42": "set, then we can loop through with a list comprehension, every batch in the validation",
  "41:47": "set, get the accuracy based on some model, stack those all up together, so that this",
  "41:57": "is a list, right? So, if we want to turn that list into a tensor, where the items of the",
  "42:02": "list, of the tensor, are the items of the list. That's what stack does. So we can stack",
  "42:07": "up all those, take the mean, convert it to a standard Python scalar, we're calling that",
  "42:16": "item, round it to four decimal places just for display, and so here is our validation",
  "42:22": "set accuracy. As you would expect, it's about 50% because it's random. So we can now train",
  "42:29": "for one epoch. So we can say, remember \u201ctrain_epoch\u201d needed the parameters? So, our parameters",
  "42:37": "in this case are the weights tensor and the bias tensor. So train one epoch using the",
  "42:44": "\u201clinear1\u201d model with the learner, with a learning rate of one, with these two parameters,",
  "42:51": "and then validate, and look at that! Our accuracy is now 68.8%. So we've trained an epoch. So",
  "43:02": "let's just repeat that many times. Train and validate, and you can see the accuracy goes",
  "43:09": "up and up and up and up and up to about 97%. So that\u2019s coole! Ae've built an SGD optimizer",
  "43:20": "of a simple linear function that is getting about 97% on our simplified MNIST where there's",
  "43:28": "just the threes in the sevens. So a lot of steps there, let's simplify this through some",
  "43:36": "refactoring. So the kind of simple refactoring we're going to do, we're going to do a couple,",
  "43:41": "but the basic idea is, we're going to create something called an optimizer class. The first",
  "43:46": "thing we'll do is, we'll get rid of the \u201clinear1\u201d function. But remember the \u201clinear1\u201d function",
  "43:54": "does \u2018x\u2019 \u2018@\u2019 \u2018w\u2019 plus \u2018b\u2019. There's actually a class in Pytorch that does",
  "44:02": "that equation for us, so we may as well use it. It's called nn.linear, and nn.linear does",
  "44:09": "two things, it does that function for us, and it also initializes the parameters for",
  "44:16": "us, so we don't have to do weights and bias init_params anymore. We just create an nn.linear",
  "44:26": "class and that's going to create a matrix of size (28, 28, 1) and a bias of size 1.",
  "44:35": "It will set requires_grad=True for us. It's all going to be encapsulated in this class,",
  "44:40": "and then when I call that as a function, it's going to do my X hat W + B. So to see the",
  "44:51": "parameters in it, we would expect it to contain 784 weights and 1 bias, we can just call that",
  "44:58": "parameters and we can destructure it to w, b and see, yep! It is 784 and 1 for the weights",
  "45:07": "and bias. So that's cool. So this is just, you could, you know, it could be an interesting",
  "45:12": "exercise for you to create this class yourself, from scratch. You should be able to, at this",
  "45:18": "point. So that you can confirm that you can recreate something that behaves exactly like",
  "45:23": "nn.linear. So, now that we've got this object which contains our parameters in a parameters",
  "45:31": "method, we could now create an optimizer. So if your optimizer we're going to pass it",
  "45:38": "the parameters to optimize and a learning rate, we\u2019ll store them away and we'll have",
  "45:44": "something called step which goes through each parameter and does that thing we just saw:",
  "45:50": "p.data -= p.grad times learning rate. And it's also going to have something called zero",
  "45:55": "grad, which goes through each parameter and zeroes it out, or we could even just set it",
  "45:59": "to None. So that's the thing we're going to call Basic Optimizer. So those are exactly",
  "46:05": "the same lines of code we've already seen wrapped up into a class. So we can now create",
  "46:10": "an optimizer, passing in the parameters of the linear model, these, and our learning",
  "46:16": "rate, and so now our training loop is: look through each mini batch in the data loader,",
  "46:23": "calculate the gradient, opt.step, opt.zero_grad, that's it! Validation function doesn't have",
  "46:33": "to change, and so let\u2019s put our training loop into a function, that's going to loop",
  "46:38": "through a bunch of epochs, call an epoch, print validate_epoch and then run it. And",
  "46:47": "it's the same! We're getting a slightly different result here, but it\u2019s much the same idea.",
  "46:55": "Okay, so that's cool, right, we've now refactoring using, you know, create our own optimizer",
  "47:06": "and using PyTorch built-in nn.linear class. And you know, by the way, we don't actually",
  "47:13": "need to use our own BasicOptim. Not surprisingly, PyTorch comes with something which does exactly",
  "47:19": "this, and not surprisingly it's called SGD. So, and actually this SGD is provided by fastai:",
  "47:27": "fastai and PyTorch provide some overlapping functionality. Then it works much the same",
  "47:31": "way, so you can pass to SGD your parameters and your learning rate, just like BasicOptim,",
  "47:39": "okay? And train it, and get the same result. So, as you can see, these classes that are",
  "47:48": "in fastai and PyTorch, are not mysterious, they're just pretty, you know, thin wrappers",
  "47:57": "around functionality that we've now written ourselves. So there's quite a few steps there,",
  "48:03": "and if you haven't done gradient descent before, then there's a lot of unpacking. So, this",
  "48:09": "lesson is kind of the key lesson. It's the one where, you know, like we should, you know,",
  "48:16": "really take us, stop and a deep breath at this point, and make sure you're comfortable.",
  "48:22": "What's the data set? What's the data loader? What's nn.linear? What's SGD? And if, you",
  "48:29": "know, if one, any or all of those don't make sense, go back to where we defined it from",
  "48:34": "scratch using Python code. Well the data loader we didn't define from scratch, but it, you",
  "48:41": "know, the functionality is not particularly interesting. You can certainly create your",
  "48:45": "own from scratch if you wanted to--that would be another pretty good exercise! Let's refactor",
  "48:53": "some more. fastai has a \u2018dataloaders\u2019 class, which as we've mentioned before is",
  "49:00": "a tiny class, that just you pass it a bunch of dataloaders and it just stores them away",
  "49:07": "as a .train and a .valid. Even though it's a tiny class, it's super handy, because with",
  "49:13": "that we now have a single object that knows all the data we have: and so it can make sure",
  "49:20": "that your training dataloader is shuffled and your validation loader isn't shuffled,",
  "49:25": "you know, make sure everything works properly. So that's what the dataloaders class is: you",
  "49:30": "can pass in the training and valid dataloader. And then the next thing we have in fastai",
  "49:35": "is the learner class. And the learner class is something where we're going to pass in",
  "49:40": "our dataloaders, we're going to pass in our model, we're going to pass in our optimization",
  "49:49": "function, we're going to pass in our loss function, we're going to pass in our metrics.",
  "49:54": "So all the stuff we've just done manually--that's all learner does! It's just going to do that",
  "50:01": "for us. So it's just going to call this train_model and this train_epoch. It's just you know,",
  "50:08": "it's inside learner. So now if we go learn.fit(), you can see again, it's doing the same thing,",
  "50:17": "getting the same result. And it's got some nice functionality. It's printing it out into",
  "50:22": "a pretty table for us, and it's showing us the losses and the accuracy and how long it",
  "50:27": "takes. But there's nothing magic, right? You've been able to do exactly the same thing by",
  "50:32": "hand using Python and PyTorch. Okay, so these abstractions are here to let you write less",
  "50:40": "code and to save some time and to save some cognitive overhead, but they're not doing",
  "50:46": "anything you can't do yourself. And that's important, right? Because if they're doing",
  "50:53": "things you can't do yourself, then you can't customize them, you can't debug them, you",
  "50:59": "know, you can't profile them. So we want to make sure that the stuff we're using is stuff",
  "51:07": "that we understand what it's doing. So this is just a linear function, it\u2019s not great:",
  "51:15": "we want a neural network. So, how do we turn this into a neural network? Remember this",
  "51:22": "is a linear function x@w+B. To turn it into a neural network, we have two linear functions,",
  "51:33": "exactly the same but with different weights and different biases and in between, this",
  "51:37": "magic line of code, which takes the result of our first linear function and then does",
  "51:43": "a max() between that and 0. So a max() of res and 0 is going to take any negative numbers",
  "51:53": "and turn them into 0\u2019s. So we're going to do a linear function, we're going to replace",
  "51:58": "the negatives with 0 and then we're going to take that and put it through another linear",
  "52:02": "function. That (believe it or not) is a neural net! So, w1 and w2 were weight tensors b1",
  "52:11": "and b2 are bias tensors (just like before) so we can initialize them (just like before)",
  "52:17": "and we can now call exactly the same training code that we did before to roll these. So",
  "52:26": "res.max(0) is called a rectified linear unit. Which you will always see referred to as ReLU.",
  "52:39": "In PyTorch it already has this function--it's called f.relu(). And so if we plot it you",
  "52:47": "can see it's as you'd expect, it's 0 for all negative numbers and then it's y=x for positive",
  "52:54": "numbers. Here's some jargon \u201crectified linear unit\u201d sounds scary, sounds complicated,",
  "53:05": "but it's actually this incredibly tiny line of code, this incredibly simple function.",
  "53:11": "And this happens a lot in deep learning. Things that sound complicated and sophisticated and",
  "53:17": "impressive turnout to be normally super simple, frankly... At least once, you know what it",
  "53:23": "is... So: Why do we do: Linear layer ReLu Linear Layer: Well if we got rid of the middle",
  "53:34": "If we got rid of the middle ReLu and just went linear layer linear layer then you could",
  "53:45": "rewrite that as a single linear layer when your multiply things and add and then multiply",
  "53:53": "things and add andYou can just change the coefficients and make it into a single multiply",
  "53:56": "and then addSo no matter how many linear layers we stack on top of each other we can never",
  "54:01": "make anything moreAnd of effective than a simple linear modelBut if you put a non-linearity",
  "54:10": "between the linear layersThen actually you have the opposite. This is now where something",
  "54:16": "called the universal approximation theorem holds which is that if the size of the weight",
  "54:22": "and bias matrices are big enoughThis can actually approximate any arbitrary function including",
  "54:29": "the function of how do I recognize threes from sevens or Or whateverSo that's kind of",
  "54:38": "amazing, right this tiny thing is actually a universal function approximator as long",
  "54:44": "as you have W1 B1 W2 and B2Have the right numbers and we know how to make them the right",
  "54:50": "numbers we use SGDCould take a very long time. It could take a lot of memoryBut the basic",
  "54:59": "idea is that there is some solution to any computable problem andThis is one of the biggest",
  "55:06": "challengesA lot of beginners have to deep learning is that there's nothing else to it",
  "55:14": "like that? There's often this likeOkay, how do I make a neural net?Oh, that is the neural",
  "55:20": "net. Well, how do I, do deep learning training with SGDthere's things to like Make a train",
  "55:28": "a bit faster. There's you know things to mean you need a few less parameters but everything",
  "55:35": "from here is justPerformance tweaks honestly, rightSo this is you know, this is the key",
  "55:47": "understanding of of training a neural networkOkay, we can simplify things a bit more We already",
  "55:58": "know that we can use nn.linear to replaceTheir weight and bias, so let's do that for both",
  "56:05": "of the linear layers, and then since we're simply takingThe result of one function and",
  "56:17": "passing it into the nextThe result of that function passive to the next and so forth",
  "56:22": "and then returned the end this is called function composition function composition is when you",
  "56:27": "justTake the result of one function pass it to a new one take a result of one function.",
  "56:31": "Pass it to a new one and so every pretty much neural network is just doing function composition",
  "56:39": "of linear layers and these are called activation functions or nonlinearities So PyTorch provides",
  "56:47": "something to do function composition for us and it's called nn.sequential so it's gonna",
  "56:53": "do a linear layer pass the result to a ReLu you pass the result to a linear layerYou'll",
  "56:59": "see here. I'm not using F.ReLU. I'm using nn.ReLU This is identical returns exactly",
  "57:04": "the same thing, but this is a classRather than a function. Yes, Rachel \u201cBy using the",
  "57:13": "non-linearity Won't using a function that makes all negative output zero make many of",
  "57:19": "the gradients in the network zero and stop the learning process dueto many zero gradients?\u201d",
  "57:25": "Well, that's a fantastic question and the answer is yes, it doesBut they won't be zero",
  "57:34": "for every image and remember the mini-batches a shuffled soEven if it's zero for every image",
  "57:40": "in one mini batch, it won't be for the next mini batchAnd it won't be the next time around",
  "57:45": "we go for another epoch. So Yes, it can create zeros and ifThe neural net ends up with a",
  "57:54": "set of parametersThat's that lots and lots of inputs end up as zeros. You can end up",
  "58:01": "with whole mini batches that are zero andYou can end up in a situation where some of the",
  "58:09": "neurons remain In active inactive means their zero and they're basically dead units And",
  "58:18": "this is a huge problemIt basically means you're wasting computationSo there's a few tricks",
  "58:25": "to avoid that which we'll be learning about a lot one. Simple trick is toNot make this",
  "58:33": "thing flat here, but just make it a less steepMine that's called a leakyReLU. Well, you leaky",
  "58:40": "rectified linear unit andIt that they helped a bitAs well learn though even better is to",
  "58:47": "make sure that we just kind of initialize to sensible initial values that are not too",
  "58:53": "big and not too small and step by sensible initial values that are particularly not too",
  "58:58": "big and generally if we do that we can keep things in the zone where they're positive",
  "59:03": "most of the time but we are going to learn about how to actually analyze inside a network",
  "59:08": "and find out how many dead units we have how many of these zeros we have becauseAs is as",
  "59:13": "you point out they are they are bad news. They don't do any work and they will Continue",
  "59:18": "to not do any work if enough of the inputs end up being zeroOkay, so now that we've got",
  "59:32": "a neural netWe can use exactly the same learner we had before but this time we pass in the",
  "59:37": "simple netInstead of the linear one. Everything else is the same and we can call fit just",
  "59:45": "like before andGenerally as your models get deeper. So here we've gone from one layer-twoAnd",
  "59:52": "I'm only counting the parameterised layers as layers. You could say it's three. I was",
  "59:56": "going to call it two. There's twoTrainable layers. So I've gone from one layer to I've",
  "60:01": "checked dropped my learning rate from 1 to 0.1because the deeper models all tend to be",
  "60:08": "kind of bumpier less nicely behaved so often you need to use lower learning ratesAnd so",
  "60:14": "we trained it for awhile okay, andCan actually find out what that training looks like by",
  "60:22": "looking inside our learner and there's an attribute we create for recorder andThat's",
  "60:27": "going to recordWell everything that appears in this tableBasically, well these three things",
  "60:33": "the training loss the validation loss and the accuracy or any metricsso recorded values",
  "60:40": "contains that kind of table of results and so item number two ofEach row will be the",
  "60:49": "accuracy and so the capital L class, which I'm using here as a nice little method called",
  "61:00": "itemgot that will will getThe second item from every row and then I can plot thatHow",
  "61:10": "the training went and I can get the final accuracyBy grabbing the last row of the table",
  "61:19": "and grabbing the second It's indexed to 0 1 2 then my final accuracy. Not bad98.3%So",
  "61:31": "this is pretty amazing, we now have a function that can solve any problemTo any level of",
  "61:37": "accuracy if we can find the right parameters and we have a way to findHopefully the best",
  "61:44": "or at least a very good that our parameters for any functionSo this is kind of the magic",
  "61:51": "yes, RachelHow could we use what we're learning here to get an idea of what the network is",
  "61:58": "learning along the wayLike Zieler and Fergus did more or lessWe will look at that laterNot",
  "62:07": "in the full detail of their paper but basically you can look in the dot parameters to see",
  "62:15": "the values of those parametersand at this point. Well, I mean, why don't you try it",
  "62:22": "yourself? Right? You've actually got nowThe parameters, so if you want to grab the model",
  "62:30": "you can actually see the learned.modelSo we can we can look inside learn.model to see",
  "62:38": "the actual model that we just trained and you can see it's got the three things in it.",
  "62:48": "They're linear then ReLU than linear, and you know, what I kind of like to do is to",
  "62:53": "put that into a variable, make it a Bit easy to work with, you can grab one layer by indexing",
  "63:01": "in parameters and that just gives me something called a generator. It's something that will",
  "63:11": "give me a list of the parameters when I ask for them. I could just go weight comma bias",
  "63:16": "equals to de-structure them and so the weight Here's 30 by 784: because that's what I asked",
  "63:31": "for. So one of the things to note here is that to create a Neural Net so something that's",
  "63:41": "more than one layer. I actually have 30 outputs not just one right so I'm kind of generating",
  "63:48": "lot so if you can think of as generating lots of featuresSo it's kind of like 30 different",
  "63:52": "linear of linear models here and then I combined those 30 back into one. So you could look",
  "64:00": "at one of those by having a look at here, so there's the numbers in the first row, we",
  "64:12": "could reshape that  into the the original shape of the images",
  "64:24": "and we could even have a look and there it is right? So you can see this is something",
  "64:34": "So this cool right we can actually see here we've got something which isWhich is kind",
  "64:44": "of learning to find things at the top and the bottom and the middle And so we could",
  "64:50": "look at the second one. Okay, no idea what that's showing and so some of them are kind",
  "64:59": "of you know, I probably got far more than I need which is why they're not that obvious.",
  "65:03": "But you can see yeah, here's another thing that's looking pretty similar kind of looking",
  "65:10": "for this little bit in the middle, so yeah, this is the basic idea to understand the features",
  "65:18": "that are not the first layer but later layers, you have to be a bit more sophisticated but",
  "65:24": "yeah to see the first layer ones you can you can just plot themOkay, so then, you know",
  "65:33": "just to compare we could use the full fastai toolkit so grab our data loaders by using",
  "65:41": "data loaders from folder as we've done before and create a cnn_ learner and a ResNet and",
  "65:47": "fit it for a single epoch and, WOAH, 99.7! All right, so we did 40 epochs and got 98.3",
  "65:58": "as I said using all the tricks you can really speed things up and make things a lot better",
  "66:05": "and so by the end of this course or at least both parts of this course, you'll be able",
  "66:12": "to from scratch at this 99.7 in a single epoch, all right, so Jargon! So jargon: just remind",
  "66:29": "us ReLU function that returns zero for negatives mini-batch a few inputs and labels, which",
  "66:37": "optionally are randomly selected the forward pass is the bit where we calculate the predictions",
  "66:43": "the loss is the function that we're going to take the derivative of and then the gradient",
  "66:48": "is the derivative of the loss with respect to each parameter the backward pass is when",
  "66:54": "we calculate those gradients gradient descent is that full thing of taking a step in the",
  "67:00": "direction opposite to the gradients by capital after calculating the loss andThen the learning",
  "67:05": "rate is the size of the step that we take Other things to know, perhaps the two most",
  "67:18": "important pieces of jargon are all of the numbers that are in a neural network the numbers",
  "67:24": "that we're learning are called parameters and then the numbers that we're calculating",
  "67:29": "so every value that's calculated every matrix multiplication element that's calculated:",
  "67:35": "They're called activations so activations and parameters are all of the numbers in the",
  "67:41": "neural net and so be very careful when I say from here on in in these lessons activations",
  "67:47": "or parameters. You've got to make sure you know what those mean because that's that's",
  "67:52": "the entire basically almost the entire set of numbers that exist inside a neural net",
  "67:58": "so activations are calculated, Parameters are learned. We're doing this stuff with Tensors",
  "68:06": "and Tensors are just regularly shaped to arrays, rank zero tensors, we call scalars, rank 1",
  "68:12": "tensor:. we call vectors rank two tensors we call matrices and we continue on to rank",
  "68:18": "3 tensors rank 4 tensors and so forth and rank five tensors are very common in deep",
  "68:24": "learning. So don't be scared of going up to higher numbers of dimensions. Okay, so let's",
  "68:30": "have a break oh we got a question, okay R: \u201cIs there a rule of thumb for what non-linearity",
  "68:38": "to choose given that there are many?\u201d Yeah, there are many non-linearities to choose from",
  "68:43": "and it doesn't generally matter very much which you choose so just use ReLU or Leaky",
  "68:50": "ReLU or yeah, whatever any anyone should work fine later on we'll we'll look at the minor",
  "68:58": "differences between between them but it's not so much something that you pick on a per",
  "69:04": "problem it's more like some take a little bit longer and a little bit more accurate",
  "69:09": "and some over it faster and a little bit less accurate. That's a good question, okay. So",
  "69:15": "before you move on it's really important that you finish the questionnaire for this chapter",
  "69:20": "because there's a whole lot of concepts that we've just done so, you know try to go through",
  "69:25": "the questionnaire go back and relook at the notebook and please run the code through the",
  "69:31": "cat experiments and make sure it makes senseAll right. Let's have a seven minute break see",
  "69:38": "you back here in seven minutes time. Okay, welcome back, so now that we know how to create",
  "69:52": "and train a Neural Net. Let's cycle back and look deeper at some applications. And so we're",
  "69:59": "going to try to kind of interpolate in from one end we've done they're kind of from scratch",
  "70:06": "version at the other end we've done the kind of four lines of code version and we're going",
  "70:10": "to gradually nibble at each end until we find ourselves in the middle and we've we've we've",
  "70:16": "touched on all of itso let's go back up to the kind of the four lines of code version",
  "70:21": "and and delve a little deeper. So, let's go back to PETs and let's think though about",
  "70:33": "like: How do you actually, start with a new dataset and figure out how to use it so, you",
  "70:44": "know the the data sets we provide it's easy enough to untar them you to say untar that",
  "70:51": "will download it and untar it. If it's a data set that you're getting you can just use the",
  "70:58": "terminal or [?]a Python or whatever, so, let's assume we have a path that's pointing at something",
  "71:07": "so initially you don't you don't know what that something is, so we can start by doing",
  "71:13": "LS to have a look and see what's inside there. So the PETs data set that we saw in Lesson",
  "71:19": "one contains three things annotations images and models and you'll see we have this little",
  "71:26": "trick here where we say path.BASE_ path equals and then the path to our data and that just",
  "71:32": "does a little simple thin:. Where when we print it out, it just doesn't show us. It",
  "71:36": "just shows us relative to this path, which is a bit convenient. So, go and have a look",
  "71:46": "at the readme for the original PETs dataset, it tells you what these images and annotations",
  "71:52": "folders are and not surprisingly the images path, so if we go path slash images, that's",
  "71:59": "how we use PathLib to grab the sub directory and then LS we can see here are the names",
  "72:06": "that the paths through the images. As it mentions here most functions and methods in fastai",
  "72:13": "which returned a collection don't return a Python list that they returned a capital L",
  "72:20": "and a capital L as we briefly mentioned is basically an enhanced list. One of the enhancements",
  "72:26": "is the way it prints the representation of it starts by showing you. How many items there",
  "72:31": "are in the list in the collection: so there's 7349 images and, It it if there's more than",
  "72:39": "ten things it truncates it and just says dot dot to avoid filling up your screen, so there's",
  "72:48": "a couple of little conveniences there, and so we can see from this output that a file",
  "72:55": "name as we mentioned in lesson 1 if the first letter is a capital it means it's a Cat and",
  "73:04": "if the first letter is lowercase it means it's a dog, but this time we've got to do",
  "73:09": "something a bit more complex a lot more complex which is figure out what breed it is and so",
  "73:14": "you can see the breed is kind of everything up to after the in the file name: It's everything",
  "73:20": "up to the the last underscore and before this number is the breed. So we want to label everything",
  "73:29": "with its breed, so we're going to take advantage of this structure, so the way I would do this",
  "73:41": "is to use a regular expression. A regular expression is something that looks at a string",
  "73:46": "and basically lets you kind of pull it apart into its pieces in very flexible way. It is",
  "73:51": "kind of simple little language for doing that. Um, if you haven't used regular expressions",
  "73:57": "before um, please Google regular expression tutorial now and look it's going to be like",
  "74:02": "one of the most useful tools you'll come across in your life. I use them almost every day",
  "74:06": ".I will go to details about how to use them since there's so many great tutorials. And",
  "74:13": "there's also a lot of great like exercises, you know, there's regex regex is short for",
  "74:19": "regular expression. There's regex crosswords, There's reges Q&A all kinds of core regex",
  "74:23": "things a lot of people like me love this tool in order to, there's also a regex lesson in",
  "74:31": "the fastAI NLP course, maybe even to regex lessons. Oh, yeah, I'm sorry for forgetting",
  "74:37": "about the first day. I know because, what an excellent resource that is! So, RegularExpressions",
  "74:47": "are how to get right the first time. So the best thing to do is to get a sample string.",
  "74:53": "So good - good way to do that would be to just grab one of the file names. So let's",
  "74:58": "pop it in Fname and then you can experiment with reg expressions. So re is the regular",
  "75:08": "expression module in Python and find all will just grab all the parts of a regular expression",
  "75:15": "that have parentheses around them. So this regular expression and R is a special kind",
  "75:20": "of string in Python which basically says don't treat backslash as special because normally",
  "75:25": "in Python like backslash n means a newline. So here's us a string, which I'm going to",
  "75:35": "capture. Any letter one or more times followed by an underscore followed by a digit one or",
  "75:43": "more times, followed by anything I probably don\u2019t have to use backslash t for this but",
  "75:50": "that\u2019s fine followed by the letters jpg followed by the end of the string and so if",
  "75:56": "I call that regular expression against my file names name, Oh! Looks good, right so",
  "76:03": "we kind of check it out! So, now that seems to work we can create a data block where the",
  "76:10": "independent variables are images the dependent variables are categories just like before",
  "76:15": "get items is going to be get image files we're going to spit it randomly as per usual and",
  "76:24": "then we're going to get the label by calling regex labeler, which is a just a handy little",
  "76:33": "fastai class which labels things with a regular expression. We can't call the regular expression",
  "76:40": "this particular regular expression directly on the path lib path object we actually want",
  "76:45": "to call it on the name attribute and fast AI has a nice little function called using",
  "76:51": "attr using attribute which takes this function and changes it to a function which will be",
  "76:57": "passed this attribute that's going to be using regex labeler on the name attribute and then",
  "77:08": "from that data block we can create the data loaders as usual there's two interesting lines",
  "77:13": "here resize and aug_transforms() aug_transforms() we have seen before in notebook 2, in the",
  "77:27": "section core data augmentation and so aug_transforms() was the thing which can zoom in and zoom out,",
  "77:36": "and warp, and rotate and change contrast and change brightness and so forth and flip, to",
  "77:43": "kind of give us almost, It's like giving us more data being generated synthetically from",
  "77:48": "the data. We already have and we also learned about random resize crop: which is a kind",
  "77:59": "of a really cool way of getting, ensuring you get square images at the same time that",
  "78:06": "you're augmenting the data here, we have a resize to a really large image but you know",
  "78:17": "by deep learning standards 460x460 is a really large image and then we're using aug_transforms()",
  "78:23": "with a size. So that's actually going to use random resize crop to a smaller size Why are",
  "78:30": "we doing that? This particular combination of two steps does something which I think",
  "78:39": "is unique to Fastai which we call pre-sizing. And the best way is, I will show you this",
  "78:46": "beautiful example of Powerpoint wizardry that I'm so excited about, to show how pre-sizing",
  "78:56": "works. What pre-sizing does, is that the first step where we say resize to 460 by 460 is,",
  "79:03": "it grabs a square, and it grabs it randomly. If it's a kind of landscape orientation photo,",
  "79:10": "it'll grab it randomly. So it'll take the whole height and randomly grab somewhere from",
  "79:14": "along the side. If it's a portrait orientation, then it will grab it, you know, take the full",
  "79:20": "width and grab a random bit from top to bottom. So then we take this area here, and here it",
  "79:28": "is, right? And so that's what the first resize does. And then the second aug_transforms bit,",
  "79:35": "will grab a random warped crop, possibly rotated, and will turn that into a square. So there's",
  "79:49": "two steps, it\u2019s first of all resize to a square that's big, and then the second step,",
  "79:54": "is to a kind of rotation and warping and zooming stage to something smaller, in this case 224",
  "80:03": "by 224. Because this first step creates something that's square, and always is the same size,",
  "80:11": "the second step can happen on the GPU. Normally, things like rotating and image warping are",
  "80:17": "actually pretty slow. Also, normally doing a zoom and rotate and a warp actually is really",
  "80:27": "destructive to the image because each one of those things requires an interpolation",
  "80:31": "step. Which it's not just slow, it actually makes the image really low quality. So we",
  "80:39": "do it in a very special way in Fastai. I think it's unique, where we do all of the all of",
  "80:46": "these kind of coordinate transforms like rotations and warps and zooms and so forth, not on the",
  "80:53": "actual pixels, but instead we kind of keep track of the changing coordinate values in",
  "80:59": "a in a non-lossy way, so the full floating-point value, and then once at the very end, we then",
  "81:05": "do the interpolation. The results are quite striking. Here is what the difference looks",
  "81:13": "like. Hopefully you can see this on the video. On the left is our pre-sizing approach, and",
  "81:23": "on the right is the standard approach that other libraries use. And you can see that",
  "81:28": "the one on the right is a lot less nicely focused, and it also has weird things like",
  "81:35": "this should be grass here, but it's actually got its bum sticking way out. This has a little",
  "81:40": "bit of weird distortions, this has got loads of weird distortions. So you can see the pre-sized",
  "81:46": "version really ends up way way better and I think we have a question, Rachel. Are the",
  "81:54": "blocks in the DataBlock an ordered list? Do they specify the input and output structures",
  "81:59": "respectively? Are there always two blocks or can there be more than two? For example,",
  "82:04": "if you wanted a segmentation model, would the second block be something about segmentation?",
  "82:10": "So, yeah, this is an ordered list. So the first item says I want to create an image,",
  "82:19": "and then the second item says I want to create a category. So that's my independent and dependent",
  "82:24": "variable. You can have one thing here, you can have three things here, you can have any",
  "82:29": "amount of things here you want. Obviously the vast majority of the time it'll be two",
  "82:33": "only: there's an independent variable and a dependent variable. We'll be seeing this",
  "82:37": "in more detail later, although if you go back to the earlier lesson when we introduced DataBlocks,",
  "82:42": "I do have a picture, kind of, showing how these pieces get together. So, after you've",
  "82:55": "put together DataBlock, created your DataLoaders, you want to make sure it's working correctly.",
  "83:00": "So the obvious thing to do for a computer vision DataBlock is show_batch and show_batch",
  "83:07": "will show you the items, and you can kind of just make sure they look sensible, that",
  "83:14": "looks like the labels are reasonable. If you add a unique=True, then it's going to show",
  "83:19": "you the same image with all the different augmentations. This is a good way to make",
  "83:23": "sure your augmentations work. If you make a mistake in your DataBlock, in this example,",
  "83:29": "there's no resize, so different images are going to be different sizes or be impossible",
  "83:34": "to collate them into a batch. So if you call \u2018.summary\u2019, this is a really neat thing,",
  "83:43": "which will go through and tell you everything that's happening. So I\u2026 Collecting the items.",
  "83:49": "How many did I find? What happened when I split them? What are the different variables,",
  "83:54": "independent, dependent variables I\u2019m creating. Let's try and create one of these. Here\u2019s",
  "84:00": "each step. Create my image. Create categorize. Here\u2019s what the first thing gave me. An",
  "84:08": "American Bulldog. Here\u2019s the final sample. Is this image, this size, this category. And",
  "84:16": "then eventually it says oh, oh, it's not possible to collate your items. I tried to collate",
  "84:21": "these zero index members of your tuples. So in other words, that's the independent variable",
  "84:26": "and I got, this was size 500 by 375, this was 375 by 500. Oh, I can't collate these",
  "84:33": "into a tensor because they're different sizes. So this is a super great debugging tool for",
  "84:38": "debugging your DataBlocks. We have a question. How does the item transforms presize work",
  "84:46": "if the resize is smaller than the image? Is a whole width or height still taken, or is",
  "84:52": "it just a random crop with the revised value? So if you remember back to Lesson 2, we looked",
  "85:04": "at the different ways of creating these things you can use squish, you can use pad, or you",
  "85:16": "can use crop. So if your image is smaller than the precise value, then squish will really",
  "85:24": "be zoom, so it will just small stretch. It'll stretch it, and then pattern crop will do",
  "85:32": "much the same thing. And so you'll just end up with a, you know, the same. This looks",
  "85:37": "like these, but it'll be a, kind of, lower, more pixelated, lower resolution because it's",
  "85:41": "having to zoom in a little bit. Okay, so a lot of people say that you should do a hell",
  "85:52": "of a lot of data cleaning before you model. We don't. We say model as soon as you can,",
  "85:59": "because remember what we found in, in Notebook 2. Your, your model can teach you about the",
  "86:05": "problems in your data. So as soon as I've got to a point where I have a DataBlock, that's",
  "86:11": "working, and I have DataLoaders, I'm going to build a model. And so here I'm, you know,",
  "86:17": "it also tells me how I'm going. So, I'm getting 7% error. Wow, that's actually really good",
  "86:23": "for a pets model. And so at this point now that I have a model I can do that stuff we",
  "86:27": "learned about earlier, in 02, the Notebook 02, where we trained our model, and used it",
  "86:32": "to clean the data. So we can look at the classification, a confusion matrix, top losses, the image",
  "86:40": "cleaner widget, you know, so forth. Okay, now one thing interesting here is in Notebook",
  "86:53": "4 we included a loss function, when we created a Learner, and here we don't pass in our loss",
  "87:00": "function. Why is that? That's because fastAI will try to automatically pick a somewhat",
  "87:07": "sensible loss function for you. And so for a image classification task it knows what",
  "87:15": "loss function is the normal one to pick, and it's done it for you, but let's have a look",
  "87:21": "and see what it actually did pick. So we could have a look at \u2018learn.loss_func\u2019 and we",
  "87:36": "will see it is cross-entropy loss. What on earth is cross-entropy loss. I'm glad you",
  "87:42": "asked. Let's find out. Cross entropy loss is really much the same as the MNIST loss",
  "87:51": "we created with that, with that, sigmoid and the one minus predictions and predictions,",
  "87:59": "but it's, it's a, kind of, extended version of that. And the extended version of that,",
  "88:05": "is that, that \u2018torch.where\u2019 that we looked at in Notebook 4, only works when you have",
  "88:11": "a binary outcome. In that case it was: \u2018Is it a three or not?\u2019 But in this case we've",
  "88:19": "got which of the thirty-seven pet breeds is it? So, we want to, kind of, create something",
  "88:26": "just like that sigmoid and \u2018torch.where\u2019, that which also works nicely for more than",
  "88:34": "two categories. So, let's see how we can do that, so first of all, let's grab a batch.",
  "88:43": "There is a\u2026 Yes? There is a question. Why do we want to build a model before cleaning",
  "88:51": "the data? I would think a clean dataset would help in training. Yeah, absolutely a current",
  "88:59": "clean dataset helps in training, but remember as we saw in notebook 02, an initial model",
  "89:07": "helps you clean the dataset. So remember how \u2018plot_top_losses\u2019 helped us identify mislabeled",
  "89:13": "images, and the confusion matrix helps us recognize which things we were getting confused,",
  "89:19": "and might need, you know, fixing and the \u2018ImageClassifierCleaner\u2019 actually let us find things like, an image",
  "89:27": "that contained two bears, rather than one bear, and cleaned it up. So a model is just",
  "89:32": "a fantastic way to help you zoom in on the data that matters, which things seem to have",
  "89:38": "the problems, which things are most important. Stuff like that. So you would go through and",
  "89:43": "you clean it, with the model helping you, and then you go back and train it again, with",
  "89:48": "the clean data. Thanks for the great question. Okay, so in order to understand cross-entropy",
  "89:59": "loss let's grab a batch of data, which we can use \u2018dls.one_batch\u2019, and that's going",
  "90:07": "to grab a batch from the training set. We could also go first(dls.train) and that's",
  "90:17": "going to do exactly the same thing. And so then we can destructure that into the independent,",
  "90:24": "dependent variable, and so the dependent variable shows us we've got a batch size of 64. So",
  "90:30": "it shows us the 64 categories. And remember those numbers simply refer to the index of,",
  "90:46": "into the vocab. So for example 16 is a boxer. And so that all happens for you automatically,",
  "90:54": "when we say \u2018show_batch\u2019, it shows us those strings. So here\u2019s a first mini-batch,",
  "91:02": "and so now we can view the predictions, that is the activations of the final layer of the",
  "91:07": "network, by calling \u2018get_prieds\u2019. And you can pass in a DataLoader, and a DataLoader",
  "91:16": "can really be anything that's going to return a sequence of mini batches. So we can just",
  "91:23": "pass in a list, containing our mini batch, as a DataLoader, and so that's going to get",
  "91:28": "the predictions for one mini batch. So here's some predictions. Okay, so the actual predictions,",
  "91:38": "if we go \u2018preds[0].sum\u2019, to grab the predictions for the first image, and add them all up,",
  "91:45": "they add up to one. And there are 37 of them. So that makes sense. Right? It's like the",
  "91:52": "very first thing is, what is the probability that that is a \u2018dls.vocab\u2019, the first",
  "92:01": "thing is what's the probability it's an Abyssinian cat. It's ten to the negative six. You see?",
  "92:08": "And so forth. So it's basically like it's not this, it's not this, it's not this, and",
  "92:12": "you can look through and, oh here this one here, you know, obviously what it thinks it",
  "92:18": "is. So how did it? You know, so we... We obviously want the probabilities to sum to one, because",
  "92:25": "it would be pretty weird if, if they didn't. It would say, you know, that the, the probability",
  "92:30": "of being one of these things is more than 1 or less than 1, which would be extremely",
  "92:35": "odd. So how do we go about creating these predictions, where each one is between zero",
  "92:44": "and one, and they all add up to 1. To do that we use something called softmax. Softmax is",
  "92:53": "basically an extension of sigmoid, to handle more than two levels, two categories. So remember",
  "93:01": "the sigmoid function looked like this. We used that for our 3s vs. 7s model. So what",
  "93:09": "if we want 37 categories, rather than two categories. We need one activation for every",
  "93:16": "category. So actually the threes and sevens model, rather than thinking of that as an",
  "93:24": "\u2018is-3\u2019 model, we could actually say: \u2018Oh that has two categories, so let's actually",
  "93:30": "create two activations. One representing how three like something is, and one representing",
  "93:35": "how seven like something is.\u2019 So let's say, you know, let's just say that we have 6 MNIST",
  "93:47": "digits and these were the... Can I do this? And this first column were the activations",
  "94:00": "of my model for, for one activation, and the second column was for a second activation.",
  "94:07": "So my final layer actually has two activations now. So this is like how much like a 3 is",
  "94:12": "it? And this is how much like a 7 is it? But this one is not at all like a 3, and it's",
  "94:17": "slightly not like a seven. This is very much like a three, and not much like a seven, and",
  "94:23": "so forth. So we can take that model, and rather having, rather than having one activation",
  "94:27": "for like, is three, we can have two activations for how much like a three, how much like a",
  "94:33": "seven. So if we take the sigmoid of that, we get two numbers between naught and one,",
  "94:41": "but they don't add up to one. So that doesn't make any sense. It can't be 0.66 chances of",
  "94:49": "three , and 0.56 chances of seven, because every digit in that data set is only one,",
  "94:54": "or the other. So that's not going to work, but what we could do is we could take the",
  "95:02": "difference between this value, and this value and, say that's how likely it is to be a three.",
  "95:09": "So in other words this one here, with a high number here, and a low number here, is very",
  "95:12": "likely to be a three. So we could basically say in the binary case, these activations,",
  "95:21": "that what really matters is their relative confidence of being a three versus a seven.",
  "95:27": "So we could calculate the difference between column one and column two, or column index",
  "95:31": "zero and column index one, right? And here's the difference between the two columns, there's",
  "95:37": "that big difference, and we could take the sigmoid of that. Right? And so this is now",
  "95:44": "giving us a single number between naught and one, and so then, since we wanted two columns,",
  "95:53": "we could make column index zero, the sigmoid, and column index one, could be one minus that,",
  "96:00": "and now look these all add up to one. So here's probability of three, probability of seven,",
  "96:08": "but the second one, probably three, probability of seven, and so forth. So like that's a way",
  "96:15": "that we could go from having two activations for every image, to creating two probabilities,",
  "96:27": "each of which is between naught and one, and each pair of which adds to one. Great. How",
  "96:36": "do we extend that to more than two columns? To extend it to more than two columns we use",
  "96:42": "this function, which is called softmax. Softmax is equal to e to the x, divided by the sum",
  "96:54": "of e to the x. Just to show you if I go softmax on my activations, I get 0.6025, 0.3975, 0.6025,",
  "97:08": "0.3975, I get exactly the same thing. Right? So softmax in the binary case, is identical",
  "97:18": "to the sigmoid that we just looked at. But in the multi category case, we basically end",
  "97:28": "up with something like this. Let's say we were doing the teddy bear, grizzly bear, brown",
  "97:32": "bear, and for that, remember, our neural net is going to have the final layer, will have",
  "97:36": "three activations. So let's say it was 0.02, -2.49, 1.25. So to calculate softmax I first",
  "97:45": "go e to the power of each of these three things, so here's e to the power of .02, e to the",
  "97:52": "power of -2.49, e to the power of 3.4, e to the power of 1.25. Ok, then I add them up",
  "97:59": "so there's the sum of the exps and then softmax will simply be 1.02 divided by 4.6 and then",
  "98:07": "this one will be 0.08 divided by 4.6. And this one will be 3.49 divided by 4.6 so since",
  "98:13": "each one of these represents each number divided by the sum, that means that the total is one.",
  "98:21": "Okay, and because all of these are positive and each one is an item divided by the sum",
  "98:28": "it means all of these must be between naught and one. So this shows you that softmax always",
  "98:34": "gives you numbers between naught and 1 and they always add up to 1. That in practice",
  "98:41": "you can just call torch dot softmax. And it will give you this result of this, this function.",
  "98:51": "So you should experiment with this in your own time, you know, write this out by hand",
  "98:57": "and try putting in these numbers, right, and, and see how that you get back the numbers",
  "99:03": "I claim you're going to get back and make sure this makes sense to you. So one of the",
  "99:08": "interesting points about softmax is, remember I told you that exp is e to the power of something,",
  "99:16": "and now what that means is that e to the power of something grows very very fast. Right?",
  "99:25": "So like exp of 4 is 54, exp of 8 is 29, 2980, right. It grows super fast and what that means",
  "99:45": "is that if you have one activation that's just a bit bigger than the others, its softmax",
  "99:51": "will be a lot bigger than the others. So intuitively the softmax function really wants to pick",
  "99:58": "one class among the others. Which is generally what you want, right, when you're trying to",
  "100:04": "train a classifier to say which breed is it. You kind of want it to to pick one and kind",
  "100:11": "of go for it, right? And so that's what softmax does. That's not what you always want. So",
  "100:19": "sometimes at inference time you want it to be a bit cautious. And so you kind of got",
  "100:23": "to remember that softmax isn't always the perfect approach but it's the default. It's",
  "100:28": "what we use most of the time and it works well on a lot of situations. So that is softmax.",
  "100:39": "Now in the binary case for the MNIST three versus sevens, this was how we calculated",
  "100:44": "the MNIST loss, we took the sigmoid and then we did either one minus that or that as our",
  "100:50": "loss function. Which is fine as you saw it, it worked, right? And so we could do this",
  "101:02": "exactly the same thing. We can't use torch dot where anymore because targets aren't just",
  "101:06": "zero or one, targets could be any number from naught to 36. So we could do that by replacing",
  "101:13": "the torch dot where with indexing. So here's an example for the binary case. Let's say",
  "101:20": "these are our targets 0 1 0 1 1 0 and these are our softmax activations which we calculated",
  "101:28": "before, they\u2019re just from some random numbers, just for a toy example. So one way to do instead",
  "101:34": "of doing torch dot where, we could instead, have a look at this, I could say I could grab",
  "101:42": "all the numbers from naught to 5 and if I index into here With all the numbers from",
  "101:49": "0 to 5 and then my targets, 0 1 0 1 0 1 1 0 then what that's going to do is it's going",
  "101:59": "to pick a row 0 it'll pick 0.6. And then for row 1 it'll pick 1, a 0.49. for row 2, it'll",
  "102:11": "pick 0, a .13, for row 4 it'll pick 1, a .003 and so forth. So this is a super nifty indexing",
  "102:24": "expression which you should definitely play with, right, and it's basically this trick",
  "102:31": "of passing multiple things to the pytorch indexer. The first thing says, which rows",
  "102:37": "should you return; and the second thing says, for each of those rows, which column should",
  "102:42": "you return? So this is returning all the rows and these columns, for each one and so this",
  "102:50": "is actually identical to torch dot where. Or isn't that tricky? And so the nice thing",
  "102:58": "is we can now use that for more than just two values. And so here's, here's the fully",
  "103:08": "worked out thing, so I've got my threes column, I've got my sevens column, here's that target,",
  "103:13": "here\u2019s the indexes from naught one, two, three, four five. And so here's 0, 0, .6;",
  "103:18": "1, 1, .49; 0, 2, .13, and so forth. So yeah this works just as well with more than two",
  "103:30": "columns. So we can add, you know, for doing a full MNIST, you know, so all the digits",
  "103:37": "from naught to nine. We could have ten columns and we would just be indexing into the ten.",
  "103:46": "So this thing we're doing where we're going minus our activations matrix, all of the numbers",
  "103:53": "from naught to N and then our targets, is exactly the same as something that already",
  "103:59": "exists in pytorch called F dot nll_loss as you can see. Exactly the same. And so again,",
  "104:06": "we're kind of seeing that these things inside pytorch and fastAI are just little shortcuts",
  "104:11": "for stuff we can write ourselves. Nll_loss stands for negative log likelihood, again,",
  "104:19": "it sounds complex, but actually it's just this indexing expression. Rather confusingly,",
  "104:28": "there's no log in it. We'll see why in a moment. So let's talk about logs. So this loss, this",
  "104:39": "loss function works quite well as we saw in the notebook 04. It's basically this, it is",
  "104:45": "exactly the same as we learned in notebook 04, just a different way of expressing it,",
  "104:50": "but we can actually make it better. Because remember the probabilities we're looking at",
  "104:56": "are between naught and one so they can't be smaller than zero. They can't be greater than",
  "105:00": "one, which means that if our model is trying to decide whether to predict .990 or .999",
  "105:08": ", it's going to think that those numbers are very very close together, but won't really",
  "105:12": "care. But actually if you think about the error, you know if there's like a hundred",
  "105:18": "things, a thousand things, then this would like be ten things are wrong and this would",
  "105:24": "be like one thing is wrong. But this is really like ten times better than this so really,",
  "105:32": "what we'd like to do is to transform the numbers between zero and one to instead between, be",
  "105:37": "between negative infinity and infinity. And there's a function that does exactly that",
  "105:42": "which is called logarithm. Okay, so, as the, so the numbers we could have can be between",
  "105:52": "zero and one and as we get closer and closer to zero it goes down to infinity and then",
  "106:03": "at one, it's going to be zero and we can't go above zero because our loss function we",
  "106:12": "want to be negative. So, this logarithm, in case you forgot, is, hopefully you vaguely",
  "106:21": "remember what logarithm is from high school, but that basically the definition is this:",
  "106:26": "if you have some number that is y that is b to the power of a Then logarithm is defined",
  "106:33": "such that a equals the logarithm of y comma b. In other words it tells you b to the power",
  "106:44": "of what equals y. Which is not that interesting of itself but one of the really interesting",
  "106:54": "things about logarithms is this very cool relationship, which is that log of a times",
  "107:00": "b equals log of a plus log of b. And we use that all the time in deep learning and machine",
  "107:08": "learning because this number here a times b can get very very big or very very small.",
  "107:16": "If you multiply things, a lot of small things together, you'll get a tiny number, if you",
  "107:21": "multiply a lot of big things together, you'll get a huge number. It can get so big or so",
  "107:24": "small that the kind of the precision in your computer's floating point gets really bad,",
  "107:32": "whereas this thing here adding is not going to get out of control. So we really love using",
  "107:39": "logarithms like particularly in a deep neural net where there's lots of layers, we're kind",
  "107:44": "of multiplying and adding many times, though, this kind of tends to come out quite nicely.",
  "107:52": "So when we take the probabilities that we saw before, the things that came out of this",
  "108:07": "function, and we take their logs and we take the mean, that is called negative log likelihood,",
  "108:20": "and so this ends up being kind of a really nicely behaved number because of this property",
  "108:25": "of the log that we described. So if you take the softmax and then take the log, then pass",
  "108:34": "that to an nll_loss because remember that we didn't actually take the log at all despite",
  "108:39": "the name, that gives you cross entropy loss. So that leaves an obvious question of why",
  "108:50": "doesn't nll_loss actually take the log and the reason for that is that it's more convenient",
  "108:57": "computationally to actually take the log back at the softmax step. So pytorch has a function",
  "109:04": "called log_softmax so since it's actually easier to do the log at the softmax stage,",
  "109:13": "it's just faster and more accurate. Pytorch assumes that you use soft log max and then",
  "109:19": "pass that to nll_loss. so nll_loss does not do the log. It assumes that you've done the",
  "109:25": "log beforehand. So log_softmax followed by nll_loss is the definition of cross-entropy",
  "109:32": "loss in pytorch. So that's our loss function and so you can pass that some activations",
  "109:38": "and some targets and get back a number and pretty much everything in pytorch every every",
  "109:45": "one of these kinds of functions, you can either use the NN version as a class like this and",
  "109:51": "then call that object as if it's a function, or you can just use F dot with the camelcase",
  "109:58": "name as a function directly and as you can see, they're exactly the same number. People",
  "110:05": "normally use the class version in the documentation in pytorch, you'll see it normally uses a",
  "110:12": "class version so we tend to use the class version as well. You'll see that it's returning",
  "110:18": "a single number and that's because it takes the mean because a loss needs to be as we've",
  "110:23": "discussed the mean but if you want to see the underlying numbers before taking the mean",
  "110:29": "you can just pass in reduction=none and that shows you the individual cross-entropy losses",
  "110:35": "before taking the mean. Okay, great, so this is a good place to stop with our discussion",
  "110:57": "of loss functions and such things. Rach, were there any questions about this? Why does the",
  "111:13": "loss function need to be negative? Well, okay, I mean I guess it doesn't but it's we want",
  "111:23": "something that the lower it is, the better, and we kind of need it to cut off somewhere.",
  "111:34": "I have to think about this more during the week because I'm, it's a bit tough, I\u2019m",
  "111:40": "a bit tired. Yeah, so let me let me refresh my memory when I'm awake Okay. Now next week",
  "111:53": "\u2026 well, nope not for the video. Next week actually happened last week so it's the thing",
  "112:01": "I'm about to say is actually your. So next week we're going to be talking about data",
  "112:09": "ethics, and I wanted to kind of segue into that by talking about how my week\u2019s gone,",
  "112:16": "because a week or two ago I did as part of a lesson I actually talked about the efficacy",
  "112:26": "of masks. I mean specifically wearing masks in public and I pointed out that the efficacy",
  "112:34": "of masks seemed like it could be really high and maybe everybody should be wearing them.",
  "112:42": "And somehow I found myself as the face of a global advocacy campaign. And so if you",
  "112:54": "go to masks4all.co, you\u2019ll find a website talking about masks. And I've been on, you",
  "113:11": "know, TV shows in South Africa and the US and England and Australia and on radio and",
  "113:19": "blah blah blah talking about masks. Why is this? Well, it's because as a data scientist,",
  "113:29": "you know, I noticed that the data around masks seemed to be getting misunderstood and it",
  "113:36": "seemed that that misunderstanding was costing possibly hundreds of thousands of lives. You",
  "113:43": "know, literally in the places that were using masks it seemed to be associated with orders",
  "113:49": "of magnitude fewer deaths and one of the things to talk about next week is like, you know,",
  "113:56": "what's your role as a data scientist. And, and you know, I strongly believe that it's",
  "114:02": "to understand the data and then do something about it. And so nobody was talking about",
  "114:07": "this So I ended up writing an article that appeared in The Washington Post that basically",
  "114:16": "called on people to really consider wearing masks (which is this article). And, you know",
  "114:29": "I was, I was lucky, I managed to kind of get a huge team of brilliant (not, not a huge,",
  "114:36": "but a pretty decent-sized team of brilliant) volunteers who helped, you know, kind of build",
  "114:41": "this website and kind of some PR folks and stuff like that. But what became clear was,",
  "114:50": "and I was talking to politicians, you know, senators, and staffers, and what was becoming",
  "114:56": "clear is that people weren't convinced by the science, which is fair enough because",
  "115:02": "it's, it's hard to. You know when the WHO and the CDC is saying you don't need to wear",
  "115:08": "a mask and some random data scientist is saying but doesn't seem to be what the data is showing.",
  "115:14": "You know, you've got half a brain you would pick the WHO and the CDC not the random data",
  "115:18": "scientist. So I really felt like I, if I was going to be an effective advocate, I needed",
  "115:24": "to sort the science out. And you, know credentialism is strong. And so it wouldn't be enough for",
  "115:31": "me to say it. I needed to find other people to say it. So I put together a team of 19",
  "115:39": "scientists, Including you know a professor of sociology, a professor of aerosol dynamics,",
  "115:49": "the founder of an African movement that's that kind of studied preventive methods for",
  "115:53": "methods for tuberculosis, a Stanford professor who studies mask disposal and cleaning methods,",
  "116:05": "a bunch of Chinese scientists who study epidemiology modeling A UCLA professor, who is one of the",
  "116:17": "top Infectious disease epidemiologist experts, and so forth. So like this kind of all-star",
  "116:24": "team of people from all around the world, and I had never met any of these people before",
  "116:30": "so (well, no not quite true, I knew Austin a little bit and I knew Zeynep a little bit,",
  "116:35": "and Lex a little bit). But on the whole you know (and well Reshama, we all know she's",
  "116:43": "awesome. So it's great to actually have a fast.ai community person there too. And, so,",
  "116:49": "but yeah, I kind of tried to pull together people from you know, as many geographies",
  "116:56": "as possible and as many areas of expertise as possible. And you know the kind of the",
  "117:02": "global community helped me find papers about, about everything. About, you know, how different",
  "117:11": "materials work, about how droplets form, about epidemiology, about case studies of people",
  "117:23": "infecting with and without masks, blah blah blah. And we ended up in the last week; basically",
  "117:28": "we wrote this paper. It contains 84 citations. And you know, we basically worked around the",
  "117:40": "clock on it as a team, and it's out. And it's been sent to a number of, some of the earlier",
  "117:49": "versions 3 or 4 days ago we sent to some governments. So one of the things is in this team. I try",
  "117:56": "to look for people who were working closely with government leaders, not just that they're",
  "118:01": "scientists. And so this, this went out to a number of government ministers. And in the",
  "118:07": "last few days, I've heard that it was a very significant part of decisions by governments",
  "118:16": "to change their, to change their guidelines around masks. And you know the fight\u2019s not",
  "118:25": "over by any means, and in particular the UK is a bit of a holdout. But I'm going to be",
  "118:31": "on ITV tomorrow and then BBC the next day. You know, it's it's kind of required stepping",
  "118:39": "out to be a lot more than just a data scientist. So I've had to pull together, you know politicians",
  "118:45": "and staffers. I've had to, you know, you know , hassle with the media to try and get you",
  "118:53": "know coverage. And you know today I'm now starting to do a lot of work with unions to",
  "118:58": "try to get unions to understand this You know, it's really a case of like saying - okay as",
  "119:02": "a data scientist, and in conjunction with real scientists, we've built this really strong",
  "119:10": "understanding that masks, you know this simple, but incredibly powerful tool. That doesn't",
  "119:18": "do anything unless I can effectively communicate this to decision-makers. So today I was you",
  "119:24": "know on the phone to, you know, one of the top union leaders in the country, explaining",
  "119:31": "what this means. Basically it turns out that in buses in America, the kind of the air conditioning",
  "119:37": "is set up so that it blows from the back to the front. And there's actually case studies",
  "119:42": "in the medical literature of how people that are seated downwind of an air conditioning",
  "119:48": "unit in a restaurant ended up all getting sick with Covid 19. And so we can see why",
  "119:55": "like bus drivers are dying. Because they're like, they're right in the wrong spot here",
  "120:02": "and their passengers aren't wearing masks. So I tried to unexplained this science to",
  "120:09": "union leaders so that they understand that to keep the workers safe it's not enough just",
  "120:15": "for the driver to wear a mask, but all the people on the bus needed to be wearing masks",
  "120:20": "as well. So, you know all this is basically to say ,,, you know as data scientists, I",
  "120:29": "think we have a responsibility to study the data and then do something about it. It's",
  "120:36": "not just a research exercise, it's not just a computation exercise, you know. What, what's",
  "120:43": "the point of doing things if it doesn't lead to anything? So, yeah, so, next week. We'll",
  "120:55": "be talking about this a lot more, but I think you know - this is a really to me kind of",
  "121:01": "interesting example of how digging into the data can lead to really amazing things happening.",
  "121:10": "And, and in this case, I strongly believe, and a lot of people are telling me they strongly",
  "121:15": "believe that this kind of advocacy work that's come out of this data analysis is, is already",
  "121:22": "saving lives. And so I hope this might help inspire you to, to take your data analysis",
  "121:28": "and to take it to places that it really makes a difference. So thank you very much, and",
  "121:33": "I'll see you next week."
}