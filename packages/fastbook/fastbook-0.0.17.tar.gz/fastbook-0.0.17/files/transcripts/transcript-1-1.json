{
  "00:01": "So hello everybody and welcome to Deep Learning for Coders, Lesson One.",
  "00:06": "This is the fourth year that we've done this, but it's a very different and very special",
  "00:16": "version for a number of reasons.",
  "00:18": "The first reason it's different is because we are bringing it to you live from day number",
  "00:23": "one of a complete shutdown.",
  "00:25": "Oh, not a complete shutdown, but nearly a complete shutdown of San Francisco.",
  "00:29": "We're going to be recording it over the next two months in the midst of this global pandemic.",
  "00:34": "So if things seem a little crazy sometimes in this course, I apologize.",
  "00:39": "So that's why this is happening.",
  "00:43": "The other reason it's special is because it's, we're trying to make this our definitive version,",
  "00:53": "right.",
  "00:54": "Since we've been doing this for a while now, we've finally got to the point where we almost",
  "00:59": "feel like we know what we're talking about.",
  "01:01": "To the point that Sylvain and I have actually written a book and we've actually written",
  "01:08": "a piece of software from scratch, called the fastai library version 2.",
  "01:12": "We've written a peer-reviewed paper about this library.",
  "01:18": "So this is kind of designed to be like the version of the course that is hopefully going",
  "01:25": "to last a while.",
  "01:28": "The syllabus is based very closely on this book, right.",
  "01:32": "So if you want to read along properly as you go, please buy it.",
  "01:39": "And I say \u201cplease buy it\u201d because actually the whole thing is also available for free",
  "01:44": "in the form of Jupyter notebooks.",
  "01:46": "And that is thanks to the huge generosity of O'Reilly Media, who have let us do that.",
  "01:54": "So you'll be able to see on the website for the course how to kind of access all this,",
  "02:04": "but here is the fastbook repo where you can read the whole damn thing.",
  "02:13": "At the moment as you see, it's a draft, but by the time you read this, it won't be.",
  "02:19": "So we have a big request here which is - the deal is this - you can read this thing for",
  "02:27": "free as Jupyter notebooks, but that is not as convenient as reading it on a Kindle or",
  "02:35": "in a paper book or whatever.",
  "02:36": "So, please don't turn this into a PDF, right.",
  "02:40": "Please don't turn it into a form designed more for reading, because kind of the whole",
  "02:46": "point is that you'll buy it.",
  "02:50": "Don't take advantage of O'Reilly's generosity by creating the thing that you know they're",
  "02:59": "not giving you for free.",
  "03:00": "And that's actually explicitly the license under which we're providing this as well.",
  "03:05": "So it's mainly a request, being a decent human being.",
  "03:11": "If you see somebody else not being a decent human being and stealing the book version",
  "03:15": "of the book, please tell them, \u201cPlease don't do that, it's not nice.\u201d",
  "03:19": "And don't be that person.",
  "03:21": "So either way, you can read along with the syllabus in the book.",
  "03:28": "There's a couple of different versions of these notebooks, right.",
  "03:35": "There is the, there's the full notebook that has the entire prose, pictures, everything.",
  "03:48": "Now we actually wrote a system to turn notebooks into a printed book and sometimes that looks",
  "03:56": "kind of weird.",
  "03:57": "For example, here's a weird looking table and if you look in the actual book, it actually",
  "04:06": "looks like a proper table, right.",
  "04:08": "So sometimes you'll see like little weird bits, okay, they are not mistakes they are",
  "04:13": "bits where we can add information to help our book turn into a proper nice book so just",
  "04:19": "just ignore them.",
  "04:20": "Now when I say we, who is we?",
  "04:23": "While I mentioned one important part of the we is Sylvain.",
  "04:30": "Sylvain is my co-author of the book and fastai version 2 library, so he is my partner in",
  "04:38": "crime here.",
  "04:39": "The other key \u201cwe\u201d here is Rachel Thomas and so maybe Rachel you can come and say hello.",
  "04:48": "She is the co-founder of fastai.",
  "04:50": "Hello yes I am the co-founder of fastai and I am also, lower sorry taller than Jeremy,",
  "04:59": "and I am the founding director of the Center for Applied Data Ethics at the University",
  "05:04": "of San Francisco.",
  "05:06": "Really excited to be a part of this course and I will be the voice you hear asking questions",
  "05:10": "from the forums.",
  "05:13": "Rachel and Sylvain are also the people in this group who actually understand math.",
  "05:19": "I am a mere philosophy graduate.",
  "05:21": "Rachel has a PhD.",
  "05:23": "Sylvain has written 10 books about math so if the math questions come along it's possible",
  "05:30": "I may pass them along.",
  "05:33": "But it is very nice to have an opportunity to work with people who understand this topic",
  "05:37": "so well.",
  "05:38": "Yes Yes Rachael, did you wanna sure oh thank you.",
  "05:46": "As Rachel mentioned the other area where she is you know real, has real world-class expertise",
  "05:53": "is data ethics, she is the founding director of the Centre for Applied Data Ethics, at",
  "06:01": "the University of San Francisco.",
  "06:02": "Thank you.",
  "06:03": "We are gonna be talking about data ethics throughout the course because well we happen",
  "06:08": "to think it's very important and so for those parts, although I'll generally be presenting",
  "06:13": "them they will be on the whole based on Rachel's Rachel's work because she actually knows what",
  "06:20": "she's talking about.",
  "06:22": "Although thanks to her I kind of know a bit about what I am talking about too.",
  "06:26": "Right, so that's that.",
  "06:31": "So should you be here, is there any point in you attempting to understand (I thought",
  "06:38": "I pressed the right button), understand deep learning.",
  "06:43": "Ok so what do you know should you should you be here.",
  "06:49": "Is there any point you attempting to learn deep learning or are you too stupid or you",
  "06:55": "don't have enough fast resources or whatever, because that's what a lot of people are telling",
  "07:01": "us.",
  "07:02": "They are saying you need teams of PhDs and massive data centers full of GPUs, otherwise",
  "07:07": "it's pointless.",
  "07:09": "Don't worry that is not at all true, couldn't be further from the truth.",
  "07:13": "In fact that the vast majority, sorry, a lot of world-class research and world-class industry",
  "07:22": "projects have come out of fastai alumni and fastai library-based projects and and elsewhere",
  "07:34": "which are created on a single GPU using a few dozen or a few hundred data points from",
  "07:43": "people that have no graduate level technical expertise, or in my case, I have no undergraduate",
  "07:51": "level technical expertise.",
  "07:53": "I'm just a philosophy major.",
  "07:55": "So there is - and we'll see it throughout the course -- but there is lots and lots and",
  "08:00": "lots of clear empirical evidence that you don't need lots of math, you don't need lots",
  "08:04": "of data, you don't need lots of expensive computers to do great stuff, with deep learning.",
  "08:08": "So just bear with us.",
  "08:10": "You'll be fine.",
  "08:12": "To do this course, you do need to code.",
  "08:15": "Preferably, you know how to code in Python.",
  "08:19": "But if you've done other languages, you can learn Python.",
  "08:22": "If the only languages you've done is something like Matlab, where you've used it more like",
  "08:27": "a scripty kind of thing, you might find it a bit - You will find it a bit heavier going.",
  "08:33": "But that's okay, stick with it.",
  "08:36": "You can learn Python as you go.",
  "08:41": "Is there any point learning deep learning?",
  "08:43": "Is it any good at stuff?",
  "08:46": "If you are hoping to build a brain, that is an AGI, I cannot promise we're gonna help",
  "08:54": "you with that.",
  "08:55": "And AGI stands for: Artificial General Intelligence.",
  "08:59": "Thank you.",
  "09:00": "What I can tell you though, is that in all of these areas, deep learning is the best-known",
  "09:07": "approach, to at least many versions of all of these things.",
  "09:13": "So it is not speculative at this point whether this is a useful tool.",
  "09:18": "It's a useful tool in lots and lots and lots of places.",
  "09:22": "Extremely useful tool.",
  "09:23": "And in many of these cases, it is equivalent to or better than human performance.",
  "09:29": "At least according to some particular narrow definition of things that humans do in these",
  "09:34": "kinds of areas.",
  "09:36": "So deep learning is pretty amazing.",
  "09:39": "And if you want to pause the video here and have a look through and try and pick some",
  "09:44": "things out that you think might look interesting and type that keyword and \u201cdeep learning\u201d",
  "09:49": "into Google.",
  "09:50": "And you'll find lots of papers and examples and stuff like that.",
  "09:55": "Deep learning comes from a background of neural networks.",
  "10:00": "As you'll see deep learning is just a type of neural network learning.",
  "10:05": "A deep one.",
  "10:07": "We'll describe exactly what that means later.",
  "10:09": "And neural networks are certainly not a new thing.",
  "10:12": "They go back at least to 1943, when McCulloch and Pitts created a mathematical model of",
  "10:17": "an artificial neuron.",
  "10:19": "And got very excited about where that could get to.",
  "10:24": "And then in the 50's, Frank Rosenblatt then built on top of that - he basically created",
  "10:32": "some subtle changes to that mathematical model.",
  "10:36": "And he thought that with these subtle changes \u201cwe could witness the birth of the machine",
  "10:40": "that is capable of perceiving, recognizing and identifying its surroundings without any",
  "10:44": "human training or control\u201d.",
  "10:47": "And he oversaw the building of this - extraordinary thing.",
  "10:51": "The Mark 1 Perceptron at Cornell.",
  "10:55": "So that was I think, this picture was 1961.",
  "11:00": "Thankfully nowadays, we don't have to build neural networks by running the damn wires",
  "11:05": "from neuron to neuron (artificial neuron to artificial neuron).",
  "11:07": "But you can kind of see the idea; lot of connections going on.",
  "11:11": "And you'll hear the word connection a lot, in this course, because that's what it's all",
  "11:14": "about.",
  "11:17": "Then we had the first AI winter, as it was known.",
  "11:20": "Which really, to a strong degree happened because an MIT professor named Marvin Minsky",
  "11:27": "and Papert wrote a book called perceptrons about Rosenblatt's invention in which they",
  "11:32": "pointed out that a single layer of these artificial neuron devices, actually couldn't learn some",
  "11:40": "critical things.",
  "11:41": "It was like impossible for them to learn something as simple as the Boolean XOR operator.",
  "11:46": "In the same book, they showed that using multiple layers of the devices actually would fix the",
  "11:52": "problem.",
  "11:53": "People ignore - didn't notice that part of the book.",
  "11:55": "And only noticed the limitation and people basically decided that neural networks are",
  "12:01": "gonna go nowhere.",
  "12:02": "And they kind of largely disappeared, for decades.",
  "12:07": "Until, in some ways, 1986.",
  "12:10": "A lot happened in the meantime but there was a big thing in 1986, which is: MIT released",
  "12:16": "a thing called a book, a series of two volumes of book called, \u201cParallel Distributed Processing\u201d.",
  "12:23": "In which they described this thing they call parallel distributed processing, where you",
  "12:28": "have a bunch of processing units, that have some state of activation and some output function",
  "12:36": "and some pattern of connectivity and some propagation rule and some activation rule",
  "12:42": "and some learning rule, operating in an environment.",
  "12:45": "And then they described how things that met these requirements, could in theory, do all",
  "12:51": "kinds of amazing work.",
  "12:52": "And this was the result of many, many researchers working together.",
  "12:56": "There was a whole group involved in this project, which resulted in this very, very important",
  "13:02": "book.",
  "13:03": "And so, the interesting thing to me, is that if you - as you go through this course come",
  "13:09": "back and have a look at this picture and you'll see we are doing exactly these things.",
  "13:15": "Everything we're learning about really is how do you do each of these eight things?",
  "13:21": "That is interesting that they include the environment because that's something which",
  "13:25": "very often, data scientists ignore.",
  "13:28": "Which is - you build a model, you've trained it, it's learned something.",
  "13:32": "What's the context it works in?",
  "13:33": "And we're talking about that, quite a bit over the next couple of lessons as well.",
  "13:40": "So in the 80's, during and after this was released people started building in this second",
  "13:48": "layer of neurons, avoiding Minsky's problem.",
  "13:52": "And in fact, it was shown, that it was mathematically provable, that by adding that one extra layer",
  "14:00": "of neurons, it was enough to allow any mathematical model to be approximated to any level of accuracy,",
  "14:08": "with these neural networks.",
  "14:10": "And so that was like the exact opposite of the Minsky thing.",
  "14:14": "That was like: \u201cHey there's nothing we can't do.",
  "14:17": "Provably there's nothing we can't do.\u201d",
  "14:20": "And so that was kind of when I started getting involved in neural networks.",
  "14:24": "So I was - a little bit later.",
  "14:25": "I guess I was getting involved in the early 90s.",
  "14:29": "And they were very widely used in industry.",
  "14:31": "I was using them for very boring things like targeted marketing for retail banks.",
  "14:35": "They tended to be big companies with lots of money that were using them.",
  "14:40": "And it certainly though was true that often the networks were too big or slow to be useful.",
  "14:46": "They were certainly useful for some things, but they - you know they never felt to me",
  "14:52": "like they were living up to the promise for some reason.",
  "14:56": "Now what I didn't know, and nobody I personally met knew was that actually there were researchers",
  "15:02": "that had shown 30 years ago that to get practical good performance, you need more layers of",
  "15:09": "neurons.",
  "15:10": "Even though mathematically, theoretically, you can get as accurate as you want with just",
  "15:14": "one extra layer.",
  "15:16": "To do it with good performance, you need more layers.",
  "15:21": "So when you add more layers to a neural network you get deep learning.",
  "15:26": "So deep doesn't mean anything like mystical.",
  "15:29": "It just means more layers.",
  "15:32": "More layers than just adding the one extra one.",
  "15:36": "So thanks to that, neural nets are now living up to their potential.",
  "15:39": "As we saw in that like what's deep learning good at thing.",
  "15:43": "So we could now say that Rosenblatt was right.",
  "15:46": "We have a machine that's capable of perceiving, recognising and identifying its surroundings",
  "15:53": "without any human training or control.",
  "15:55": "That is - That's definitely true.",
  "15:56": "I don't think there's anything controversial about that statement based on the current",
  "16:00": "technology.",
  "16:01": "So we're gonna be learning how to do that.",
  "16:05": "We're gonna be learning how to do that in exactly the opposite way, of probably all",
  "16:09": "of the other math and technical education you've had.",
  "16:14": "We are not gonna start with a two-hour lesson about the sigmoid function or a study of linear",
  "16:25": "algebra or a refresher course on calculus.",
  "16:30": "And the reason for that, is that people who study how to teach and learn have found that",
  "16:38": "is not the right way to do it for most people.",
  "16:42": "For most people - So we work a lot based on the work of Professor David Perkins from Harvard",
  "16:50": "and others who work at similar things, who talk about this idea of playing the whole",
  "16:57": "game.",
  "16:58": "And so playing the whole game is like it's based on the sports analogy if you're gonna",
  "17:01": "teach somebody baseball.",
  "17:04": "You don't take them out into a classroom and start teaching them about the physics of a",
  "17:11": "parabola, and how to stitch a ball, and a three-part history of 100 years of baseball",
  "17:20": "politics, and then 10 years later, you let them watch a game.",
  "17:24": "And then 20 years later, you let them play a game.",
  "17:27": "Which is kind of like how math education is being done, right?",
  "17:32": "Instead with baseball, step one is to say, hey, let's go and watch some baseball.",
  "17:37": "What do you think?",
  "17:38": "That was fun, right?",
  "17:39": "See that guy there...he took a run there\u2026before the other guy throws a ball over there...hey,",
  "17:42": "you want to try having a hit?",
  "17:44": "Okay, so you're going to hit the ball, and then I have to try to catch it, then he has",
  "17:47": "to run,run,run over there...and so from step one, you are playing the whole game.",
  "17:53": "And just to add to that, when people start, they often may not have a full team, or be",
  "17:59": "playing the full nine innings, but they still have a sense of what the game is, a kind of",
  "18:04": "a big picture idea.",
  "18:05": "So, there is lots and lots of reasons that this helps most human beings, (though) not",
  "18:13": "everybody, right?",
  "18:14": "There's a small percentage of people who like to build things up from the foundations and",
  "18:19": "the principles, and not surprisingly, they are massively overrepresented in a university",
  "18:24": "setting, because the people who get to be academics are the people who thrive with,",
  "18:29": "(according) to me, the upside down way of how things are taught.",
  "18:33": "But outside of universities most people learn best in this top-down way, where you start",
  "18:40": "with the full context.",
  "18:42": "So step number two in the seven principles, and I'm only going to mention the first three,",
  "18:47": "is to make the game worth playing.",
  "18:50": "Which is like, if you're playing baseball, you have a competition.",
  "18:54": "You know, you score, you try and win, you bring together teams from around the community",
  "19:00": "and you have people try to beat each other.",
  "19:02": "And you have leaderboards, like who's got the highest number of runs or whatever.",
  "19:09": "So this is all about making sure that the thing you're doing, you're doing it properly.",
  "19:15": "You're making it the whole thing, you're providing the context and the interest.",
  "19:23": "So, for the fastai approach to learning deep learning, what this means is that today we're",
  "19:30": "going to train models end to end.",
  "19:33": "We're going to actually train models, and they won't just be crappy models.",
  "19:38": "They will be state-of-the-art world-class models from today, and we can try to have",
  "19:45": "you build your own state-of-the-art world-class models from either today or next lesson, depending",
  "19:51": "on how things go.",
  "19:53": "Then, number three in the seven principles from Harvard is, work on the hard parts.",
  "19:59": "Which is kind of like this idea of practice, deliberate practice.",
  "20:10": "Work on the hard parts means that you don't just swing a bat at a ball every time, you",
  "20:19": "know, you go out and just muck around.",
  "20:22": "You train properly, you find the bit that you are the least good at, you figure out",
  "20:27": "where the problems are, you work damn hard at it.",
  "20:31": "So, in the deep learning context, that means that we do not dumb things down.",
  "20:40": "Right?",
  "20:41": "By the end of the course, you will have done the calculus.",
  "20:45": "You will have done the linear algebra.",
  "20:47": "You will have done the software engineering of the code, right?",
  "20:54": "You will be practicing these things which are hard, so it requires tenacity and commitment.",
  "21:04": "But hopefully, you'll understand why it matters because before you start practicing something",
  "21:11": "you'll know why you need that thing because you'll be using it.",
  "21:14": "Like to make your model better, you'll have to understand that concept first.",
  "21:19": "So for those of you used to a traditional university environment, this is gonna feel",
  "21:24": "pretty weird and a lot of people say: \u201cthat they regret (you know after a year of studying",
  "21:31": "fastai) that they spent too much time studying theory, and not enough time training models",
  "21:38": "and writing code.",
  "21:40": "That's the kind of like, the number one piece of feedback we get from people who say, \u201cI",
  "21:43": "wish I've done things differently.\u201d",
  "21:45": "It's that.",
  "21:46": "So please try to, as best as you can, since you're here, follow along with this approach.",
  "21:53": "We are gonna be using a software stack - Sorry Rachel.",
  "21:59": "Yes?",
  "22:00": "I just need to say one more thing about the approach.",
  "22:03": "I think since, so many of us spent so many years with the traditional educational approach",
  "22:07": "of bottom-up, that this can feel very uncomfortable at first.",
  "22:12": "I still feel uncomfortable with it sometimes, even though I'm committed to the idea.",
  "22:17": "And that, some of it is also having to catch yourself and being okay with not knowing the",
  "22:22": "details.",
  "22:23": "Which I think can feel very unfamiliar, or even wrong when you're kind of new to that.",
  "22:28": "Of like: \u201cOh wait, I'm using something and I don't understand every underlying detail.\u201d",
  "22:32": "But you kind of have to trust that we're gonna get to those details later.",
  "22:36": "So I can't empathise because I did not spend lots of time doing that.",
  "22:40": "But I will tell you this - teaching this way is very, very, very hard.",
  "22:44": "And I very often find myself jumping back into a foundations first approach.",
  "22:49": "Because it's just so easy to be like: \u201cOh you need to know this.",
  "22:52": "You need to know this.",
  "22:53": "You need to do this.",
  "22:54": "And then you can know this.\u201d",
  "22:55": "That's so much easier to teach.",
  "22:57": "So I do find this much much more challenging to teach, but hopefully it's worth it.",
  "23:02": "We spent a long long time figuring out how to get deep learning into this format.",
  "23:07": "But one of the things that helps us here, is the software we have available.",
  "23:12": "If you haven't used Python before - it's ridiculously flexible and expressive and easy-to-use language.",
  "23:23": "We have plenty of bits about it we don't love but on the whole we love the overall thing.",
  "23:28": "And we think it's - Most importantly, the vast, vast, vast majority of deep learning",
  "23:34": "practitioners and researchers are using Python.",
  "23:38": "On top of Python, there are two libraries that most folks are using today; PyTorch and",
  "23:43": "TensorFlow.",
  "23:44": "There's been a very rapid change here.",
  "23:48": "TensorFlow was what we were teaching until a couple of years ago.",
  "23:51": "It's what everyone was using until a couple of years ago.",
  "23:55": "It got super bogged down, basically TensorFlow got super bogged down.",
  "24:00": "This other software called PyTorch came along that was much easier to use and much more",
  "24:06": "useful for researchers and within the last 12 months, the percentage of papers at major",
  "24:15": "conferences that uses PyTorch has gone from 20% to 80% and vice versa, those that use",
  "24:21": "TensorFlow have gone on from 80% to 20%.",
  "24:25": "So basically all the folks that are actually building the technology were all using, are",
  "24:29": "now using, PyTorch and you know industry moves a bit more slowly but in the next year or",
  "24:35": "two you will probably see a similar thing in industry.",
  "24:39": "Now, the thing about PyTorch is it's super super flexible and really is designed for",
  "24:45": "flexibility and developer friendliness, certainly not designed for beginner friendliness and",
  "24:53": "it's not designed for what we say, it doesn't have higher level API's, by which I mean there",
  "24:58": "isn't really things to make it easy to build stuff quickly using PyTorch.",
  "25:06": "So to deal with that issue, we have a library called fastai that sits on top of PyTorch.",
  "25:14": "Fastai is the most popular higher level API for PyTorch.",
  "25:20": "It is, because our courses are so popular, some people are under the mistaken impression",
  "25:28": "that fastai is designed for beginners or for teaching.",
  "25:34": "It is designed for beginners and teaching, as well as practitioners in industry and researchers.",
  "25:44": "The way we do this makes sure that it's, that it's the best API for all of those people",
  "25:50": "as we use something called a layered API and so there's a peer-reviewed paper that Sylvain",
  "25:59": "and I wrote that described how we did that and for those of you that are software engineers,",
  "26:05": "it will not be at all unusual or surprising.",
  "26:08": "It's just totally standard software engineering practices, but they are practices that were",
  "26:13": "not followed in any deep learning library we had seen.",
  "26:17": "Just basically lots of re-factoring and decoupling and so by using that approach, it's allowed",
  "26:24": "us to build something which you can do super low-level research, you can do state-of-the-art",
  "26:32": "production models and you can do kind of super easy, beginner, but beginner world-class models.",
  "26:44": "So that's the basic software stack, there's other pieces of software we will be learning",
  "26:47": "about along the way.",
  "26:50": "But the main thing I think to mention here is it actually doesn't matter.",
  "26:55": "If you learn this software stack and then at work you need to use TensorFlow and Keras,",
  "27:01": "you will be able to switch in less than a week.",
  "27:06": "Lots and lots of students have done that, it's never been a problem.",
  "27:13": "The important thing is to learn the concepts and so we're going to focus on those concepts",
  "27:21": "and by using an API which minimizes the amount of boilerplate you have to use, it means you",
  "27:28": "can focus on the bits that are important.",
  "27:30": "The actual lines of code will correspond much more to the actual concepts you are implementing.",
  "27:38": "You are going to need a GPU machine.",
  "27:41": "A GPU is a Graphics Processing Unit, and specifically, you need an Nvidia GPU.",
  "27:49": "Other brands of GPU just aren't well supported by any Deep Learning libraries.",
  "27:56": "Please don't buy one.",
  "27:57": "If you already have one you probably shouldn't use it.",
  "28:00": "Instead you should use one of the platforms that we have already got set up for you.",
  "28:05": "It's just a huge distraction to be spending your time doing, like, system administration",
  "28:10": "on a GPU machine and installing drivers and blah blah blah.",
  "28:16": "And run it on Linux.",
  "28:18": "Please.",
  "28:19": "That's what everybody's doing, not just us, everybody's running it on Linux.",
  "28:22": "Make life easy for yourself.",
  "28:23": "It's hard enough to learn Deep Learning without having to do it in a way that you are learning,",
  "28:28": "you know, all kinds of arcane hardware support issues.",
  "28:32": "There's a lot of free options available and so, please, please use them.",
  "28:43": "If you're using an option that is not free don't forget to shut down your instance.",
  "28:48": "So what's gonna be happening is you gonna be spinning up a server that lives somewhere",
  "28:52": "else in the world, and you're gonna be connecting to it from your computer and training and",
  "28:57": "running and building models.",
  "29:02": "Just because you close your browser window doesn't mean your server stops running on",
  "29:06": "the whole.",
  "29:07": "Right?",
  "29:08": "So don't forget to shut it down because otherwise you're paying for it.",
  "29:11": "Colab, is a great system which is free.",
  "29:16": "There's also a paid subscription version of it.",
  "29:19": "Be careful with Colab.",
  "29:21": "Most of the other systems we recommend save your work for you automatically and you can",
  "29:27": "come back to it at any time.",
  "29:28": "Colab doesn't.",
  "29:29": "So be sure to check out the Colab platform thread on the forums, to learn about that.",
  "29:37": "So, I mention the forums...",
  "29:44": "The forums are really, really important because that is where all of the discussion and set",
  "29:52": "up and everything happens.",
  "29:54": "So for example if you want help with setup here.",
  "29:56": "You know there is a setup help thread and you can find out, you know, how to best set",
  "30:04": "up Colab, and you can see discussions about it and you can ask questions, and please remember",
  "30:09": "to search before you ask your question, right?",
  "30:12": "Because it's probably been asked before, unless you're one of the very, very earliest people",
  "30:19": "who are doing the course.",
  "30:23": "So, once you\u2026",
  "30:25": "So, step one is to get your server set up by just following the instructions from the",
  "30:32": "forums or from the course website.",
  "30:34": "And the course website will have lots of step-by-step instructions for each platform.",
  "30:40": "They will vary in price, they will vary in speed, they will vary in availability, and",
  "30:45": "so forth.",
  "30:46": "Once you are finished following those instructions",
  "30:48": "The last step of those instructions will end up showing you something like this: a course",
  "30:58": "v4 folder, so a version four of our course.",
  "31:01": "By the time you see this video, this is likely to have more stuff in it, but it will have",
  "31:05": "an NB's standing for notebooks folder.",
  "31:08": "So you can click on that, and that will show you all of the notebooks for the course.",
  "31:15": "What I want you to do is scroll bottom and find the one called app Jupyter.",
  "31:21": "Click on that, and this is where you can start learning about Jupyter notebook.",
  "31:28": "What's Jupyter Notebook?",
  "31:30": "Jupyter Notebook is something where you can start typing things, and press Shift-Enter,",
  "31:40": "and it will give you an answer.",
  "31:41": "And so the thing you're typing is python code, and the thing that comes out is a result of",
  "31:47": "that code.",
  "31:49": "And so you can put in anything in python.",
  "31:52": "X equals three times four.",
  "31:57": "X plus one, and as you can see, it displays a result anytime there's a result to display.",
  "32:05": "So for those of you that have done a bit of coding before, you will recognise this as",
  "32:10": "a REPL.",
  "32:11": "R-E-P-L, read, evaluate, print, loop.",
  "32:16": "Most languages have some kind of REPL.",
  "32:19": "The Jupyter notebook REPL is particularly interesting, because it has things like headings,",
  "32:29": "graphical outputs, interactive multimedia.",
  "32:35": "It's a really astonishing piece of software.",
  "32:38": "It's won some really big awards.",
  "32:40": "I would have thought the most widely used REPL, outside of shells like bash.",
  "32:48": "It's a very powerful system.",
  "32:50": "We love it.",
  "32:51": "We've written our whole book in it, we've written the entire FASTAI library with it,",
  "32:56": "we do all our teaching with it.",
  "32:59": "It's extremely unfamiliar to people who have done most of their work in IDE.",
  "33:07": "You should expect it to feel as awkward as perhaps the first time you moved from a GUI",
  "33:11": "to a command line.",
  "33:13": "It's different.",
  "33:14": "So if you're not familiar with REPL-based systems, it's gonna feel super weird.",
  "33:22": "But stick with it, because it really is great.",
  "33:25": "The kind of model going on here is that, this webpage I'm looking at, is letting me type",
  "33:32": "in things for a server to do, and show me the results of computations a server is doing.",
  "33:37": "So the server is off somewhere else.",
  "33:40": "It's not running on my computer right?",
  "33:43": "The only thing running on the computer is this webpage.",
  "33:46": "But as I do things, so for example if I say X equals X times three.",
  "33:53": "This is updating the servers state.",
  "33:56": "There's this state.",
  "33:57": "It's like what's currently the value of X and so I can find out, now X is something",
  "34:03": "different.",
  "34:04": "So you can see, when I did this line here, it didn't change the earlier X plus one, right?",
  "34:10": "So that means that when you look at a Jupyter notebook, it's not showing you the current",
  "34:15": "state of your server.",
  "34:17": "It's just showing you what that state was, at the time that you printed that thing out.",
  "34:22": "It's just like if you use a shell like bash.",
  "34:25": "And you type \u201cls\u201d.",
  "34:27": "And then you delete a file.",
  "34:29": "That earlier \u201cls\u201d you printed doesn't go back and change.",
  "34:32": "That's kind of how REPLs generally work.",
  "34:36": "Including this one.",
  "34:39": "Jupyter notebook has two modes.",
  "34:43": "One is edit mode, which is when I click on a cell and I get a flashing cursor and I can",
  "34:49": "move left and right and type.",
  "34:52": "Right?",
  "34:53": "There's not very many keyboard shortcuts to this mode.",
  "34:56": "One useful one is \u201ccontrol\u201d or \u201ccommand\u201d + \u201c/\u201d. Which will comment and uncomment.",
  "35:02": "The main one to know is \u201cshift\u201d + \u201center\u201d to actually run the cell.",
  "35:08": "At that point there is no flashing cursor anymore.",
  "35:10": "And that means that I'm now in command mode.",
  "35:12": "Not edit mode.",
  "35:14": "So as I go up and down, I'm selecting different cells.",
  "35:17": "So in command mode as we move around, we're now selecting cells.",
  "35:23": "And there are now lots of keyboard shortcuts you can use.",
  "35:27": "So if you hit \u201cH\u201d you can get a list of them.",
  "35:31": "That, for example - And you'll see that they're not on the whole, like \u201ccontrol\u201d or \u201ccommand\u201d",
  "35:36": "with something they're just the letter on its own.",
  "35:38": "So if you use like Vim, you'll be more familiar with this idea.",
  "35:42": "So for example if I hit \u201cC\u201d to copy and \u201cV\u201d to paste.",
  "35:46": "Then it copies the cell.",
  "35:47": "Or \u201cX\u201d to cut it.",
  "35:51": "\u201cA\u201d to add a new cell above.",
  "35:55": "And then I can press the various number keys, to create a heading.",
  "35:59": "So number two will create a heading level II.",
  "36:02": "And as you can see, I can actually type formatted text not just code.",
  "36:08": "The formatted text I type is in Markdown.",
  "36:16": "Like so.",
  "36:23": "My numbered one work.",
  "36:24": "There you go.",
  "36:27": "So that's in Markdown.",
  "36:29": "If you haven't used Markdown before, it's a super super useful way to write formatted",
  "36:35": "text.",
  "36:36": "That is used very, very, very widely.",
  "36:38": "So learn it because it's super handy.",
  "36:42": "And you need it for Jupiter.",
  "36:46": "So when you look at our book notebooks.",
  "36:52": "For example, you can see an example of all the kinds of formatting and code and stuff",
  "36:58": "here.",
  "36:59": "So you should go ahead and go through the \u201capp_jupyter\u201d.",
  "37:05": "And you can see here how you can create plots for example.",
  "37:09": "And create lists of things.",
  "37:11": "And import libraries.",
  "37:13": "And display pictures and so forth.",
  "37:18": "If you wanna create a new notebook, you can just go \u201cNew\u201d \u201cPython 3\u201d and that",
  "37:26": "creates a new notebook.",
  "37:30": "Which by default is just called \u201cUntitled\u201d so you can then rename it to give it whatever",
  "37:35": "name you like.",
  "37:38": "And so then you'll now see that, in the list here, \u201cnewname\u201d.",
  "37:45": "The other thing to know about Jupiter is that it's a nice easy way to jump into a terminal",
  "37:50": "if you know how to use a terminal.",
  "37:51": "You certainly don't have to for this course at least for the first bit.",
  "37:55": "If I go to a new terminal, You can see here I have a terminal.",
  "38:08": "One thing to note is for the notebooks are attached to a Github repository.",
  "38:19": "If you haven't used Github before that's fine.",
  "38:22": "but basically they're attached to a server where from time to time we will update the",
  "38:27": "notebooks on it.",
  "38:29": "And we will see you'll see on the course website, in the forum we tell you how to make sure",
  "38:33": "you have the most recent versions.",
  "38:36": "When you grab our most recent version you don't want to conflict with or overwrite your",
  "38:41": "changes.",
  "38:42": "So as you start experimenting it's not a bad idea to like select a notebook and click duplicate",
  "38:50": "and then start doing your work in the copy.",
  "38:52": "And that way when you get an update of our latest course materials, it's not gonna interfere",
  "38:59": "with the experiments you've been running.",
  "39:06": "So there are two important repositories to know about.",
  "39:09": "One is the fast book repository which we saw earlier, which is kind of the full book with",
  "39:20": "all the outputs and pros and everything.",
  "39:26": "And then the other one is the course V4 repository.",
  "39:30": "And here is the exact same notebook from the course V4 repository.",
  "39:35": "And for this one we remove all of the pros and all of the pictures and all of the outputs",
  "39:42": "and just leave behind the headings and the code.",
  "39:46": "In this case you can see some outputs because I just ran that code, most of it.",
  "39:51": "There won't be any.",
  "39:53": "No, No, I guess we have left outputs.",
  "39:56": "I'm not sure to keep that or not.",
  "39:58": "So you may or may not see the outputs.",
  "40:02": "So the idea with this is.",
  "40:04": "This is properly the version that you want to be experimenting with.",
  "40:10": "Because it kind of forces you to think about like what's going on as you do each step,",
  "40:15": "rather than just reading it and running it without thinking.",
  "40:19": "We kind of want you to do it in a small bare environment in which you thinking about like",
  "40:24": "what did the book say why was this happening and if you forget anything then you kind of",
  "40:29": "go back to the book.",
  "40:31": "The other thing to mention is both the course V4 version and the fast book version at the",
  "40:37": "end have a questionnaire.",
  "40:42": "And a quite a few folks have told us you know that in amongst the reviewers and stuff that",
  "40:47": "they actually read the questionnaire first.",
  "40:50": "We spent many, many weeks writing the questionnaires, Sylvain and I.",
  "40:58": "And the reason for that is because we try to think about like what do we want you to",
  "41:04": "take away from each notebook.",
  "41:08": "So you kind of read the questionnaire first.",
  "41:10": "You can find out what are the things we think are important.",
  "41:12": "What are the things you should know before you move on.",
  "41:15": "So rather than having like a summary section at the end saying at the end of this you should",
  "41:19": "know, blah blah blah, we instead have a questionnaire to do the same thing, so please make sure",
  "41:25": "you do the questionnaire before you move onto the next chapter.",
  "41:28": "You don't have to get everything right, and most of the time answering the questions is",
  "41:32": "as simple as going back to that part of the notebook and reading the prose, but if you've",
  "41:38": "missed something, like do go back and read it because these are the things we are assuming",
  "41:43": "you know.",
  "41:45": "So if you don't know these things before you move on, it could get frustrating.",
  "41:50": "Having said that, if you get stuck after trying a couple of times, do move onto the next chapter,",
  "41:57": "do two or three more chapters and then come back.",
  "42:01": "maybe by the time you've done a couple more chapters, you know, you will get some more",
  "42:04": "perspective.",
  "42:05": "We try to re-explain things multiple times in different ways, so it's okay if you tried",
  "42:13": "and you get stuck, then you can try moving on.",
  "42:17": "Alright, so, let's try running the first part of the notebook.",
  "42:26": "So here we are in 01 intro, so this is chapter 1 and here is our first cell.",
  "42:37": "So I click on the cell and by default, actually, there will be a header in the toolbar as you",
  "42:45": "can see.",
  "42:46": "You can turn them on or off.",
  "42:47": "I always leave them off myself and so to run this cell, you can either click on the play,",
  "42:54": "the run button or as I mentioned, you can hit shift enter.",
  "42:57": "So for this one this i'll just click and as you can see this star appears, so this says",
  "43:03": "I'm running and now you can see this progress bar popping up and that is going to take a",
  "43:08": "few seconds and so as it runs to print out some results.",
  "43:16": "Don't expect to get exactly the same results as us, there is some randomness involved in",
  "43:21": "training a model, and that's okay.",
  "43:24": "Don't expect to get exactly the same time as us.",
  "43:27": "If this first cell takes more than five minutes unless you have a really old GPU that is probably",
  "43:33": "a bad sign.",
  "43:34": "You might want to hop on the forums and figure out what's going wrong or maybe it only has",
  "43:39": "windows which really doesn't work very well for this moment.",
  "43:43": "Don't worry that we don't know what all the code does yet.",
  "43:45": "We are just making sure that we can train a model . So here we are, it's finished running",
  "43:52": "and so as you can see, it's printed out some information and in this case it's showing",
  "43:58": "me that there is an error rate of 0.005 at doing something.",
  "44:05": "What is the something it's doing?",
  "44:07": "Well, what it's doing here is it's actually grabbing a dataset, we call the pets dataset,",
  "44:14": "which is a dataset of pictures of cats and dogs.",
  "44:19": "And it's trying to figure out; which ones are cats and which ones are dogs.",
  "44:26": "And as you can see, after about well less than a minute, it's able to do that with a",
  "44:33": "0.5% error rate.",
  "44:35": "So it can do it pretty much perfectly.",
  "44:37": "So we've trained our first model.",
  "44:40": "We have no idea how.",
  "44:41": "We don't know what we were doing.",
  "44:42": "But we have indeed trained our model.",
  "44:44": "So that's a good start.",
  "44:47": "And as you can see, we can train models pretty quickly on a single computer.",
  "44:51": "Which you know - Many of which you can get for free.",
  "44:55": "One more thing to mention is, if you have a Mac - doesn't matter whether you have Windows",
  "45:01": "or Mac or Linux in terms of what's running in the browser.",
  "45:05": "But if you have a Mac, please don't try to use that GPU.",
  "45:11": "Mac's actually - Apple doesn't even support Nvidia GPUs anymore.",
  "45:16": "So that's really not gonna be a great option.",
  "45:19": "So stick with Linux.",
  "45:21": "It will make life much easier for you.",
  "45:25": "Right, actually the first thing we should do is actually try it out.",
  "45:30": "So if - I claim we've trained a model that can pick cats from dogs.",
  "45:35": "Let's make sure we can.",
  "45:38": "So let's - Check out this cell.",
  "45:42": "This is interesting.",
  "45:43": "Right?",
  "45:44": "We've created a widgets dot file upload object and displayed it.",
  "45:48": "And this is actually showing us a clickable button.",
  "45:51": "So as I mentioned this is an unusual REPL.",
  "45:53": "We can even create GUIs, in this REPL.",
  "45:55": "So if I click on this file upload.",
  "45:58": "And I can pick \u201ccat\u201d.",
  "46:00": "There we go.",
  "46:04": "And I can now turn that uploaded data into an image.",
  "46:11": "There's a cat.",
  "46:14": "And now I can do predict, and it's a cat.",
  "46:23": "With a 99.96% probability.",
  "46:26": "So we can see we have just uploaded an image that we've picked out.",
  "46:30": "So you should try this.",
  "46:31": "Right?",
  "46:32": "Grab a picture of a cat.",
  "46:33": "Find one from the Internet or go and take a picture of one yourself.",
  "46:36": "And make sure that you get a picture of a cat.",
  "46:39": "This is something which can recognise photos of cats, not line drawings of cats.",
  "46:44": "And so as we'll see, in this course.",
  "46:47": "These kinds of models can only learn from the kinds of information you give it.",
  "46:52": "And so far we've only given it, as you'll discover, photos of cats.",
  "46:57": "Not anime cats, not drawn cats, not abstract representations of cats but just photos.",
  "47:07": "So we're now gonna look at; what's actually happened here?",
  "47:11": "And you'll see at the moment, I am not getting some great information here.",
  "47:16": "If you see this, in your notebooks, you'll have to go: file, trust notebook.",
  "47:26": "And that just tells Jupiter that it's allowed to run the code necessary to display things,",
  "47:31": "to make sure there isn't any security problems.",
  "47:34": "And so you'll now see the outputs.",
  "47:36": "Sometimes you'll actually see some weird code like this.",
  "47:40": "This is code that actually creates outputs.",
  "47:44": "So sometimes we hide that code.",
  "47:46": "Sometimes we show it.",
  "47:48": "So generally speaking, you can just ignore the stuff like that and focus on what comes",
  "47:52": "out.",
  "47:53": "So I'm not gonna go through these.",
  "47:54": "Instead I'm gonna have a look at it - same thing over here on the slides.",
  "48:01": "So what we're doing here is; we're doing machine learning.",
  "48:05": "Deep learning is a kind of machine learning.",
  "48:08": "What is machine learning?",
  "48:09": "Machine learning is, just like regular programming, it's a way to get computers to do something.",
  "48:16": "But in this case, it's pretty hard to understand how you would use regular programming to recognise",
  "48:22": "dog photos from cat photos.",
  "48:24": "How do you kind of create the loops and the variable assignments and the conditionals",
  "48:28": "to create a program that recognises dogs vs cats in photos.",
  "48:32": "It's super hard.",
  "48:33": "Super super hard.",
  "48:35": "So hard, that until kind of the deep learning era, nobody really had a model that was remotely",
  "48:41": "accurate at this apparently easy task.",
  "48:44": "Because we can't write down the steps necessary.",
  "48:47": "So normally, you know, we write down a function that takes some inputs and goes through our",
  "48:51": "program.",
  "48:52": "Produces some results.",
  "48:56": "So this general idea where the program is something that we write (the steps).",
  "49:03": "Doesn't seem to work great for things like recognising pictures.",
  "49:07": "So back in 1949, somebody named Arthur Samuel started trying to figure out a way to solve",
  "49:12": "problems like recognising pictures of cats and dogs.",
  "49:16": "And in 1962, he described a way of doing this.",
  "49:23": "Well first of all he described the problem: \u201cProgramming a computer for these kinds",
  "49:26": "of computations is at best a difficult task.",
  "49:31": "Because of the need to spell out every minute step of the process in exasperating detail.",
  "49:38": "Computers are giant morons which all of us coders totally recognise.\u201d",
  "49:42": "So he said, okay, let's not tell the computer the exact steps, but let's give it examples",
  "49:47": "of a problem to solve and figure out how to solve it itself.",
  "49:50": "And so, by 1961 he had built a checkers program that had beaten the Connecticut state champion,",
  "49:56": "not by telling it the steps to take to play checkers, but instead by doing this, which",
  "50:03": "is: \u201carrange for an automatic means of testing the effectiveness of a weight assignment in",
  "50:10": "terms of actual performance and a mechanism for altering the weight assignment so as to",
  "50:16": "maximise performance.\u201d",
  "50:19": "This sentence is the key thing.",
  "50:22": "And it's a pretty tricky sentence so you can spend some time on it.",
  "50:24": "The basic idea is this; instead of saying inputs to a program and then outputs.",
  "50:32": "Let's have inputs to a - let's call the program now model.",
  "50:36": "It is the same basic idea.",
  "50:38": "Inputs to a model and results.",
  "50:40": "And then we're gonna have a second thing called weights.",
  "50:44": "And so the basic idea is that this model is something that creates outputs based not only",
  "50:51": "on, for example, the state of a checkers board, but also based on some set of weights or parameters",
  "50:58": "that describe how that model is going to work.",
  "51:02": "So the idea is, if we could, like, enumerate all the possible ways of playing checkers,",
  "51:09": "and then kind of describe each of those ways using some set of parameters or what Samuel",
  "51:14": "called weights.",
  "51:16": "Then if we had a way of checking how effective a current weight assignment is in terms of",
  "51:22": "actual performance, in other words, does that particular enumeration of a strategy for playing",
  "51:28": "checkers end up winning or losing games, and then a way to alter the weight assignment",
  "51:33": "so as to maximise the performance.",
  "51:36": "So then oh let's try increasing or decreasing each one of those weights one at a time to",
  "51:41": "find out if there is a slightly better way of playing checkers and then do that lots",
  "51:45": "of lots of times then eventually such a procedure could be made entirely automatic and then",
  "51:52": "the machine so programmed would learn from its experience so this little paragraph is,",
  "51:58": "is the thing.",
  "52:00": "This is machine learning a way of creating programs such that they learn, rather than",
  "52:09": "programmed.",
  "52:11": "So if we had such a thing, then we would basically now have something that looks like this: you",
  "52:17": "have inputs and weights again going into a model, creating results, i.e. you won or you",
  "52:23": "lost, and then a measurement of performance.",
  "52:27": "So remember that was this key step and then the second key step is a way to update the",
  "52:30": "weights based on the measured performance and then you could look through this process",
  "52:36": "and create a) train a machine learning model so this is the abstract idea.",
  "52:43": "So after it ran for a while, right, it's come up with a set of weights which it's pretty",
  "52:49": "good, right, we can now forget the way it was trained and we have something that is",
  "52:55": "just like this, right, except the word program is now replaced with the word model.",
  "53:02": "So a trained model can be used just like any other computer program.",
  "53:07": "So the idea is we are building a computer program not by putting up the steps necessary",
  "53:14": "to do the task, but by training it to learn to do the task at the end of which it's just",
  "53:20": "another program and so this is what's called inference right is using a trained model as",
  "53:27": "a program to do a task such as playing checkers so machine learning is training programs developed",
  "53:38": "by allowing a computer to learn from its experience rather than through manually coding.",
  "53:43": "Ok how would you do this for image recognition, what is that model and that set of weights",
  "53:54": "such that as we vary them it could get better and better at recognising cats versus dogs,",
  "54:00": "I mean for checkers",
  "54:02": "It's not too hard to imagine how you could kind of enumerate, depending on different",
  "54:07": "kinds of \u201chow far away the opponent's piece is from your piece,\u201d what should you do",
  "54:11": "in that situation.",
  "54:12": "How should you weigh defensive versus aggressive strategies, blah blah blah.",
  "54:16": "Not at all obvious how you do that for image recognition.",
  "54:20": "So what we really want, is some function in here which is so flexible that there is a",
  "54:29": "set of weights that could cause it to do anything.",
  "54:33": "A real--like the world's most flexible possible function--and turns out that there is such",
  "54:40": "a thing.",
  "54:41": "It's a neural network.",
  "54:44": "So we'll be describing exactly what that mathematical function is in the coming lessons.",
  "54:50": "To use it, it actually doesn't really matter what the mathematical function is.",
  "54:56": "It's a function which is, we say, \u201cparameterised\u201d by some set of weights by which I mean, as",
  "55:04": "I give it a different set of weights it does a different task, and it can actually do any",
  "55:12": "possible task: something called the universal approximation theorem tells us that mathematically",
  "55:18": "provably, this functional form can solve any problem that is solvable to any level of accuracy.",
  "55:26": "If you just find the right set of weights.",
  "55:29": "Which is kind of restating what we described earlier in that, like, how do we deal with",
  "55:33": "Minsky (the Marvin Minsky) problem so neural networks are so flexible that if you could",
  "55:40": "find the right set of weights they can solve any problem including \u201cIs this a car or",
  "55:45": "is this dog.\u201d",
  "55:46": "So that means you need to focus your effort on the process of training that is finding",
  "55:51": "good weights, good weight assignments to use Samuel's terminology.",
  "55:57": "So how do you do that?",
  "56:00": "We want a completely general way to do this--to update the weights based on some measure of",
  "56:09": "performance, such as how good is it at recognising cats versus dogs.",
  "56:14": "And luckily it turns out such a thing exists!",
  "56:17": "And that thing is called stochastic gradient descent (or SGD).",
  "56:22": "Again, we'll look at exactly how it works, we'll build it ourselves from scratch, but",
  "56:27": "for now we don't have to worry about it.",
  "56:29": "I will tell you this, though, neither SGD nor neural nets are at all mathematically",
  "56:34": "complex.",
  "56:35": "They nearly entirely are addition and multiplication.",
  "56:39": "The trick is it just a lot of them--like billions of them--so many more than we can intuitively",
  "56:45": "grasp.",
  "56:46": "They can do extraordinarily powerful things, but they're not rocket science at all.",
  "56:53": "They are not complex things, and we will see exactly how they work.",
  "56:59": "So that's the Arthur Samuel version, right?",
  "57:03": "Nowadays we don't use quite the same terminology, but we use exactly the same idea.",
  "57:09": "So that function that sits in the middle,",
  "57:13": "we call an architecture.",
  "57:15": "An architecture is the function that we're adjusting the weights to get it to do something.",
  "57:21": "That's the architecture, that's the functional form of the model.",
  "57:24": "Sometimes people say model to mean architecture, so don't let that confuse you too much.",
  "57:29": "But, really the right word is architecture.",
  "57:31": "We don't call them weights; we call them parameters.",
  "57:35": "Weights has a specific meaning- it's quite a particular kind of parameter.",
  "57:40": "The things that come out of the model, the architecture with the parameters, we call",
  "57:47": "them predictions.",
  "57:50": "The predictions are based on two kinds of inputs: independent variables that's the data,",
  "57:56": "like the pictures of the cats and dogs, and dependent variables also known as labels,",
  "58:03": "which is like the thing saying \u201cthis is a cat\u201d, \u201cthis is a dog\u201d, \u201cthis is",
  "58:07": "a cat\u201d.",
  "58:08": "So, that's your inputs.",
  "58:10": "So, the results are predictions.",
  "58:13": "The measure of performance, to use Arthur Samuel's word, is known as the loss.",
  "58:19": "So, the loss is being calculated from the labels on the predictions and then there's",
  "58:24": "the update back to the parameters.",
  "58:27": "Okay, so, this is the same picture as we saw, but just putting in the words that we use",
  "58:33": "today.",
  "58:34": "So, this picture- if you forget, if I say these are the parameters of this used for",
  "58:40": "this architecture to create a model- you can go back and remind yourself what they mean.",
  "58:44": "What are the parameters?",
  "58:45": "What are the predictions?",
  "58:46": "What is the loss?",
  "58:48": "Okay, the loss of some function that measures the performance of the model in such a way",
  "58:54": "that we can update the parameters.",
  "58:57": "So, it's important to note that deep learning and machine learning are not magic, right?",
  "59:06": "The model can only be created where you have data showing you examples of the thing that",
  "59:13": "you're trying to learn about.",
  "59:15": "It can only learn to operate on the patterns that you've seen in the input used to train",
  "59:21": "it, right?",
  "59:22": "So, if we don't have any line drawings of cats and dogs, then there's never going to",
  "59:27": "be an update to the parameters that makes the architecture and so the architect and",
  "59:33": "the parameters together is the model.",
  "59:34": "So, to say the model, that makes the model better at predicting line drawings of cats",
  "59:40": "and dogs because they just, they never received those weight updates because they never received",
  "59:44": "those inputs.",
  "59:47": "Notice also that this learning approach only ever creates predictions.",
  "59:51": "It doesn't tell you what to do about it.",
  "59:54": "That's going to be very important when we think about things like a recommendation system",
  "59:58": "of like \u201cwhat product do we recommend to somebody\u201d?",
  "60:01": "Well, I don't know- we don't do that, right?",
  "60:05": "We can predict what somebody will say about a product we've shown them, but we're not",
  "60:10": "creating actions.",
  "60:11": "We're creating predictions.",
  "60:12": "That's a super important difference to recognize.",
  "60:17": "It's not enough just to have examples of input data like pictures of dogs and cats.",
  "60:22": "We can't do anything without labels.",
  "60:26": "And so very often, organisations say: \u201cwe don't have enough data\u201d.",
  "60:31": "Most of the time they mean: \u201cwe don't have enough labelled data\u201d.",
  "60:35": "Because if a company is trying to do something with deep learning, often it's because they're",
  "60:40": "trying to automate or improve something they're already doing.",
  "60:43": "Which means by definition they have data about that thing or a way to capture data about",
  "60:48": "that thing.",
  "60:49": "Cus they're doing it.",
  "60:50": "Right?",
  "60:51": "But often the tricky part is labelling it.",
  "60:55": "So for example in medicine.",
  "60:58": "If you're trying to build a model for radiology.",
  "61:01": "You can almost certainly get lots of medical images about just about anything you can think",
  "61:06": "of.",
  "61:07": "But it might be very hard to label them according to malignancy of a tumour or according to",
  "61:12": "whether or not meningioma is present or whatever, because these kinds of labels are not necessarily",
  "61:18": "captured in a structured way, at least in the US medical system.",
  "61:24": "So that's an important distinction that really impacts your kind of strategy.",
  "61:32": "So then a model, as we saw from the PDP book, a model operates in an environment.",
  "61:39": "Right?",
  "61:40": "You roll it out and you do something with.",
  "61:44": "And so then, this piece of that kind of PDP framework is super important.",
  "61:50": "Right?",
  "61:51": "You have a model that's actually doing something.",
  "61:54": "For example, you've built a predictive policing model that predicts (doesn't recommend actions)",
  "61:59": "it predicts where an arrest might be made.",
  "62:02": "This is something a lot of jurisdictions in the US are using.",
  "62:07": "Now it's predicting that, based on data and based on labelled data.",
  "62:11": "And in this case it's actually gonna be using (in the US) for example data where, I think,",
  "62:21": "depending on whether you're black or white, black people in the US, I think, get arrested",
  "62:25": "something like seven times more often for say marijuana possession than whites.",
  "62:31": "Even though the actual underlying amount of marijuana use is about the same in the two",
  "62:38": "populations.",
  "62:39": "So if you start with biased data and you build a predictive policing model.",
  "62:42": "Its prediction will say: \u201coh you will find somebody you can arrest here\u201d based on some",
  "62:49": "biased data.",
  "62:50": "So then, law enforcement officers might decide to focus their police activity on the areas",
  "62:56": "where those predictions are happening.",
  "62:58": "As a result of which they'll find more people to arrest.",
  "63:02": "And then they'll use that, to put it back into the model.",
  "63:05": "Which will now find: \u201coh there's even more people we should be arresting in the black",
  "63:10": "neighbourhoods\u201d and thus it continues.",
  "63:13": "So this would be an example of how a model interacting with its environment creates something",
  "63:17": "called a positive feedback loop.",
  "63:19": "Where the more a model is used, the more biased the data becomes, making the model even more",
  "63:24": "biased and so forth.",
  "63:26": "So one of the things to be super careful about with machine learning is; recognising how",
  "63:32": "that model is actually being used and what kinds of things might happen as a result of",
  "63:38": "that.",
  "63:39": "I was just going to add that this is an example of proxies because here arrest is being used",
  "63:49": "as a proxy for crime, and I think that pretty much in all cases, the data that you actually",
  "63:56": "have is a proxy for some value that you truly care about.",
  "64:00": "And that difference between the proxy and the actual value often ends up being significant.",
  "64:06": "Thanks, Rachel.",
  "64:11": "That's a really important point.",
  "64:13": "Okay, so let's finish off by looking at what's going on with this code.",
  "64:24": "So the code we ran is, basically -- one, two, three, four, five, six -- lines of code.",
  "64:35": "So the first line of code is an import line.",
  "64:40": "So in Python you can't use an external library until you import from it.",
  "64:47": "Normally in place, people import just the functions and classes that they need from",
  "64:53": "the library.",
  "64:55": "But Python does provide a convenient facility where you can import everything from a module,",
  "65:02": "which is by putting a start there.",
  "65:04": "Most of the time, this is a bad idea.",
  "65:07": "Because, by default, the way Python works is that if you say import star, it doesn't",
  "65:12": "only import the things that are interesting and important in the library you're trying",
  "65:17": "to get something from.",
  "65:19": "But it also imports things from all the libraries it used, and all the libraries they used,",
  "65:23": "and you end up kind of exploding your namespace in horrible ways and causing all kinds of",
  "65:27": "bugs.",
  "65:29": "Because fastai is designed to be used in this REPL environment where you want to be able",
  "65:37": "to do a lot of quick rapid prototyping, we actually spent a lot of time figuring out",
  "65:42": "how to avoid that problem so that you can import star safely.",
  "65:45": "So, whether you do this or not, is entirely up to you.",
  "65:50": "But rest assured that if you import star from a fastai library, it's actually been explicitly",
  "65:57": "designed in a way that you only get the bits that you actually need.",
  "66:02": "One thing to mention is in the video you see it's called \u201cfastai2.\u201d",
  "66:06": "That's because we're recording this video using a prerelease version.",
  "66:10": "By the time you are watching the online, the MOOC, version of this, the 2 will be gone.",
  "66:19": "Something else to mention is, there are, as I speak, four main predefined applications",
  "66:26": "in fastai, being vision, text, tabular and collaborative filtering.",
  "66:31": "We'll be learning about all of them and a lot more.",
  "66:35": "For each one, say here's vision, you can import from the .all, kind of meta-model, I guess",
  "66:42": "we could call it.",
  "66:43": "And that will give you all the stuff that you need for most common vision applications.",
  "66:48": "So, if you're using a REPL system like Jupyter notebook, it's going to give you all the stuff",
  "66:56": "right there that you need without having to go back and figure it out.",
  "67:02": "One of the issues with this is a lot of the python users don't.",
  "67:07": "If they look at something like untar_data, they would figure out where it comes from",
  "67:12": "by looking at the import line.",
  "67:14": "And so if you import star, you can't do that anymore.",
  "67:16": "The good news, in a REPL, you don't have to.",
  "67:20": "You can literally just type the symbol, press SHIFT - ENTER and it will tell you exactly",
  "67:27": "where it came from.",
  "67:29": "As you can see.",
  "67:30": "So that's super handy.",
  "67:34": "So in this case, for example, to do the actual building of the dataset, we called ImageDataLoaders.from_name_func.",
  "67:44": "I can actually call the special doc function to get the documentation for that.",
  "67:51": "As you can see, it tells me exactly everything to pass in, what all the defaults are, and",
  "67:57": "most importantly, not only what it does, but SHOW IN DOCS pops me over to the full documentation",
  "68:08": "including an example.",
  "68:11": "Everything in the fastAI documentation has an example and the cool thing is: the entire",
  "68:17": "documentation is written in Jupyter Notebooks.",
  "68:21": "So that means you can actually open the Jupyter Notebook for this documentation and run the",
  "68:26": "line of code yourself and see it actually working and look at the outputs and so forth.",
  "68:33": "Also in the documentation, you'll find that there are a bunch of tutorials.",
  "68:37": "For example, if you look at the vision tutorial, it will cover lots of things but one of the",
  "68:41": "things we will cover is, as you can see in this case, pretty much the same kind of stuff",
  "68:45": "we are actually looking at in Lesson 1.",
  "68:48": "So there is a lot of documentation in fastAI and taking advantage of it is a pretty good",
  "68:53": "idea.",
  "68:54": "It is fully searchable and as I mentioned, perhaps most importantly, every one of these",
  "68:59": "documentation pages is also a fully interactive Jupyter Notebook.",
  "69:05": "So, looking through more of this code, the first line after the import is something that",
  "69:14": "uses untar_ data.",
  "69:15": "That will download a dataset, decompress it, and put it on your computer.",
  "69:20": "If it is already downloaded, it won't download it again.",
  "69:23": "If it is already decompressed it won't decompress it again.",
  "69:26": "And as you can see, fastAI already has predefined access to a number of really useful datasets.",
  "69:33": "such as this PETS dataset.",
  "69:34": "Datasets are a super important part, as you can imagine of deep learning.",
  "69:40": "We will be seeing lots of them.",
  "69:42": "And these are created by lots of heroes (and heroines) who basically spend months or years",
  "69:48": "collating data that we can use to build these models.",
  "69:51": "The next step is to tell fastAI what this data is and we will be learning a lot about",
  "70:00": "that.",
  "70:01": "But in this case, we are basically saying, \u2018okay, it contains images'.",
  "70:06": "It contains images that are in this path.",
  "70:08": "So untar_data returns the path that is whereabouts it has been decompressed to.",
  "70:14": "Or if it is already decompressed, it tells us where it was previously decompressed to.",
  "70:19": "We have to tell it things like \u2018okay, what images are actually in that path'.",
  "70:24": "One of the really interesting ones is label_func.",
  "70:27": "How do you tell, for each file, whether it is a cat or a dog.",
  "70:34": "And if you actually look at the ReadME for the original dataset, it uses a slightly quirky",
  "70:37": "thing which is they said, \u2018oh, anything where the first letter of the filename is",
  "70:42": "an uppercase is a cat'.",
  "70:45": "That's what they decided.",
  "70:46": "So we just created a little function here called is_cat that returns the first letter,",
  "70:51": "is it uppercase or not.",
  "70:52": "And we tell fastai that's how you tell if it's a cat.",
  "70:57": "We'll come back to these two in a moment.",
  "71:01": "So the next thing, now we've told it what the data is.",
  "71:05": "We then have to create something called a learner.",
  "71:06": "A learner is a thing that learns, it does the training.",
  "71:10": "So you have to tell it what data to use.",
  "71:13": "Then you have to tell it what architecture to use.",
  "71:17": "I'll be talking a lot about this in the course.",
  "71:20": "But, basically, there's a lot of predefined neural network architectures that have certain",
  "71:25": "pros and cons.",
  "71:27": "And for computer vision, the architecture is called ResNet.",
  "71:30": "Just a super great starting point, and so we're just going to use a reasonably small",
  "71:35": "one of them.",
  "71:36": "So these are all predefined and set up for you.",
  "71:40": "And then you can tell fastai what things you want to print out as it's training.",
  "71:43": "And in this case, we're saying \u201coh, tell us the error, please, as you train\u201d.",
  "71:48": "So then we can call this really important method called fine_tune that we'll be learning",
  "71:51": "about in the next lesson which actually does the training.",
  "71:57": "valid_pct does something very important.",
  "72:01": "It grabs, in this case, 20% of the data (.2 proportion), and does not use it for training",
  "72:08": "a model.",
  "72:09": "Instead, it uses it for telling you the error rate of the model.",
  "72:13": "So, always in fastai this metric, error_rate, will always be calculated on a part of the",
  "72:20": "data which has not been trained with.",
  "72:22": "And the idea here, and we'll talk a lot about more about this in future lessons.",
  "72:27": "But the basic idea here is we want to make sure that we're not overfitting.",
  "72:31": "Let me explain.",
  "72:34": "Overfitting looks like this.",
  "72:35": "Let's say you're trying to create a function that fits all these dots, right.",
  "72:39": "A nice function would look like that, right.",
  "72:44": "But you could also fit, you can actually fit it much more precisely with this function.",
  "72:48": "Look, this is going much closer to all the dots than this one is.",
  "72:51": "So, this is obviously a better function.",
  "72:53": "Except, as soon as you get outside where the dots are, especially if you go off the edges,",
  "73:00": "it's obviously doesn't make any sense.",
  "73:02": "So, this is what you'd call an overfit function.",
  "73:06": "So, overfitting happens for all kinds of reasons.",
  "73:09": "We use a model that's too big or we use not enough data.",
  "73:12": "We'll be talking all about it, right.",
  "73:15": "But, really the craft of deep learning is all about creating a model that has a proper",
  "73:22": "fit.",
  "73:23": "And the only way you know if a model has a proper fit is by seeing whether it works well",
  "73:27": "on data that was not used to train it.",
  "73:32": "And so, we always set aside some of the data to create something called a validation set.",
  "73:37": "The validation set is the data that we use not to touch it at all when we're training",
  "73:41": "a model, but we're only using it to figure out whether the model's actually working or",
  "73:49": "not.",
  "73:51": "One thing that Sylvain mentioned in the book, is that one of the interesting things about",
  "73:57": "studying fastai is you learn a lot of interesting programming practices.",
  "74:04": "And so I've been programming, I mean, since I was a kid, so like 40 years.",
  "74:11": "And Sylvain and I both work really, really hard to make python do a lot of work for us",
  "74:18": "and to use, you know, programming practices which make us very productive and allow us",
  "74:23": "to come back to our code, years later and still understand it.",
  "74:27": "And so you'll see in our code we'll often do things that you might not have seen before.",
  "74:35": "And so we, a lot of students who have gone through previous courses say they learned",
  "74:39": "a lot about coding and python coding and software engineering from the course.",
  "74:44": "So, yeah check, when you see something new, check it out and feel free to ask on the forums",
  "74:49": "if you're curious about why something was done that way.",
  "74:53": "One thing to mention is, just like I mentioned import star is something most Python programmers",
  "74:59": "don't do cause most libraries don't support doing it properly.",
  "75:06": "We do a lot of things like that.",
  "75:07": "We do a lot of things where we don't follow a traditional approach to python programming.",
  "75:11": "Because I've used so many languages over the years, I code not in a way that's specifically",
  "75:20": "pythonic, but incorporates like ideas from lots of other languages and lots of other",
  "75:24": "notations and heavily customised our approach to python programming based on what works",
  "75:31": "well for data science.",
  "75:34": "That means that the code you see in fastai is not probably, not gonna fit with, the kind",
  "75:41": "of style guides and normal approaches at your workplace, if you use Python there.",
  "75:46": "So, obviously, you should make sure that you fit in with your organization's programming",
  "75:52": "practices rather than following ours.",
  "75:56": "But perhaps in your own hobby work, you can follow ours and see if you find that are interesting",
  "76:01": "and helpful, or even experiment with that in your company if you're a manager and you",
  "76:05": "are interested in doing so.",
  "76:08": "Okay, so to finish, I'm going to show you something pretty interesting, which is, have",
  "76:17": "a look at this code untar data, image data loaders from name func, learner, fine tune.",
  "76:28": "Untar data, segmentation date loaders, from label func, learner, fine tune.",
  "76:34": "Almost the same code, and this has built a model that does something, whoa totally different!",
  "76:39": "It's something which has taken images.",
  "76:43": "This is on the left, this is the labeled data.",
  "76:45": "It's got images with color codes to tell you whether it's a car, or a tree, or a building,",
  "76:52": "or a sky, or a line marking or a road.",
  "76:54": "And on the right is our model, and our model has successfully figured out for each pixel,",
  "76:59": "is that a car, line marking, a road.",
  "77:03": "Now it's only done it in, under 20 seconds right.",
  "77:07": "So it's a very small quick model.",
  "77:08": "It's made some mistakes -- like it's missing this line marking, and some of these cars",
  "77:13": "it thinks is house, right?",
  "77:16": "But you can see so if you train this for a few minutes, it's nearly perfect.",
  "77:21": "But you can see the basic idea is that we can very rapidly, with almost exactly the",
  "77:26": "same code, create something not that classifies cats and dogs but does what's called segmentation:",
  "77:32": "figures out what every pixel and image is.",
  "77:35": "Look, here's the same thing:",
  "77:37": "from import star text loaders from folder",
  "77:40": "learner learn fine-tune",
  "77:43": "Same basic code.",
  "77:44": "This is now something where we can give it a sentence and it can figure out whether that",
  "77:49": "is expressing a positive or negative sentiment, and this is actually giving a 93% accuracy",
  "77:55": "on that task in about 15 minutes on the IMDb dataset, which contains thousands of full-length",
  "78:04": "movie reviews (in fact, 1000- to 3000-word reviews).",
  "78:09": "This number here that we got with the same three lines of code would have been the best",
  "78:14": "in the world for this task in a very, very, very popular academics dataset in like 2015",
  "78:20": "I think.",
  "78:22": "So we are creating world-class models, in our browser, using the same basic code.",
  "78:32": "Here's the same basic steps again:",
  "78:33": "from import star untar data",
  "78:35": "tabular data loaders from csv",
  "78:38": "learner fit",
  "78:40": "This is now building a model that is predicting salary based on a csv table containing these",
  "78:52": "columns.",
  "78:53": "So this is tabular data.",
  "78:57": "Here's the same basic steps",
  "78:58": "from import * untar data",
  "79:00": "collab data loaders from csv learner fine-tune",
  "79:03": "This is now building something which predicts, for each combination of a user and a movie,",
  "79:11": "what rating do we think that user will give that movie, based on what other movies they've",
  "79:17": "watched and liked in the past.",
  "79:19": "This is called collaborative filtering and is used in recommendation systems.",
  "79:23": "So here you've seen some examples of each of the four applications in fastai.",
  "79:30": "And as you'll see throughout this course, the same basic code and also the same basic",
  "79:34": "mathematical and software engineering concepts allow us to do vastly different things using",
  "79:42": "the same basic approach.",
  "79:44": "And the reason why is because of Arthur Samuel.",
  "79:47": "Because of this basic description of what it is you can do if only you have a way to",
  "79:57": "parameterize a model and you have an update procedure which can update the weights to",
  "80:02": "make you better at your loss function, and in this case we can use neural networks, which",
  "80:10": "are totally flexible functions.",
  "80:14": "So that's it for this first lesson.",
  "80:18": "It's a little bit shorter than our other lessons going to be and the reason for that is that",
  "80:23": "we are as I mentioned at the start of a global pandemic here, or at least in the West (in",
  "80:30": "other countries they are much further into it).",
  "80:33": "So we spent some time talking about that at the start of the course and you can find that",
  "80:37": "video elsewhere.",
  "80:40": "So in the future lessons there will be more on deep learning.",
  "80:46": "So, what I suggest you do over the next week, before you work on the next lesson, is just",
  "80:54": "make sure that you can spin up a GPU server, you can shut down when it's finished and that",
  "80:58": "you can run all of the code here and, as you go through, see if this is using Python in",
  "81:06": "a way you recognise, use the documentation, use that doc function, do some search on the",
  "81:15": "fastai doc, see what it does, see if you can actually grab the fastai documentation notebooks",
  "81:20": "themselves and run them.",
  "81:22": "Just try to get comfortable, that you can if you can know your way around.",
  "81:26": "Because the most important thing to do with this style of learning, this top-down learning,",
  "81:30": "is to be able to run experiments and that means you need to be able to run code.",
  "81:35": "So my recommendation is: don't move on until you can run the code, read the chapter of",
  "81:41": "the book, and then go through the questionnaire.",
  "81:48": "We still got some more work to do about validation sets and test sets and transfer learning.",
  "81:53": "So you won't be able to do all of it yet but try to to all the parts you can, based on",
  "81:58": "what we've seen of the course so far.",
  "82:01": "Rachel, anything you want to add before we go.",
  "82:04": "Okay, so thanks very much for joining us for lesson one everybody and are really looking",
  "82:08": "forward to seeing you next time where we will learn about transfer learning and then we",
  "82:15": "will move on to creating an actual production version of an application that we can actually",
  "82:22": "put out on the Internet and you can start building apps that you can show your friends",
  "82:28": "and they can start playing with.",
  "82:30": "Bye Everybody"
}