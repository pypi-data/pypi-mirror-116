{
  "00:00": "Welcome to lesson five and we'll be talking about ethics for data science and this corresponds",
  "00:05": "to chapter 3 of the book.",
  "00:07": "I've also just taught a six-week version of this course, I'm currently teaching an eight-week",
  "00:13": "version of this course and will release some combination or subset of that as a Fast AI",
  "00:19": "and USF ethics for data science class.",
  "00:22": "If you want more detail, coming in July.",
  "00:25": "I am Rachel Thomas I am the founding director of the Center for Applied Data Ethics at the",
  "00:31": "University of San Francisco and also co-founder of fastai together with Jeremy Howard.",
  "00:35": "My background, I have a PhD in math and worked as a data scientist and software engineer",
  "00:41": "in the tech industry and then have been working at USF and on fastai for the past four years",
  "00:48": "now.",
  "00:51": "So ethics issues are in the news.",
  "00:56": "These articles I think are all from this fall, kind of showing up at this intersection of",
  "01:01": "how technology is impacting our world in many kind of increasingly powerful ways.",
  "01:08": "Many of which really raised concerns and I want to start by talking about three cases",
  "01:13": "that I hope everyone working in technology knows about and is on the lookout for.",
  "01:17": "So even if you only watch five minutes of this video, these are kind of the three cases",
  "01:22": "I want you to see and one is feedback loops.",
  "01:25": "And so feedback loops can occur whenever your model is controlling the next round of data",
  "01:30": "you get.",
  "01:31": "So the data that's returned quickly becomes flawed by the software itself.",
  "01:35": "And this can show up in many places.",
  "01:38": "One example is with recommendation systems.",
  "01:40": "And so recommendation systems are essentially about predicting what content the user will",
  "01:45": "like but they're also determining what content the user is even exposed to and helping determine",
  "01:52": "what has a chance of becoming popular.",
  "01:54": "And so YouTube has gotten a lot of attention about this for kind of highly recommending",
  "02:03": "many conspiracy theories, many kind of very damaging conspiracy theories.",
  "02:07": "There is also they've kind of put together recommendations of paedophilia picked out",
  "02:13": "of what were kind of in home movies but when are kind of strung together, ones that happen",
  "02:18": "to have young girls in bathing suits or in their pajamas.",
  "02:25": "So there's some really really concerning results and this is not something that anybody intended",
  "02:31": "and we'll talk about this more later.",
  "02:32": "Um, I think particularly for many of us coming from a science background we are often used",
  "02:36": "to thinking of like oh you know like we observe the data but really whenever you're building",
  "02:40": "products that interact with the real world you're also kind of controlling what the data",
  "02:45": "looks like.",
  "02:49": "Second case study I want everyone to know about it comes from software that's used to",
  "02:54": "determine poor people's health benefits.",
  "02:57": "It's used in over half of the 50 states and the Verge did an investigation on what happened",
  "03:02": "when it was rolled out in Arkansas and what happened is there was a bug and the software",
  "03:06": "implementation that incorrectly cut coverage for people with cerebral palsy or diabetes,",
  "03:12": "including Tammy Dobbs who's pictured here and was interviewed in the article.",
  "03:17": "And so these are people that really needed this health care and it was erroneously cut",
  "03:23": "due to this bug and so they were really and they couldn't get any sort of explanation",
  "03:28": "and there was no appeals or recourse process in place.",
  "03:32": "And eventually, this all came out through a lengthy court case.",
  "03:35": "But it's something where it caused a lot of suffering in the meantime.",
  "03:40": "And so it's really important to implement systems with a way to identify and address",
  "03:44": "mistakes and to do that quickly in a way that hopefully minimizes damage because we all",
  "03:49": "know software can have bugs.",
  "03:51": "Our code can behave in unexpected ways and we need to be prepared for that.",
  "03:57": "I wrote more about this idea in a post two years ago \u201cWhat HBR gets wrong about algorithms",
  "04:03": "and bias\u201d . And then the third case study that everyone should know about, so this is",
  "04:09": "Latanya Sweeney who's director of the Data Privacy lab at Harvard, has a PhD in Computer",
  "04:15": "Science.",
  "04:16": "And She noticed several years ago that when you google her name you would get these ads",
  "04:21": "saying \u201cLatanya, Sweeney, Arrested?\u201d",
  "04:24": "implying that she has a criminal record.",
  "04:25": "She's the only Latanya Sweeney and has never been arrested.",
  "04:28": "She paid $50 to the background check company and confirmed that she's never been arrested.",
  "04:33": "She tried googling some other names and she noticed, for example, Kristen Lindquist got",
  "04:39": "much more neutral ads that just say we found Kristen Lindquist even though Kristen Lindquist",
  "04:44": "has been arrested three times.",
  "04:46": "And so being a computer scientist, Dr Sweeney to study this very systematically she looked",
  "04:51": "at over 2,000 names and found that this pattern held in which disproportionately African American",
  "04:58": "names were getting these ads suggesting that the person had a criminal record regardless",
  "05:03": "of whether they did and Traditionally European American or white names",
  "05:07": "were getting more neutral ads.",
  "05:11": "And this problem of, kind of, bias in advertising shows up a ton.",
  "05:15": "Advertising is, kind of, the profit model for most of the major tech platforms, and",
  "05:23": "it kind of continues to pop up in high-impact ways.",
  "05:26": "Just last year there was research showing how Facebook's ad system discriminates even",
  "05:31": "when the person placing the ad is not trying to do so.",
  "05:35": "So for instance the same housing ad, exact same text, if you change the photo between",
  "05:41": "a white family and a black family, it served two very different audiences.",
  "05:45": "So this is something that can really impact people when they're looking for housing, when",
  "05:48": "they're applying for jobs, and is a, is a definite area of concern.",
  "05:56": "So now I want to, kind of, step back and ask why, why does this matter.",
  "06:00": "And, so a very kind of extreme, extreme example, it's just that data collection has played",
  "06:05": "a pivotal role in several genocides, including, including the Holocaust.",
  "06:10": "And so this is a photo of Adolf Hitler meeting with the CEO of IBM at the time.",
  "06:17": "I think this photo was taken in 1937, and IBM continued to partner with the Nazis, kind",
  "06:24": "of, long past when many other companies broke their ties.",
  "06:28": "They produced computers that were used in concentration camps to code whether people",
  "06:33": "were Jewish, how they were executed, and this is also different from now, where you might",
  "06:40": "sell somebody a computer, and they never hear from them again.",
  "06:43": "These machines require a lot of maintenance, some kind of ongoing relationship with vendors",
  "06:46": "to, kind of, upkeep and repair them.",
  "06:49": "And it's something that a Swiss judge ruled: It does not seem unreasonable to deduce that",
  "06:55": "IBM's technical assistance facilitated the task of the Nazis in the commission of their",
  "06:59": "crimes against humanity.",
  "07:01": "Acts also involving accountancy and classification by IBM machines and utilized in the concentration",
  "07:06": "camps themselves.\u2019",
  "07:07": "I'm told that they haven't gotten around to apologizing yet.",
  "07:11": "Oh, that\u2019s...",
  "07:12": "I guess they've been busy.",
  "07:14": "\u2026terrible too, yeah.",
  "07:15": "Okay.",
  "07:16": "Yeah, and so this is a very, kind of, very sobering example, but I think it's important",
  "07:22": "to keep in mind, kind of, what can go wrong, and how technology can be used for, for harm.",
  "07:29": "For very, very terrible harm.",
  "07:32": "And so this just kind of raises a question, questions that we all need to grapple with",
  "07:36": "of: \u2018How would you feel if you discovered that you had been part of a system that ended",
  "07:39": "up hurting society?\u2019",
  "07:41": "\u2018Would you, would you even know?",
  "07:42": "\u2018Would you be open to finding out, kind of, how, how things you had built may have",
  "07:48": "been harmful?\u2019 and \u2018How can you help make sure this doesn't happen?\u2019",
  "07:51": "And so I think these are questions that we all, all need to grapple with.",
  "07:56": "It's also important to think about unintended consequences on how your tech could be used,",
  "08:04": "or misused whether that's by harassors, by authoritarian governments, for propaganda",
  "08:10": "or disinformation.",
  "08:13": "And then on a, kind of a more concrete level you could even end up in jail.",
  "08:17": "And so there was a Volkswagen engineer who got prison time for his role in the diesel",
  "08:23": "cheating case.",
  "08:24": "So if you remember this is where Volkswagen was cheating on emissions test, and one of",
  "08:28": "the, kind of, programmers that was a part of that.",
  "08:32": "And that person was just following orders from what their boss told them to do, but",
  "08:35": "that is not, not a good excuse for, for doing something that's unethical.",
  "08:40": "And so something to be aware of.",
  "08:46": "So ethics is, the, the discipline dealing with what's good and bad.",
  "08:49": "It's a set of moral principles.",
  "08:52": "It's not a set of answers, but it's kind of learning what sort of, what sort of questions",
  "08:57": "to ask, and even how to weigh these decisions.",
  "09:00": "And I'll say some more about, kind of, ethical foundations, and different ethical philosophies",
  "09:06": "later, later on in this lesson.",
  "09:08": "But first I'm going to, kind of, start with some, some use cases.",
  "09:12": "Ethics is not the same as religion, laws, social norms, or feelings.",
  "09:17": "Although it does have overlap with all these things.",
  "09:20": "It's not a fixed set of rules.",
  "09:23": "It's well founded standards of right and wrong, and this is something where clearly not everybody",
  "09:28": "agrees on the ethical action in, in, every case, but that doesn't mean that, kind of,",
  "09:34": "anything goes, or that all actions are considered equally ethical.",
  "09:37": "There are many things that are widely agreed upon, and there are, kind of, a philosophical,",
  "09:44": "philosophical underpinnings for, kind of, making these decisions.",
  "09:46": "And ethics is also the ongoing study and development of our ethical standards.",
  "09:50": "It's a kind of, never-ending process of learning to, kind of, practice our ethical wisdom.",
  "09:57": "And I'm gonna refer it, several times too...",
  "10:00": "So here I'm referring to a few articles from the Markkula Center for tech ethics at Santa",
  "10:05": "Clara University.",
  "10:07": "In particular the work of Shannon Vallor, Brian Green and Irina Raicu, who is fantastic",
  "10:12": "and they have a lot of resources, some of which I'll circle back to later, later in",
  "10:16": "this talk.",
  "10:17": "I spent years of my life studying ethics.",
  "10:20": "It was my major at university and I spent much time on the question of what is ethics?",
  "10:26": "I think I'll a away from that is that studying the philosophy ethics was not particularly",
  "10:30": "helpful in learning about ethics.",
  "10:33": "Yes, and I will try to keep this kind of very, very applied and very practical.",
  "10:38": "Also, very kind of tech industry-specific, of what, what do you need in terms of applied",
  "10:43": "ethics?",
  "10:44": "Markoulis said it's great - they somehow they take stuff that I thought was super dry and",
  "10:48": "turn it into useful checklists and things.",
  "10:53": "I did want to note this was really neat so Casey Fiesler\u2019s a professor at University",
  "11:00": "of Colorado that I really admire and she created a crowd-sourced spreadsheet of tech ethics",
  "11:06": "syllabi.",
  "11:07": "This was maybe two years ago and got over 200 syllabi entered into this this crowd-sourced",
  "11:13": "spreadsheet and then she did a meta-analysis on them, of kind of looking at all sorts of",
  "11:18": "aspects of the syllabi, and what's being taught and how it's being taught and ... And published",
  "11:24": "a paper on it.",
  "11:25": "What do we teach when we teach tech ethics, and a few interesting things about it?",
  "11:29": "Is it raises, there a lot of ongoing discussions and lack of agreement on how to, how to best",
  "11:36": "teach tech ethics.",
  "11:38": "Should it be a standalone course versus worked into every course in the curriculum?",
  "11:42": "Who should teach it - a computer scientist, a philosopher, or a sociologist?",
  "11:48": "And she analyzed for the syllabi what was the course home, and the instructor home?",
  "11:53": "And you can see that the the instructors came from a range of courses, including computer",
  "11:58": "science.",
  "11:59": "A range of disciplines, computer science, information science, philosophy, science and",
  "12:02": "tech studies, engineering, law, math, business.",
  "12:06": "What topics to cover -- a huge range of topics that can be covered, including a law and policy",
  "12:13": "privacy and surveillance in equality justice and human rights, environmental impact, AI",
  "12:19": "and robots, and professional ethics, work in labor, cybersecurity.",
  "12:24": "The list goes on and on, and so this is clearly more than can be covered in any even a full",
  "12:28": "semester length course and certainly not in a kind of a single, single lecture.",
  "12:37": "What learning outcomes?",
  "12:38": "This is an area where there's a little bit more agreement, where kind of the number one",
  "12:41": "skill that courses were trying to teach was critique, followed by spotting issues, making",
  "12:46": "arguments.",
  "12:47": "And so a lot of this is just even learning to spot what the issues are and how to critically",
  "12:53": "evaluate, kind of a piece of technology or a design proposal to see what could go wrong,",
  "12:59": "what the the risks could be.",
  "13:01": "All right.",
  "13:02": "So we're gonna go through kind of a few different core topics.",
  "13:07": "And as I suggested this is gonna be a kind of extreme subset of what could be covered.",
  "13:12": "I was trying to pick things that we think are very important and high-impact.",
  "13:18": "So one is recourse and accountability.",
  "13:21": "So I already shared this example earlier of, you know, the system that was determining",
  "13:27": "poor people's healthcare benefits having a bug.",
  "13:30": "And something that was kind of terrible about this was nobody took responsibility, even",
  "13:35": "once the bug was found.",
  "13:37": "So the creator of the algorithm was interviewed and asked, they asked him you know should",
  "13:44": "people be able to get an explanation for why their benefits have been cut? and he gave",
  "13:48": "this very callous answer of, \u2018You know, yeah, they probably should, but I should probably",
  "13:53": "dust under my bed, you know, like who's gonna do that,\u201d which is very callous.",
  "13:57": "And then he ended up blaming the policymakers for how they had rolled out the algorithm.",
  "14:03": "The policymakers, you know, could blame the software engineers that implemented it, and",
  "14:08": "so there was a lot of passing the buck here.",
  "14:11": "Dana Boyd has said that, you know, it's always been a challenge for bureaucracy to assign",
  "14:19": "responsibility, or bureaucracy is used to evade responsibility, and today's algorithmic",
  "14:24": "systems are often extending bureaucracy.",
  "14:26": "A couple of questions and comments about cultural context of any notes that there didn't seem",
  "14:35": "to be any mention of cultural contexts for ethics as part of those syllabi, and somebody",
  "14:42": "else was asking, \u2018How do you deal, you know, is this culturally dependent?",
  "14:46": "And how do you deal with that?\u201d",
  "14:47": "It is culturally dependent.",
  "14:49": "I will mention this briefly later on, so I'm gonna share three different ethical philosophies",
  "14:54": "that are kind of from the West, and we'll talk just briefly of one slide on.",
  "14:59": "For instance right now, there are a number of indigenous data sovereignty",
  "15:02": "movements.",
  "15:03": "And I know the Maori data sovereignty movement has been particularly active, but different,",
  "15:07": "you know, different cultures do have different views on ethics, and I think that the cultural",
  "15:13": "context is incredibly important.",
  "15:15": "And we will not get into it tonight, but there's also kind of a growing field of algorithmic",
  "15:20": "colonialism, and, kind of, studying what are some of the issues when you have technologies",
  "15:26": "built in one, you know, particular country, and culture, being implemented, you know,",
  "15:32": "halfway across the world in very different cultural context, often with little to no",
  "15:37": "input from people, people living in that culture.",
  "15:42": "And although I do want to say that there are things that are widely, although not universally,",
  "15:49": "agreed on, and so, for instance the Universal Declaration on Human Rights, despite the name",
  "15:55": "it is not universally accepted but many, many different countries have accepted that as",
  "15:59": "a human rights framework, and as those being fundamental rights, and so there are, kind",
  "16:03": "of, principles that are often held cross culturally, although, yeah, it's rare for something, probably,",
  "16:10": "to be truly, truly universal.",
  "16:15": "So returning to this topic of, kind of, accountability and recourse.",
  "16:18": "Something to keep in mind is the data contains errors.",
  "16:24": "And so there was a dank database used in California.",
  "16:28": "It's tracking supposedly gang members, and an auditor found that there were 42 babies",
  "16:37": "under the age of 1, who had been entered into this database.",
  "16:40": "And something concerning about the database is that it's basically never updated, I mean",
  "16:44": "people are added, but they're not removed and so once you're in there, you're in there.",
  "16:49": "And 28 of those babies were marked as having admitted to being gang members.",
  "16:53": "And so keep in mind that this is just a really obvious example of the error, but how many",
  "16:59": "other kind of totally wrong entries are there.",
  "17:03": "Another example of data containing errors involves the, the three credit bureaus in",
  "17:10": "the United States.",
  "17:11": "The FTC's large-scale study of credit reports found that 26% had at least one mistake in",
  "17:17": "their files, and 5% had errors that could be devastating.",
  "17:21": "I'm gonna, this is the headline of an article that was written by a public radio reporter",
  "17:26": "who went to get a an apartment and the landlord called him back afterwards and said, you know,",
  "17:33": "your background check showed up that you had firearms convictions, and this person did",
  "17:38": "not have any firearms convictions, and it's something where in most cases the landlord",
  "17:42": "would probably not even tell, tell you and let you know, that's why you weren't getting",
  "17:45": "the apartment.",
  "17:46": "And so, and this guy looked into it.",
  "17:50": "I should note that this guy was white, which I'm sure helped him in getting the benefit",
  "17:53": "of the doubt and found this error, and he made dozens of calls, and could not get it",
  "17:59": "fixed until he told them that he was a reporter, and that he was going to be writing about",
  "18:03": "it, which is something that most of us would not be able to do.",
  "18:06": "But it was...",
  "18:07": "Even once he had pinpointed the error, and he had to, you know, talk to the, you know,",
  "18:12": "like, County Clerk, in the place he used to live.",
  "18:16": "It was still a very difficult process to get it updated, and this can have a huge, huge",
  "18:21": "impact on people's lives .There's also the issue of when technology is used in ways that",
  "18:30": "the creators may not have intended.",
  "18:31": "So for instance with facial recognition it is pretty much entirely being developed for",
  "18:37": "adults, yet NYPD is putting the photos of children as young as age 11 into, into, databases.",
  "18:45": "And we know the error rates are higher.",
  "18:47": "This is not how it was developed.",
  "18:49": "So this is, this is a, a, serious, serious concern.",
  "18:53": "And there are a number of, kind of, misuses.",
  "18:56": "The Georgetown Center for Privacy and Technology, which is fantastic, you should definitely",
  "19:00": "be following them, did a report, \u2018Garbage In, Garbage Out [...]\u2019, looking at how police",
  "19:06": "were using facial recognition in practice and they found some really concerning examples,",
  "19:12": "for instance, in one case NYPD had a photo of a suspect, and they\u2026",
  "19:19": "It wasn't returning any matches, and they said: \u2018Well this person kind of looks like",
  "19:22": "Woody Harrelson,\u2019 so then they googled the actor Woody Harrelson, and put his face into",
  "19:28": "the facial recognition and used that to generate leads.",
  "19:31": "And this is clearly not the correct use at all, but it's, it's a way that it's being,",
  "19:36": "it's being used.",
  "19:38": "And so there's, kind of, total lack of accountability here.",
  "19:43": "And then another, kind of, study of cases in all 50 states of police officers, kind",
  "19:49": "of, abusing confidential databases to look up ex-romantic partners, or to look up activists",
  "19:55": "and so, you know, here this is not necessarily error in the data, although that can be present",
  "20:02": "as well, but kind of keeping in mind how it can be misused by the users.",
  "20:07": "All right.",
  "20:08": "So the next topic is feedback loops and metrics.",
  "20:11": "And so I talked a bit about feedback loops in the beginning as kind of one of one of",
  "20:14": "the three key use cases.",
  "20:18": "And so this is a topic, I wrote a blog post about this fall \u201cThe problem with metrics",
  "20:23": "is a big problem for AI\u201d.",
  "20:25": "And then together with David Uminsky who is director of The Data Institute expanded this",
  "20:29": "into a paper \u201cReliance on metrics is a fundamental challenge for AI\u201d.",
  "20:33": "And this was accepted to the \u201cEthics and Data Science\u201d conference.",
  "20:38": "But over emphasizing metrics can lead to a number of problems including manipulation,",
  "20:44": "gaming, myopic focus on short-term goals because it's easier to track short-term quantities,",
  "20:53": "unexpected negative consequences.",
  "20:55": "And much of AI and machine learning centers on optimizing a metric.",
  "21:00": "This is kind of both, you know, the strength of machine learning is it's gotten really",
  "21:03": "really good at optimizing metrics.",
  "21:05": "But I think this is also kind of inherently a weakness or a limitation.",
  "21:11": "I'm going to give a few examples.",
  "21:12": "And this can happen even not just in machine learning kind of but in analog examples as",
  "21:20": "well.",
  "21:21": "So this is from a study of when English is, England's public health system implemented",
  "21:27": "a lot more targets around numbers in the early 2000s.",
  "21:33": "And the study was called \u201cWhat's measured is what matters\u201d.",
  "21:36": "And so they found so one of the targets was around reducing ER wait times, which seems",
  "21:40": "like a good goal.",
  "21:42": "However this led to cancelling scheduled operations to draft extra staff into the ER.",
  "21:48": "So if they felt like there were too many people in the ER they would just start canceling",
  "21:51": "operations so they could get more doctors requiring patients to wait in queues of ambulances",
  "21:57": "because time waiting an ambulance didn't count towards your your er wait time.",
  "22:03": "Turning stretchers into beds by putting them in hallways, I mean there are so big discrepancies",
  "22:08": "in the numbers reported by hospitals versus by patients.",
  "22:11": "And so if you ask the hospital on average how long are people waiting you get a very",
  "22:15": "different answer than when you're asking the patients how long did you have to wait?.",
  "22:21": "Another, another example is of essay grading software.",
  "22:29": "And so this essay grading software I believe is being used in 22 states now in the United",
  "22:34": "States.",
  "22:35": "Yes, 20 states and it tends to focus on metrics like sentence length, vocabulary, spelling,",
  "22:42": "subject verb agreement.",
  "22:43": "Because these are the things that we, we know how to measure and how to measure with a computer.",
  "22:47": "But it can't evaluate things like creativity or novelty.",
  "22:53": "However gibberish essays with lots of sophisticated words score well.",
  "22:57": "And there are even examples of people creating computer programs to generate these kind of",
  "23:03": "gibberish sophisticated essays.",
  "23:05": "And then there you know graded by this other computer program and highly rated.",
  "23:11": "And there's also bias in this.",
  "23:13": "Essays by african-american students received lower grades from the computer than from expert",
  "23:18": "human graders.",
  "23:19": "And essays by students from mainland China received higher scores from the computer than",
  "23:25": "from expert human graders.",
  "23:27": "And authors of the study thought that they, this, this results suggest they may be using",
  "23:32": "chunks of pre memorized text that score well.",
  "23:39": "And this is, these are just kind of two examples, I have a bunch more in the blog post and even",
  "23:42": "more in the paper of ways that metrics can invite manipulation and gaming whenever they're",
  "23:49": "they're given a lot of emphasis.",
  "23:51": "And this is a good hearts laws of kind of a law that a lot of people talk about.",
  "23:57": "And it's this idea that the more you rely on a metric the kind of the less reliable",
  "24:02": "it becomes.",
  "24:06": "So returning to this example of feedback loops and recommendation systems, Guillaume Chaslot",
  "24:11": "is a former google, youtube engineer.",
  "24:15": "YouTube is owned by Google and he wrote a really great post.",
  "24:19": "And he's done a ton to raise awareness about this issue and founded the nonprofit \u2018AlgoTransparency\u2019",
  "24:26": "which kind of externally tries to monitor YouTube's recommendations.",
  "24:30": "He's partnered with the Guardian and the Wall Street Journal to do investigations.",
  "24:33": "But he wrote a post around how kind of in the earlier days the recommendation system",
  "24:40": "was designed to maximize watch time.",
  "24:43": "And so and this is, this is something else that's often going on with metrics is that",
  "24:48": "any metric is just a proxy for what you truly care about.",
  "24:51": "And so here, you know the team at Google was saying well, you know, if you're watching",
  "24:56": "more YouTube it signals to, to us that they're happier.",
  "25:01": "However this also ends up incentivizing content that tells you the rest of the media is lying",
  "25:06": "because kind of believing that everybody else is lying will encourage you to spend more",
  "25:11": "time on a particular platform.",
  "25:13": "So Guillaume wrote a great post about this kind of mechanism that's at play and you know,",
  "25:19": "this is not just YouTube.",
  "25:20": "This is any recommendation system could, I think be susceptible to this and there have",
  "25:26": "been a lot of talk about kind of issues with many recommendation systems across platforms.",
  "25:34": "But it is, it is something to be mindful of, and something that the, kind of, creators",
  "25:38": "of this did not anticipate.",
  "25:42": "And last year, Gillaume, kind of, gathered this data on...",
  "25:47": "So here the x-axis is the number of channels, number of YouTube channels recommending a",
  "25:53": "video, and the y-axis is the log of the views, and we see this extreme outlier, which was",
  "25:58": "Russia's Today take, Russia Today's take on the Mueller Report, and this is something",
  "26:04": "that Guillaume observed, and then was picked up by the the Washington Post.",
  "26:09": "But this, this strongly suggests that Russia Today has perhaps gamed the the Recommendation",
  "26:15": "Algorithm, which is, which is not surprising, and it's something that I think many content",
  "26:18": "creators are conscious of, and trying to, you know, experiment and see what, what gets",
  "26:23": "more heavily recommended, and thus more views.",
  "26:27": "So it's, it\u2019s also important to note that our online environments are designed to be",
  "26:31": "addictive, and so when, kind of, what we click on is often used as a proxy of, of what we",
  "26:38": "enjoy, or what we like, that's not necessarily though for of our, kind of, like our best",
  "26:42": "selves, or our higher selves.",
  "26:43": "It's, you know, it's what we're clicking on, in this, kind of, highly addictive environment",
  "26:50": "that's often appealing to some of our, kind of, lower instincts.",
  "26:54": "Zeynep Tufekci uses the analogy of a, of a cafeteria, that's, kind of, shoving salty,",
  "27:00": "sugary, fatty foods in our faces, and then learning that \u2018Hey people really like, salty,",
  "27:04": "sugary, fatty foods!\u2019, which I think most of us do in a, kind of, very primal way, but",
  "27:10": "we often, you know, kind of, our higher self is like \u2018Oh I don't want to be eating junk",
  "27:13": "food all the time,\u2019 and online we often, kind of, don't have a great mechanisms to",
  "27:19": "say, you know, like \u2018Oh I really want to read like more long-form articles that took",
  "27:25": "months to research, and are gonna take a long time to digest.\u2019",
  "27:28": "While we may want to do that our online environments are not, not always conducive to it.",
  "27:33": "Yes?",
  "27:34": "Sylvain made the comment about the false sense of security argument, which is very relevant",
  "27:40": "to masks and things.",
  "27:42": "Don\u2019t you have anything to say about its false sense of security argument?",
  "27:48": "Can you say more?",
  "27:56": "There's a common feedback at the moment that people shouldn\u2019t wear masks, because they",
  "28:03": "might have a false sense of security.",
  "28:05": "That, kind of, makes sense to you from an ethical point of view, to be telling people",
  "28:11": "that?",
  "28:12": "No, I don't think that's a good argument at all.",
  "28:16": "In general there's so many, other people including Jeremy have pointed this out...",
  "28:21": "There's so many actions we take to make our lives safer, whether that's wearing seatbelts,",
  "28:25": "or wearing helmets when biking, practicing safe sex, like all sorts of things where we",
  "28:30": "really want to maximize our safety, and so I think, and Zeynep Tufekci had a great thread",
  "28:37": "on this today of...",
  "28:40": "It's not that there can never be any sort of impact in which people have a false sense",
  "28:44": "of security, but it is something that you would really want to be gathering data on,",
  "28:48": "and build a strong case around, and not just assume it's gonna happen, and that\u2026",
  "28:53": "In most cases people can think of, even if that is a small second-order effect, the effect",
  "28:59": "of doing something that increases safety tends to have a much larger impact on actually increasing",
  "29:05": "safety.",
  "29:06": "You have anything to add to that or...",
  "29:11": "Yes I mentioned before a lot of our incentives are focused on short term metrics.",
  "29:20": "Long term things are much harder to measure, and often involve kind of complex relationships.",
  "29:27": "And then the, the, fundamental business model of, of, most of the tech companies is around",
  "29:32": "manipulating people's behavior, and monopolizing their time, and these things.",
  "29:35": "I don't think an advertising is inherently bad, but they...",
  "29:38": "I think it can be negative when, when taken to an extreme.",
  "29:43": "There's a great essay by James Grimmelmann \u2018The Platform is the Message\u2019, and he",
  "29:48": "points out: \u2018These platforms are structurally at war with themselves.\u2019",
  "29:53": "\u2018The same characteristics that make outrageous & offensive content unacceptable are what",
  "29:59": "make it go viral in the first place.\u2019",
  "30:02": "And so there's this, kind of, real tension here in which often things, yeah, that, kind",
  "30:08": "of, can make content really offensive or unacceptable to us, are also what are, kind of, fuelling",
  "30:14": "their popularity, and being promoted in many cases.",
  "30:19": "I mean this is it, this is an interesting essay, because he, he, does this, like, really",
  "30:23": "in-depth dive on the \u2018Tide Pod Challenge\u2019, which was this meme around eating Tide Pods,",
  "30:30": "which are poisonous, do not eat them, and he really analyzes it though.",
  "30:35": "It's a great look at meme culture, which is very common and how, kind of, argues there's",
  "30:42": "probably no example of someone talking about the \u2018Tide Pod Challenge\u2019, that isn't partially",
  "30:46": "ironic, which is common in memes, that even, kind of, whatever you're saying they're, kind",
  "30:50": "of, layers of irony, and different groups are interpreting them differently, and that",
  "30:54": "even when you try to counteract them, you're still promoting them.",
  "30:57": "So with the \u2018Tide Pod Challenge\u2019 a lot of, like, celebrities we're telling people",
  "31:02": "\u2018Don't eat Tide Pods\u2019, but that was also then, kind of, perpetuating, the, the popularity",
  "31:06": "of this meme.",
  "31:07": "So it's...",
  "31:08": "This is an essay I would recommend, that I think it's pretty insightful.",
  "31:15": "And so this is a...",
  "31:16": "We'll get to disinformation shortly, but the the major tech platforms often incentivize",
  "31:21": "and promote disinformation, and this is unintentional, but it's, it is somewhat built into their",
  "31:25": "design and architecture, their recommendation systems, and ultimately their business models.",
  "31:34": "And then on the...",
  "31:35": "On the topic of metrics.",
  "31:36": "I'm...",
  "31:37": "I just wonder, bring up, so there's this idea of blitzscaling, and the premise is that if",
  "31:43": "a company grows big enough, and fast enough, profits will eventually follow.",
  "31:48": "It prioritizes speed over efficiency, and risks potentially disastrous defeat, and Tim",
  "31:53": "O'Reilly wrote a really great article last year talking about many of the problems with",
  "31:58": "this approach, which, I would say, is incredibly widespread, and is, I would say, the fundam...",
  "32:03": "Kind of fundamental model underlying a lot of venture capital.",
  "32:07": "And in it, though, investors kind of end up anointing winners, as opposed to, to market",
  "32:11": "forces.",
  "32:12": "It tends to lend itself towards creating monopolies and duopolies.",
  "32:17": "It can...",
  "32:19": "It's bad for founders, and people end up kind of spreading themselves too thin.",
  "32:24": "So there are a number, a number of significant downsides to this.",
  "32:27": "Why am I bringing this up in an ethics lesson?",
  "32:30": "When we were talking about metrics.",
  "32:33": "But hockey, hockey stick growth requires automation, and a reliance on metrics.",
  "32:38": "Also prioritizing speed above all else doesn't leave time to reflect on ethics, and that",
  "32:43": "is something that's hard that I think it, you do often have to kind of pause to, to",
  "32:47": "think about ethics, and that following this model, when you do have a problem, it's often",
  "32:53": "going to show up on a huge scale, if you've, if you've scaled very quickly.",
  "32:56": "So, I think, this is something to at least, at least be aware of.",
  "33:06": "So one person asks about: \u2018Is there a dichotomy between AI ethics, which seems like a very",
  "33:14": "First World problem, and wars, poverty, environmental exploitation, has been, kind of, a different",
  "33:20": "level of a problem, I guess?\u2019",
  "33:23": "And there's an answer here, which somebody else, maybe you can comment on whether you",
  "33:27": "agree, or I have anything to add which is that, \u2018AI ethics\u2026\u2019, they're saying,",
  "33:32": "\u2018...is very important also for other parts of the world particularly in areas with high",
  "33:36": "cell phone usage.",
  "33:37": "For example, many countries in Africa have high cell penetration, people get their news",
  "33:42": "from Facebook, and WhatsApp, and YouTube, and though it's useful, it's been the source",
  "33:45": "of many problems\u2019.",
  "33:46": "Did you have any comments on, on kind of...?",
  "33:49": "Yeah so...",
  "33:50": "I think the first question\u2026",
  "33:51": "So AI ethics, as I noted earlier, and I'm using the, the, phrase data ethics here, but",
  "33:56": "it's this very broad, and it refers to a lot of things.",
  "33:59": "I think if people are talking about the, you know, \u2018In the future can computers achieve",
  "34:05": "sentience and what are the ethics around that?\u2019 and that is not my focus at all.",
  "34:12": "I'm very much focused on, and this is our mission with the Center for Applied Data Ethics",
  "34:16": "at the University of San Francisco, is, kind of, how are people being harmed now?",
  "34:19": "What are the most immediate harms?",
  "34:22": "And so in that sense, I don't think that data ethics has to be a First World or, kind of,",
  "34:28": "futuristic issue.",
  "34:29": "It's\u2026",
  "34:30": "It\u2019s what's happening now, and yeah, and as, as the person said in a few examples,",
  "34:34": "well one example, I'll get to later is definitely the, the genocide in Myanmar in which the",
  "34:41": "Muslim minority, the Rohingya are experiencing genocide.",
  "34:47": "The UN has ruled that Facebook played a determining role in that, which is really intense, and",
  "34:55": "terrible.",
  "34:56": "And so, I think, that's an example of technology, yeah, leading to very real harm now.",
  "35:01": "They're also WhatsApp,, which is owned by, owned by Facebook.",
  "35:05": "There have been issues with people spreading disinformation, and rumors, and it's led to",
  "35:11": "several lynching, dozens of lynchings in India.",
  "35:14": "Of people kind of spreading these false rumors of \u2018Oh there's a kidnapper coming around\u2019,",
  "35:19": "and in these kind of small, remote villages, and then a visitor, or, or stranger shows",
  "35:22": "up and gets killed.",
  "35:25": "WhatsApp also played a very important role, or bad role, in the election of Bolsonaro",
  "35:30": "in Brazil, election of Duterte in the Philippines.",
  "35:34": "So, I think, technology is having a kind of very immediate impact on, on people.",
  "35:42": "And that...",
  "35:43": "Those are the types of ethical questions I'm really interested in, and that I hope, I hope",
  "35:47": "you are interested in as well.",
  "35:49": "Do you have anything else to say about that or..?",
  "35:54": "And I will, I will talk about disinformation.",
  "35:55": "I realize those were, kind of, some disinformation focused, and I'm gonna talk about bias first.",
  "36:00": "I think it's bias, then disinformation.",
  "36:03": "Yes?",
  "36:04": "Question.",
  "36:05": "Mhm.",
  "36:06": "\u2018When we talk about ethics, how much of this is intentional unethical behavior?",
  "36:08": "I see a lot of the examples as more of incompetent behavior, or bad modeling, where the product,",
  "36:14": "or models are rushed without sufficient testing, or thought around bias thereforth, but not",
  "36:19": "necessarily malintent.",
  "36:20": "Yeah, no, I agree with that.",
  "36:22": "I think that most of this is unintentional.",
  "36:25": "I do think there's a, often, no...",
  "36:26": "Well...",
  "36:27": "We'll get into some cases.",
  "36:29": "I think that, I think in many cases the profit incentives are misaligned, and I do think",
  "36:36": "that, when people are earning a lot of money it is very hard to consider actions that would",
  "36:41": "reduce their profits, even if they would prevent harm and increase, kind of, ethics.",
  "36:48": "And so I think that, you know, there's at some point where valuing profit over how people",
  "36:54": "are being harmed is, you know, when does, when does that become intentional is, you",
  "37:01": "know, a question to debate, but I, you know, I don't, I don't think people are setting",
  "37:04": "out to say like \u2018I want to cause a genocide\u2019, or \u2018I want to help authoritarian leader",
  "37:09": "get elected\u2019.",
  "37:11": "Most people are not are not starting with that, but I think sometimes it's a carelessness,",
  "37:14": "and a thoughtlessness, but that I, I do think we are responsible for that, and we're responsible",
  "37:20": "to kind of be more careful, and more thoughtful in how we approach things.",
  "37:24": "Alright So bias...",
  "37:27": "So bias, I think, is a, an issue that's probably gotten a lot of attention, which is great",
  "37:33": "and, I want to get a little bit more in-depth, because sometimes discussions on bias stay",
  "37:37": "a bit superficial.",
  "37:39": "There was a great paper by Harini Suresh and John Guttag, last year, that looked at, kind",
  "37:45": "of, came with this taxonomy of different types of bias, and how they had, kind of, different",
  "37:50": "sources in the machine learning, kind of, pipeline.",
  "37:54": "And it was really helpful, because, you know, different sources have different causes, and",
  "37:58": "they also require different, different approaches for addressing then.",
  "38:03": "The...",
  "38:04": "Harini wrote a blog post version of the paper as well, which I love when researchers do",
  "38:09": "that.",
  "38:10": "I hope more of you, if you're writing an academic paper, also write the blogpost version.",
  "38:12": "I'm just going to go through a few of these types.",
  "38:17": "So one is, representation bias, and so I would imagine many of you have heard of Joy Buolamwini\u2019s",
  "38:24": "work, which has rightly received a lot of publicity.",
  "38:28": "In \u2018Gender Shades, she and Timnit Gebru investigated commercial computer vision products",
  "38:33": "from Microsoft, IBM and Face++, and then Joy Buolamwini and Deb Raji, did a follow-up study",
  "38:39": "that looked at Amazon and Kairos and several other companies, and the typical results they",
  "38:44": "kind of found basically everywhere was that these products performed significantly, significantly",
  "38:49": "worse on dark skinned women.",
  "38:51": "So they were, kind of, doing worse on people with darker skin, compared to lighter skin.",
  "38:56": "Worse on women than on men, and then the kind of intersection of that, dark skinned women",
  "38:59": "had these very high error rates.",
  "39:01": "And so one example is IBM, their product was 99.7% accurate on light skinned men, and only",
  "39:10": "65% percent accurate on dark skinned women.",
  "39:12": "And again, this is a commercial computer vision product that was released.",
  "39:16": "Question?",
  "39:17": "It's a question from the TWIML study group.",
  "39:22": "\u2018At the Volkswagen example, in many cases its management that drives and rewards unethical",
  "39:29": "behavior.",
  "39:30": "What can an individual engineer do in a case like this?",
  "39:33": "Especially in a place like Silicon Valley where people move companies so often?\u2019",
  "39:38": "Yeah, so I think, I think that's a great point.",
  "39:41": "Yeah, and that is an example where I would have, I would have much rather seen people",
  "39:45": "that were higher ranking doing jail time about this, because, I think, that they were, they",
  "39:49": "were driving that, and, I think, that, yeah, it's great to remember that.",
  "39:54": "I know many people in the world don't have this option, but I think for many of us working",
  "40:03": "in tech, particularly in Silicon Valley, we tend to have a lot of options, and often more",
  "40:07": "options than we realize like.",
  "40:08": "Okay?",
  "40:09": "I talk to people frequently that feel trapped in their jobs, even though, you know, they're",
  "40:15": "a software engineer in Silicon Valley, and, and so many companies are hiring.",
  "40:18": "And so I think it is important to use that leverage.",
  "40:21": "I think a lot of the, kind of, employee organizing movements are very promising, and that can",
  "40:26": "be useful, but really trying to, kind of, vet the, the ethics of the company you're",
  "40:32": "joining, and also being willing to walk away if you, if, if you're able to do so.",
  "40:41": "That's a great, great question.",
  "40:44": "So what this, this example of representation bias here, the, kind of, way to address this",
  "40:49": "is to build a more representative data set.",
  "40:52": "It's very important to keep consent in mind of the, the, people, of, if you're using pictures",
  "40:56": "of people, but Joy Buolamwini and Timnit Gebru did this, as part of, as part of \u2018Gender",
  "41:02": "Shades\u2019.",
  "41:03": "However, this is a...",
  "41:05": "The fact that this was a problem not just for one company, but basically kind of every",
  "41:09": "company they looked at, was due to this underlying problem, which is that in machine learning",
  "41:16": "bench, benchmark datasets spur on a lot of research, however, kind of, several years",
  "41:22": "ago all the, kind of, popular facial datasets were primarily of light skinned men.",
  "41:29": "For instance.",
  "41:30": "IJB-A, a kind of popular face dataset several years ago, only 4% of the images were of dark-skinned",
  "41:36": "women.",
  "41:37": "Yes?",
  "41:38": "Question: \u2018I've been worried about COVID-19 contact tracing, and the erosion of privacy",
  "41:47": "location tracking, private surveillance Companies, etc.",
  "41:50": "What can we do to protect our digital rights post COVID.",
  "41:53": "Can we look to any examples in history of what to expect?\u2019",
  "41:58": "That is\u2026",
  "41:59": "That is a huge question, and something I have been thinking about as well.",
  "42:03": "I am, I'm gonna put that off till later to talk about, and that is something where in",
  "42:09": "the course I teach, I have an entire unit on privacy and surveillance, which I do not",
  "42:13": "in tonight's lecture, but I can share some materials, although I am already really, even",
  "42:18": "just like, rethinking how I'm gonna teach privacy and surveillance in the age of COVID-19",
  "42:23": "compared to two months ago, when I taught it the first time, but that is something I",
  "42:29": "think about a lot, and I will talk about later if we have time or, or on the forums, if we,",
  "42:35": "if we don't.",
  "42:36": "That's a great question.",
  "42:37": "A very important question...",
  "42:40": "On the topic.",
  "42:41": "And, and I will say, and I have not had the time to look into them yet, I do know that",
  "42:45": "there are groups that are working on what are kind of more privacy protecting approaches",
  "42:50": "for, for tracking and they're also groups putting out, like if we are going to use some",
  "42:55": "sort of tracking, what are the safeguards that need to be in place to do it responsibly.",
  "43:00": "Yes?",
  "43:01": "I've been looking at that too.",
  "43:02": "It does seem like this is a solvable problem with, with technology.",
  "43:05": "Not all of these problems are, but you can certainly store tracking history on somebody's",
  "43:11": "cell phone, and then you could have something where you say when you've been infected, and",
  "43:20": "at that point you could tell people that they've been infected by sharing the location, in",
  "43:29": "a privacy preserving way.",
  "43:30": "I think some people are trying to work on that.",
  "43:32": "I'm not sure it's actually technically a problem.",
  "43:34": "So I think there are, sometimes, there are ways to provide the minimum kind of level,",
  "43:40": "you know, kind of application with, with, you know, whilst keeping privacy.",
  "43:46": "Yeah, and then I think it's very important to also have things of, you know, clear like",
  "43:52": "expiration date, like we, you know, like looking back at 9/11 in the United States that, kind",
  "43:57": "of, ushered in all these laws, that were now, kind of stuck with, that have really eroded",
  "44:01": "privacy.",
  "44:02": "Of anything we do around COVID-19, being very clear, we are just doing this for COVID-19,",
  "44:07": "and then there's a time limit, and expires, and it's kind of for this clear purpose.",
  "44:13": "And they're also issues though of, you know, I mentioned earlier about data containing",
  "44:16": "errors, you know, this has already been an issue, and some other countries that we're",
  "44:21": "doing, kind of, more surveillance focused approaches of, you know, what about like when",
  "44:27": "it's wrong and people are getting kind of quarantined, and they don't even know why,",
  "44:31": "and for no reason, and so to be mindful of those.",
  "44:34": "But yeah, we\u2019ll, we'll talk more about this, kind of, later on.",
  "44:38": "Back to...",
  "44:39": "Back to bias...",
  "44:41": "Yeah, we had kind of the, the benchmarks.",
  "44:44": "So, when the benchmark that's, you know, widely used, has bias, then that is really, kind",
  "44:50": "of, replicated at scale, and we're seeing this with ImageNet as well, which is, you",
  "44:55": "know, probably the most widely studied computer vision dataset out there.",
  "44:59": "Two-thirds of the ImageNet images are from the West.",
  "45:02": "So this pie chart shows that the 45% of the images in ImageNet are from the United States,",
  "45:10": "7% from Great Britain, 6% from Italy, 3% from Canada, 3% from Australia, you know, and we're",
  "45:17": "covering a lot of, a lot of this pie without having, having gotten to outside the West,",
  "45:24": "and so then this has shown up in concrete ways of classifiers trained on ImageNet.",
  "45:29": "So one of the categories is bridegroom, a man getting married, there are a lot of, you",
  "45:34": "know, cultural components to that, and so they have, you know, much higher error rates",
  "45:38": "on, on bridegrooms from, from the Middle East, or from the Global South.",
  "45:46": "And there are, there are people now, kind of, working to diversify these datasets, but",
  "45:50": "it is quite dangerous, that they can really be, kind of, widely built on its scale, or",
  "45:54": "have been widely built on its scale before these biases were recognized.",
  "46:00": "Another key study is the COMPAS Recidivism Algorithm, which is used in determining who,",
  "46:08": "who has to pay bail, so in the U.S a very large number of people are in prison, who",
  "46:14": "have not even had a trial yet just, because they're too poor to afford bail, as well as",
  "46:18": "sentencing decisions and parole decisions.",
  "46:22": "And ProPublica, I did a famous investigation in 2016 that I imagine many of you have heard",
  "46:26": "of, in which they found that the false positive rate for black defendants was nearly twice",
  "46:33": "as high as for white defendants.",
  "46:35": "So black defendants who were\u2026",
  "46:37": "A study from Dartmouth found that it was, the software is no more accurate than Amazon",
  "46:43": "Mechanical Turk workers.",
  "46:44": "So random people on the Internet.",
  "46:47": "It's also the software is, you know, this proprietary black box using over 130 inputs,",
  "46:52": "and it's no more accurate than a linear classifier on three variables.",
  "46:57": "Yet it's still in use, and it's in use in many states, Wisconsin as one place where",
  "47:03": "it was challenged, yet the Wisconsin Supreme Court upheld its use.",
  "47:07": "If you're interested in the, kind of, topic of how you define fairness, because there",
  "47:12": "is a lot of intricacy here, and I mean, I don't know anybody working on this who thinks",
  "47:17": "that what COMPAS is doing is, is right, but they're using this different, different definition",
  "47:22": "of fairness.",
  "47:23": "Arvind Narayanan has a fantastic tutorial \u2018...21 fairness definitions and their politics\u2019,",
  "47:28": "that I, that I, highly recommend.",
  "47:34": "And so, going back to kind of this taxonomy of types of bias, this is an example of historical",
  "47:40": "bias, and historical bias is a fundamental structural issue, with the first step of the",
  "47:45": "data generation process, and it can exist even given perfect sampling and feature selection.",
  "47:50": "So, kind of, with the, the, image classifier that was something where we could, you know,",
  "47:54": "go gather a more representative set of images, and that would help address it, that is not",
  "47:58": "the case here.",
  "47:59": "So gathering kind of more data on the U.S. criminal justice system it's all going to",
  "48:07": "be biased, because that's really, kind of, baked into, baked into, our history and our",
  "48:10": "current state.",
  "48:12": "And so this is I think good, good to recognize.",
  "48:15": "One thing that can be done to, try to at least, mitigate this is to, to really talk to domain",
  "48:23": "experts, and by the people impacted, and so a really positive example of this is a tutorial",
  "48:32": "from the Fairness, Accountability, and Transparency Conference that Kristian Lum who's the Lead",
  "48:37": "Statistician for the Human Rights Data Analysis Group, and now professor UPenn, organized",
  "48:42": "together with a former Public Defender, Elizabeth Bender, who's the staff attorney for New York's",
  "48:49": "Legal Aid Society, and Terrence Wilkerson, an innocent man who was arrested, and cannot",
  "48:53": "afford bail.",
  "48:55": "And Elizabeth and Terrence, were able to provide a lot of insight to how the criminal justice",
  "48:59": "system works in practice which is often, kind of, very different from the, you know, more,",
  "49:04": "kind of, clean, logical abstractions that computer scientists deal with, but it's really",
  "49:10": "important to understand those kind of intricacies of how this is going to be implemented, and",
  "49:14": "used in these, you know, messy, complicated real world systems.",
  "49:19": "Question?",
  "49:21": "\u2018Aren\u2019t the AI biases transferred from real life biases.",
  "49:26": "For instance, aren\u2019t people being treated differently isn't everyday phenomenon too.\u2019",
  "49:34": "That's correct, yes.",
  "49:35": "So this is often, yeah, coming from, from real-world biases, and I'll come to this in",
  "49:41": "a moment, but algorithmic systems can amplify those biases, so they can make them even worse,",
  "49:47": "but yeah, they are often being learned from, from existing data.",
  "49:51": "I\u2026",
  "49:52": "I asked it because, I guess, I often see this being raised as it's, kind of, a reason not",
  "49:58": "to worry about AI.",
  "49:59": "So it\u2019s not AI.",
  "50:00": "Well, I'm gonna get to that in a moment.",
  "50:04": "Actually, think in two slides.",
  "50:06": "So hold on to that question.",
  "50:10": "I just wanna talk about one other type of bias first.",
  "50:12": "Measurement bias.",
  "50:13": "So this was an interesting paper by Sendhil Mullainathan and Ziad Obermeyer, where they",
  "50:18": "looked at historic electronic health record data to try to determine what factors are",
  "50:24": "most predictive of stroke and they said, you know, this could be useful like prioritizing",
  "50:27": "patients at the ER.",
  "50:30": "And so they found that the number one most predictive factor was prior stroke, which,",
  "50:34": "that makes sense.",
  "50:36": "Second was, cardiovascular disease, that's also, that seems reasonable, and then third",
  "50:41": "most, kind of, still very predictive factor was accidental injury, followed by having",
  "50:47": "a benign breast lump, a colonoscopy, or sinusitis.",
  "50:51": "And so, I'm not a medical doctor, but I can tell something weird is going on with factors",
  "50:56": "three through six here.",
  "50:58": "Like, why would these things be predictive of, of stroke.",
  "51:01": "Does anyone want to think about, about why this might be?",
  "51:07": "Any guesses you want to read?",
  "51:14": "Oh someone's yeah.",
  "51:16": "Okay, the first answer was they test for it any time someone has stroke?",
  "51:26": "confirmation bias?",
  "51:28": "overfitting? is because they happen to be in hospital already?",
  "51:33": "Biased data?",
  "51:34": "EHR records these events?",
  "51:39": "Because the data was taken before certain advances in medical science?",
  "51:42": "These are, these are all good guesses.",
  "51:44": "Not, not quite what I was looking for, but good good thinking.",
  "51:48": "That's such a nice way of saying no.",
  "51:51": "So what, what the researchers say here is that this was about their patients, they are",
  "51:58": "people that utilize health care a lot and people that don't and they call it, kind of,",
  "52:01": "High Utility versus Low Utility Of Healthcare and there are a lot of factors that go into",
  "52:05": "this, I'm sure just who has health insurance and who can afford their co-pays, there may",
  "52:09": "be cultural factors, there may be racial and gender bias, there is racial and gender bias",
  "52:14": "on how people are treated.",
  "52:16": "So a lot of factors, and basically people that utilize health care a lot they will go",
  "52:22": "to a doctor when they have sinusitis and they will also go in when they're having a stroke",
  "52:26": "and people that do not utilize health care much are probably not going to go in possibly",
  "52:29": "for either and so, so what the authors write is that we haven't measured stroke which is,",
  "52:35": "you know, a region of the brain being denied, kind of, new blood and new oxygen; what we've",
  "52:40": "measured is: who had symptoms, who went to the doctor, received tests and then got this",
  "52:44": "diagnosis of stroke and, you know, that seems like it might be a reasonable proxy for, for",
  "52:49": "who had a stroke but a proxy is you know never exactly what you wanted and in many cases",
  "52:55": "that, that gap ends up being significant and so this is just one form that, that measurement",
  "52:59": "bias can take but I think it's something to really, kind of, be on the lookout for because",
  "53:03": "it can be quite subtle.",
  "53:05": "And so now starting to return to a point that was brought up earlier, aren't, aren't people",
  "53:11": "biased?",
  "53:12": "Yes.",
  "53:13": "Yes, we are and so there have been dozens and dozens, if not hundreds of studies on",
  "53:18": "this, but I'm just going to quote a few, all of which are linked to in this Sendhil Mullainathan",
  "53:23": "New York Times article if you want to find, find the studies, so this all comes from,",
  "53:28": "you know, peer-reviewed research.",
  "53:29": "But when doctors were shown identical files they were much less likely to recommend a",
  "53:34": "helpful cardiac procedure to black patients compared to white patients, and so that was",
  "53:38": "you know, same file, but just changing the race of the patient.",
  "53:42": "When bargaining for a used car, black people were offered initial prices $700 higher and",
  "53:47": "received fewer concessions.",
  "53:50": "Responding to apartment rental ads on Craigslist with a black name elicited fewer responses",
  "53:54": "than with a white name.",
  "53:55": "An all-white jury was 16 points more likely to convict a black defendant than a white",
  "54:00": "one, but when a jury had just one black member it convicted both at the same rate.",
  "54:04": "And so I share these to show that kind of no matter what type of data you're looking,",
  "54:09": "working on, whether that is Medical data or sales data or housing data or criminal justice",
  "54:14": "data that it's very likely that there's, there's bias in it.",
  "54:18": "There's a question: No, I was gonna say I find that last one really interesting, like",
  "54:22": "this kind of idea that a single black member of the jury, I guess it has some kind of like",
  "54:28": "anchoring impact, like it kind of suggests that, I'm sure you're going to talk about",
  "54:31": "diversity later, but I just want to keep this in mind that maybe even a tiny bit of diversity",
  "54:37": "here just reminds people that there's a, you know, a range of different types of people",
  "54:41": "and perspectives.",
  "54:42": "No, that's it, that's a great point yeah.",
  "54:49": "And so the question that was, kind of, asked earlier is so why does algorithmic bias matter?",
  "54:53": "Like, I have just shown you that humans are really biased too - so why are, why are we",
  "54:59": "talking about algorithmic bias?",
  "55:00": "And people have brought this up, kind of, like what's what's the fuss about it?",
  "55:04": "And there, I think algorithmic bias is a very significant, worth talking about and I'm going",
  "55:10": "to share four reasons for that.",
  "55:13": "One is the machine learning can amplify bias.",
  "55:16": "So it's not just encoding existing biases but in some cases it's making them worse and",
  "55:20": "there have been a few studies on this, one I like is from Maria De-Arteaga of CMU and",
  "55:28": "here they were, they took people's, I think job descriptions from LinkedIn, and what they",
  "55:33": "found is that Imbalances ended up being compounded and so in the group of surgeons, only 14%",
  "55:40": "were women however in the true positives, so they were trying to predict the, the job",
  "55:45": "title from the summary, women were only 11% in the true positives.",
  "55:52": "So this kind of imbalance has gotten worse.",
  "55:54": "And basically there was, kind of, this asymmetry where the, you know, the algorithm has learned",
  "55:58": "it's safer for, for women to, kind of, not, not guess surgeon.",
  "56:05": "Another, so this is one reason, another reason that algorithmic bias is a concern is that",
  "56:15": "algorithms are used very differently than human decision-makers in practice and so people",
  "56:19": "sometimes talk about them as though they are plug-and-play and are interchangeable of,",
  "56:22": "you know, with humans this bias and the algorithm is you know, this bias, why don't we just",
  "56:27": "substitute it in?",
  "56:28": "However, the, the whole system around it ends up, kind of, being different in practice.",
  "56:33": "One...",
  "56:34": "One, kind of, aspect of this is people are more likely to assume algorithms are objective",
  "56:39": "or error-free, even if they're given the option of a human override, and so if you give a",
  "56:45": "person, you know, even if you just say hey, I'm just giving the judge this recommendation,",
  "56:48": "they don't have to follow it, if it's coming from a computer many people are gonna take",
  "56:52": "that as objective.",
  "56:54": "In some cases also, there may be, you know, pressure from their boss to, you know, not",
  "56:58": "disagree with the computer more times, you know, nobody's gonna get fired by going with",
  "57:02": "the computer recommendation.",
  "57:06": "Algorithms are more likely to be implemented with no appeals process in place, and so we",
  "57:10": "saw that earlier when we were talking about recourse.",
  "57:14": "Algorithms are often used at scale.",
  "57:16": "They can be replicating an identical bias at scale.",
  "57:20": "And algorithmic systems are cheap.",
  "57:21": "And all of these, I think, are interconnected.",
  "57:23": "So in many cases, I think that algorithmic systems are being implemented, not because",
  "57:29": "they produce better outcomes for everyone, but because they're, kind of, a cheaper way",
  "57:33": "to do things at scale, you know, offering a recourse process is more expensive.",
  "57:38": "Being on the lookout for errors is more expensive.",
  "57:40": "So this is kind of cost-cutting measures, and Cathy O'Neil talks about many of these",
  "57:44": "themes in her book, \u2018Weapons of Mass Destruction\u2019, kind of, under the idea that the, \u2018The privileged",
  "57:50": "are processed by people; the poor are processed by algorithms.\u2019",
  "57:53": "There's a question?",
  "57:54": "Two questions.",
  "57:55": "Hmm.",
  "57:56": "\u2018This seems like an intensely deep topic, needing specialized expertise to avoid getting",
  "58:00": "it wrong.",
  "58:01": "If you were building an ML product, would you approach an academic institution for consultation",
  "58:06": "on this?",
  "58:07": "Do you see a data, product, development triad becoming a quartet, involving an ethics or",
  "58:15": "data privacy expert?\u2019",
  "58:17": "Yes.",
  "58:18": "So I think interdisciplinary work is very important.",
  "58:21": "I would...",
  "58:23": "I would definitely focus on trying to find, kind of, domain experts on whatever your particular",
  "58:27": "domain is, who understand the intricacies of that domain, is important.",
  "58:32": "And, I think, with the, kind of, with the academic it depends.",
  "58:37": "You do want to make sure you get someone who is, kind of, applied enough to, kind of, understand",
  "58:41": "how, how, things are happening in, in, in industry.",
  "58:44": "But yeah, I think involving more people, and people from more fields is, is a good, a good",
  "58:49": "approach on the whole.",
  "58:50": "\u2018Someone invents and publishes a better ML technique, like attention or transformers,",
  "58:55": "and then next a graduate student demonstrates, using it to improve facial recognition by",
  "59:00": "5%, and then a small start-up publishes an app that does better facial recognition, and",
  "59:04": "then a government uses the app to study downtown walking patterns in endangered species, and",
  "59:08": "after these successes, for court-ordered monitoring, and then a repressive government then takes",
  "59:10": "that method to identify ethnicities, and then you get a genocide.",
  "59:15": "No one's made a huge ethical error at any incremental step, yet the result is horrific.",
  "59:22": "I have no doubt that Amazon will soon serve up a personally customized price for each",
  "59:26": "item that maximizes their profits.",
  "59:27": "How can such ethical creep be addressed, where the effect is remote for many small causes?\u2019",
  "59:35": "This all...",
  "59:36": "Yeah, so that, that's a, kind of a, great summary of how, yeah, these things can happen",
  "59:42": "somewhat incrementally.",
  "59:43": "I'll talk about some tools to implement, kind of, towards the end of this lesson, that hopefully",
  "59:49": "can help us.",
  "59:50": "So some of it is, I think, we do need to get better at, kind of, trying to think a few",
  "59:53": "more steps ahead, than we have been.",
  "59:57": "You know, in particular we've seen examples of people, you know, there was this study",
  "60:01": "of how do I identify protesters in a crowd, even when they had scarves, or sunglasses,",
  "60:05": "or hats on.",
  "60:06": "You know, and when the, the, researchers on that were questioned, they were like, \u2018Oh",
  "60:09": "it never even occurred to us that bad guys would use this, you know, we just thought",
  "60:12": "it would be for finding bad people\u2019.",
  "60:16": "And so I do think, kind of, everyone should be building their ability to think a few more",
  "60:23": "steps ahead, and part of this is like it's great to do this in teams, preferably in diverse",
  "60:27": "teams, can help with that, that process.",
  "60:31": "I mean, on this question of computer vision there has been, you know, just in the last",
  "60:34": "few months, is it Joe Redmon, creator of YOLO, who has said that he's no longer working on",
  "60:42": "computer vision just because he thinks the, the misuses so far outweigh the the positives,",
  "60:49": "and Timnit Gebru said she's, she's considering that as well.",
  "60:52": "So, I think, there are, there are times where you have to consider...",
  "60:58": "And then, I think, also really actively thinking about how to, what safeguards do we need to",
  "61:02": "put in place to, kind of, address the, the misuses that are happening.",
  "61:05": "Yes?",
  "61:06": "I just wanted to say somebody really liked the Cathy O'Neil quote: \u2018Privileged are",
  "61:12": "processed by people; the poor processed by algorithms\u2019 and they're looking forward",
  "61:16": "to learning more, reading more from Kathy O'Neal.",
  "61:18": "Is there a book that you would recommend?",
  "61:19": "Yes.",
  "61:20": "Yeah.",
  "61:21": "And in\u2026",
  "61:22": "And Kathy O'Neal also writes in the\u2026",
  "61:24": "And Kathy O'Neill's a fellow, fellow math PhD, but she also has written a number of",
  "61:30": "good articles.",
  "61:31": "And it...",
  "61:33": "The book, kind of, goes through a number of those case studies of how algorithms are being",
  "61:36": "used in different places.",
  "61:39": "So, kind of in...",
  "61:43": "In summary of \u2018Humans are biased, why do, why are we making a fuss about algorithmic",
  "61:49": "bias?\u2019",
  "61:50": "So, one is we saw earlier.",
  "61:51": "Machine learning can create feedback loops.",
  "61:53": "So it's, you know, it's not just, kind of, observing what's happening in the world, but",
  "61:57": "it's also determining outcomes, and it's, kind of, determining what future data is.",
  "62:01": "Machine learning can amplify bias.",
  "62:04": "Algorithms and humans are used very differently in practice, and then I\u2019ll also say technology",
  "62:08": "is power, and with that comes responsibility, and I think for, for all of us to, to have",
  "62:13": "access to deep learning, we're still in a, kind of, very fortunate and small percentage",
  "62:17": "of the world, that is able to use this technology right now, and I hope, I hope we will all",
  "62:23": "use it responsibly, and really take our power seriously.",
  "62:26": "And I just, I just noticed the time, and I think we're about to start next section on,",
  "62:35": "on analyzing or, kind of, steps, steps we can take, so this would be a good, a good,",
  "62:41": "place to take a break.",
  "62:42": "So let's meet back in seven minutes, at 7:45.",
  "62:48": "All right, let's start back up, and actually I was at a slightly different place than I",
  "62:54": "thought, but just a few questions that, that, you can ask about projects you're working",
  "63:01": "on, and I, I hope you will ask about projects you're working on.",
  "63:05": "The first is, should we, \u2018should we even be doing this?\u2019, and considering that maybe",
  "63:11": "there's some work that we shouldn't do.",
  "63:14": "There's a paper \u2018When the Implication Is Not to Design (Technology)\u2019.",
  "63:19": "As engineers we often tend to respond to problems with, you know, \u2018What can I make or build",
  "63:23": "to address this?\u2019, but sometimes the answer is to not make or build anything.",
  "63:30": "One example of research that I think has a huge amount of downside, and really no upside",
  "63:35": "I see was, kind of, to identify the ethnicity, particularly for people of ethnic minorities.",
  "63:42": "And so there was work done identifying the Chinese Uyghurs, which is the Muslim minority",
  "63:47": "in Western China, which has since, you know, over a million people have been placed in",
  "63:51": "internment camps.",
  "63:52": "And I think this is a very, very harmful, harmful line of research.",
  "63:59": "I think that the, you know, there have been at least two attempts of, building, building",
  "64:03": "a classifier to try to identify someone's sexuality, which is, it's probably, just picking",
  "64:12": "up on kind of stylistic differences, but this is something that a, could also be quite,",
  "64:16": "quite dangerous, as in many countries it's, it's illegal to be gay.",
  "64:22": "Yes.",
  "64:23": "So this is a question for me, which I don't know the answer to.",
  "64:27": "Yeah.",
  "64:28": "As that title says a Stanford scientist says he built the gaydar using \u2018the lamest\u2019",
  "64:33": "AI possible to prove a point.",
  "64:34": "And my understanding is, that point was to say, you know, I guess it's something like",
  "64:39": "\u2018Hey, you could use fast AI Lesson 1.",
  "64:41": "After an hour or two you can build this thing.",
  "64:45": "Anybody can do it.",
  "64:47": "You know, how do you feel about this idea that there's a role to demonstrate what's",
  "64:52": "readily available with the technology we have?",
  "64:54": "Yeah, I mean, that's something that I think...",
  "64:57": "So I appreciate that, and I'll talk about this a little bit later, OpenAI with GPT-2,",
  "65:05": "I think, was trying to raise a, raise a, debate around, around dual use and what is responsible",
  "65:11": "release of, of dual use technology, and what's a, kind of, responsible way to raise, raise",
  "65:20": "awareness of what is possible.",
  "65:22": "In the, in the cases of researchers that have done this on the sexuality question, to me",
  "65:28": "it hasn't seemed like they've put adequate thought into, how they're conducting that,",
  "65:33": "and who they're collaborating with, to ensure that it is something that is leading to, kind",
  "65:40": "of, helping address the problem, but I think you're right that, I think, there is probably",
  "65:47": "some place for letting people, yeah, know what is probably widely available now.",
  "65:51": "It reminds me a bit of my pen testing in infosec\u2026",
  "65:53": "Yeah.",
  "65:54": "...where, where it's, kind of, considered...",
  "65:58": "Well, there's an ethical way that you can go about pointing out that it's trivially",
  "66:01": "easy to break into a company\u2019s system.",
  "66:04": "Yes.",
  "66:05": "Yeah.",
  "66:06": "Yes.",
  "66:07": "Yeah, I would, I would agree with that, that there, there is an ethical way, but I think",
  "66:08": "that's something that we as a community, still have more work to do in even determining what",
  "66:14": "that is.",
  "66:16": "Other questions to consider are what bias is in the data and something I should highlight",
  "66:21": "is, people often ask me, you know, how can I debias my data, or ensure that its bias",
  "66:27": "free, and that's not possible.",
  "66:29": "All data contains bias, and the, kind of, most, most important thing is just to understand,",
  "66:36": "kind of, how your data set was created, and what its limitations are, so that you're not",
  "66:39": "blindsided by that bias, but you're never going to fully remove it.",
  "66:43": "And some of the, I think, most promising approaches in this area are work like, Timnit Gebru's",
  "66:50": "\u2018Datasheets for Datasets\u2019, which is, kind of, going through and asking, kind of, a bunch",
  "66:54": "of questions about how your dataset was created, and for what purposes, and how it's being",
  "66:58": "maintained, and you know what are the risks in that.",
  "67:01": "Just to really kind of be aware of, of the context of your data.",
  "67:07": "Can the code and data be audited?",
  "67:09": "I think, particularly in the United States, we have a lot of issues with when private",
  "67:13": "companies are creating software that's really impacting people through the criminal justice",
  "67:19": "system, or hiring and when these things are, you know, kind of their proprietary black",
  "67:24": "boxes that are protected in court.",
  "67:26": "That, this creates a lot of kind of of issues of you know, what are what are our rights",
  "67:31": "around that?",
  "67:33": "Looking at error rates for different subgroups is really important and that's what so kind",
  "67:37": "of so powerful about Joy Buolamwini\u2019s work.",
  "67:40": "If she had just looked at light-skinned versus dark skin and men versus women, she wouldn't",
  "67:45": "have identified just how poorly the algorithms were doing on dark-skinned women.",
  "67:52": "What is the accuracy of a simple rule-based alternative?",
  "67:55": "And this is something I think Jeremy talked about last week, which is just kind of good,",
  "67:59": "good machine learning practice, to have a baseline.",
  "68:03": "But particularly in cases like the COMPAS Recidivism, where this 130-variable black",
  "68:08": "box is not doing much better than a linear classifier on three variables.",
  "68:13": "That raises kind of a question of why are, why are we using this?",
  "68:16": "And then what processes are in place to handle appeals or mistakes, because there will be",
  "68:22": "errors in the data.",
  "68:23": "There may be bugs and the implementation, and we need to have a process for recourse.",
  "68:28": "Yes.",
  "68:29": "Can you explain this for me now?",
  "68:33": "Sorry, I\u2019m asking my own questions, nobody voted them up at all.",
  "68:38": "What's the thinking behind this idea that a simpler model, is it that you gottta say",
  "68:43": "that a simpler model, all other things being the same, you should pick the simpler one?",
  "68:47": "Is that what this baseline\u2019s for?",
  "68:48": "And if so, what's the kind of thinking behind that?",
  "68:52": "Well, with the COMPAS Recidivism Algorithm ... Some of this for me is linked to the proprietary",
  "69:01": "black box nature, and so you're right.",
  "69:03": "Maybe if we had a way to introspect and what were our rights around appealing something.",
  "69:07": "But I would say, yeah, like why use the more complex thing if the, the simpler one works",
  "69:11": "the same.",
  "69:15": "And then how diverse is the team that built it and I'll talk more about team diversity",
  "69:18": "later, later in this lesson.",
  "69:21": "Okay, it was Jeremy at the start, but I'm not the teacher.",
  "69:26": "So it actually is, \u201cJeremy, Do you think transfer learning makes this tougher, auditing",
  "69:31": "the data that led to the initial model?\u201d",
  "69:34": "I assume they mean \u201cJeremy, please ask Rachel.\u201d",
  "69:36": "No, they were, they were asking you.",
  "69:40": "That's, that's a good question.",
  "69:44": "Again, I think it's important.",
  "69:47": "I would, I would say I think it's important to have information probably on both datasets,",
  "69:52": "what the initial data set used was and what the the data set you used to fine-tune it.",
  "69:58": "Do you have thoughts on that?",
  "70:01": "What she said.",
  "70:04": "And then I'll say so while bias and fairness as well as accountability and transparency",
  "70:11": "are important, they aren't everything.",
  "70:14": "And so there's this great paper, \u201cA Mulching Proposal\u201d by Os Keyes, et al.",
  "70:19": "And here they talk about a system for turning the elderly into high nutrient slurry, so",
  "70:25": "this is something that it's clearly unethical but they proposed a way to do it that is fair",
  "70:30": "and accountable and transparent and meets these qualifications.",
  "70:34": "And so that kind of shows that some of the limitations of this framework, as well as",
  "70:39": "kind of being a good, a good technique for kind of inspecting whatever framework you",
  "70:46": "are using, of trying to find something that's clearly unethical that code that can meet,",
  "70:51": "meet the standards you've put forth.",
  "70:53": "That, that technique, I really like it.",
  "70:56": "It's like, it's my favorite technique from philosophy.",
  "71:00": "It's this idea that you, you say, okay, given this premise, here's what it implies.",
  "71:07": "And then you try and find an implied result which intuitively it is clearly saying.",
  "71:12": "And it's a really, it's, it's yeah, it's the number one philosophical thinking tool I got",
  "71:19": "out of university.",
  "71:20": "And sometimes we can have a lot of fun with it, like this time, too.",
  "71:26": "Thank you.",
  "71:27": "All right, so the next kind of big case study, your topic I want to discuss is disinformation.",
  "71:35": "So in 2016, in Houston a group called Heart of Texas posted about protests outside an",
  "71:46": "Islamic Center.",
  "71:48": "And they told people to come armed.",
  "71:50": "Another Facebook group posted about a counter-protest to show up supporting freedom of religion",
  "71:57": "and inclusivity.",
  "72:00": "And so there were kind of a lot of people present",
  "72:04": "at this, more people on the the side supporting freedom of religion.",
  "72:08": "And a reporter, though, for the Houston Chronicle noticed something odd -- which he was not",
  "72:13": "able to get in touch with the organizers for either side.",
  "72:16": "And It came out many months later that both sides had been organized by Russian trolls.",
  "72:22": "And so this is something where you had the people protesting were, you know, genuine",
  "72:28": "Americans kind of protesting their beliefs, but they were doing it in this way that had",
  "72:33": "been kind of completely framed very disingenuously by, by Russian operatives.",
  "72:41": "And, so when thinking about disinformation, it is not, people often think about so-called",
  "72:50": "fake news, you know and inspecting like a single post -- is this, you know, Is this",
  "72:54": "true or false?",
  "72:56": "But really disinformation is often about orchestrated campaigns of manipulation and that it involves,",
  "73:02": "kind of, all the seeds of truth, kind of the best propaganda always involves kernels of",
  "73:09": "truth, at least.",
  "73:11": "It also involves kind of misleading context and, and can, can involve very kind of sincere,",
  "73:16": "sincere people that get, get swept in it.",
  "73:21": "A report came out this fall, an investigation from Stanford's Internet Observatory, where",
  "73:28": "Renee DiResta and Alex Stamos work, of Russia's kind of most recent disinformation, or most",
  "73:36": "recently identified disinformation campaign.",
  "73:39": "And it was operating in six different countries in Africa.",
  "73:43": "It often purported to be local news sources.",
  "73:46": "It was multi-platform.",
  "73:48": "They were encouraging people to join their whatsapp and telegram groups and they were",
  "73:53": "hiring local people as reporters and a lot of, a lot of the content was not, not necessarily",
  "73:59": "disinformation.",
  "74:00": "It was stuff on culture and sports and local weather.",
  "74:03": "I mean there was a lot of kind of very pro-Russia coverage.",
  "74:09": "But then it covered a range of topics and so this is kind of a very sophisticated phase",
  "74:14": "of disinformation.",
  "74:15": "And in many cases it was hiring, hiring locals kind of as reporters to work for these sites.",
  "74:23": "And I should say well I've just given two examples of Russia.",
  "74:26": "Russia - certainly does not have a monopoly on disinformation.",
  "74:28": "There are plenty of, plenty of people involved and producing it.",
  "74:35": "Kind of on a topical topical Issue, there's been a lot of disinformation around around",
  "74:43": "Corona virus and Covid 19.",
  "74:47": "I, in terms of kind of a personal level, if you're looking for advice on spotting disinformation",
  "74:52": "or to share with loved ones about this, Mike Caulfield is a great person to follow.",
  "74:59": "\u2026 and he's even.",
  "75:00": "So he tweets @holden and then he has started an infodemic blog specifically about the about",
  "75:05": "Covid 19, but he, he talks about his approach, and how people have been trained in schools",
  "75:11": "for 12 years - here's a text, read it, use your critical thinking skills to figure out",
  "75:16": "what you think about it.",
  "75:17": "But professional fact checkers do the opposite -- they get to a page and they immediately",
  "75:20": "get off of it and look for kind of higher, higher quality sources to see if they can",
  "75:25": "find confirmation.",
  "75:27": "And Caulfield also really promotes the idea of a lot of critical thinking techniques that",
  "75:33": "have been taught take a long time, and you know, we're not going to spend 30 minutes",
  "75:38": "evaluating each tweet that we see in our Twitter stream.",
  "75:40": "It's better to give people an approach that they can do in 30 seconds that, you know,",
  "75:45": "It's not gonna be fail proof If you're just doing something for 30 seconds.",
  "75:47": "But it's better to to check, than to have something that takes 30 minutes that you're",
  "75:51": "just not going to do at all.",
  "75:53": "So I wanted to kind of put this out there as a resource.",
  "75:55": "I mean as a whole kind of set of lessons at lessons.checkplease.cc.",
  "76:01": "And he's a, he's a professor.",
  "76:04": "And I, in the data ethics course I'm teaching right now.",
  "76:09": "I made my first lesson the first half of which is kind of specifically about Corona virus",
  "76:14": "disinformation.",
  "76:15": "I've made that available on YouTube.",
  "76:17": "I've already shared it.",
  "76:18": "And so I'll add a link on the forums, If you want if you want a lot more detail on, on",
  "76:24": "disinformation than just kind of this the short bit here.",
  "76:27": "But so going back to kind of like what is disinformation?",
  "76:32": "It's important to think of it as an ecosystem again, it's not just a single post or a single",
  "76:37": "news story that, you know, it's misleading or has false elements in it.",
  "76:42": "But it's this really this broader ecosystem Claire Wartell first draft news, who is a",
  "76:49": "Leading expert on this and does a lot around kind of training journalists and how journalists",
  "76:52": "can report responsibly, talks about the trumpet of amplification and this is where rumors",
  "76:58": "or Memes or things can start on 4chan and 8chan and then move to closed messaging groups",
  "77:06": "such as WhatsApp, Telegram, Facebook Messenger.",
  "77:10": "From there to conspiracy communities on Reddit or YouTube, then to kind of more mainstream",
  "77:16": "social media and then picked up by the professional media and politicians.",
  "77:20": "And so this can make it very hard to address.",
  "77:22": "that it is this kind of multi-platform in many cases campaigns may be utilizing kind",
  "77:28": "of the differing rules or loopholes between the different platforms.",
  "77:32": "And I think we certainly are seeing more and more examples where it doesn't have to go",
  "77:35": "through all these steps But can can jump jump forward- And online discussion is very, very",
  "77:45": "significant because it helps us form our opinions, and then this is tough because I think most",
  "77:52": "of us think of ourselves as pretty independent-minded, but discussion really does you know we evolved",
  "77:57": "as kind of social beings and to be influenced by by people in our in-group and in opposition",
  "78:02": "to people in our out-group and so online discussion impacts us.",
  "78:06": "People discuss all sorts of things online: here's a Reddit discussion about whether the",
  "78:10": "US should cut defense spending, and you have comments, you're wrong and the defense budget",
  "78:16": "is a good example of how badly the U.S. spends money on the military, and someone else says",
  "78:21": "yeah, but that's already happening, here's a huge increase in the military budget, the",
  "78:25": "Pentagon budgets already increasing.",
  "78:27": "I didn't mean to sound like, stop paying for the military, I'm not saying that we cannot",
  "78:32": "pay the bills, but I think it would make sense to cut defense spending.",
  "78:36": "Does anyone want to guess what subreddit this is from?",
  "78:42": "unpopularopinion, news, changemyview, netneutrality.",
  "78:51": "These are good guesses but they're wrong.",
  "78:59": "I love the way you say, though.",
  "79:02": "This is all from what it is.",
  "79:04": "It's from the sub simulatorgpt2 oh, so these comments are all written by GPT-2, and this",
  "79:11": "is in good fun.",
  "79:12": "It was clearly labeled on the subreddit, that it's coming in GPT-2 is a language model from",
  "79:19": "OpenAI.",
  "79:20": "That was kind of in a trajectory of research that many, many groups were on, and so it",
  "79:30": "was released I guess about a year ago and, should I read the unicorn story Jeremy?",
  "79:37": "Okay.",
  "79:38": "So many of you have probably have probably seen: this this was cherry-picked, but this",
  "79:44": "is still very, very impressive.",
  "79:46": "So human written prompt was given to the Language Model: \u201cin a shocking finding, scientists",
  "79:52": "discovered a herd of unicorns living in a remote previously unexplored valley in the",
  "79:56": "Andes mountains.",
  "79:58": "Even more surprising to the researchers was the fact that the unicorn spoke perfect English\u201d.",
  "80:03": "And then the next part is all generated by the language model.",
  "80:07": "So this is a deep learning model that produced this, and the computer model generated \u201cdr.",
  "80:13": "Jorge Perez found what appeared to be a natural fountain surrounded by two peaks of rock and",
  "80:18": "silver snow.",
  "80:19": "Perez and the others then ventured further into the valley.",
  "80:23": "By the time we reached the top of one peak, the water looked blue, with some crystals",
  "80:27": "on tops.",
  "80:28": "Perez and his friends were astonished to see the Unicorn herd.",
  "80:33": "These creatures could be seen from the air without having to move too much to see them.",
  "80:37": "They were so close, they could touch their horns.",
  "80:40": "While examining these bizarre creatures, the scientists discovered that the creatures also",
  "80:44": "spoke some fairly regular English.",
  "80:46": "Perez stated we can see for example that they have a common language, something like a dialect",
  "80:51": "or dialectic.\u201d",
  "80:52": "And so I think this is really compelling prose, to have been generated by a computer in this",
  "80:58": "form.",
  "81:00": "So we've also seen advances in computers generating pictures as specifically GANs.",
  "81:11": "So Katie Jones was listed on LinkedIn as a Russia and Eurasia fellow, she was connected",
  "81:17": "to several people from mainstream Washington think-tanks, and The Associated Press discovered",
  "81:23": "that she is not a real person.",
  "81:25": "This photo was generated by a GAN.",
  "81:28": "And so this, I think it's kind of scary, when we start thinking about how compelling the",
  "81:35": "text that's being generated is, and combining that with pictures, these photos are all from",
  "81:41": "thispersondoesnotexist.com, generated by GANs.",
  "81:46": "And there's a very, very real and imminent risk that online discussion will be swamped",
  "81:54": "with fake manipulative agents, to an even greater extent than it already has.",
  "82:00": "And this can be used to influence public opinion.",
  "82:04": "So.",
  "82:05": "oh, actually this is...well, I\u2019ll keep going.",
  "82:09": "So, going back in time to 2017, the FCC was considering repealing net neutrality",
  "82:17": "and so they opened up for comments to see, \u201cHow do Americans feel about net neutrality?\u201d",
  "82:22": "and this is a sample of many of the comments that were opposed to net neutrality.",
  "82:27": "They wanted to repeal it, and included...",
  "82:30": "I'll just read a few clips.",
  "82:31": "\u201cAmericans as opposed to Washington bureaucrats deserve to enjoy the services they desire.\u201d",
  "82:37": "\u201cIndividual citizens as opposed to Washington bureaucrats should be able to select whichever",
  "82:42": "services they desire.\u201d",
  "82:43": "\u201cPeople like me as opposed to so-called experts should be free to buy whatever products",
  "82:48": "they choose.\u201d",
  "82:49": "And these have been helpfully color-coded so you can see a pattern: that this was a",
  "82:53": "bit of a Mad Libs, where you had a few choices (for green) for the first noun, and then in",
  "83:02": "orange or red--I guess--it's \u201cas opposed to\u201d or \u201crather than.\u201d",
  "83:07": "Orange: we've got either \u201cWashington bureaucrats,\u201d \u201cso-called experts,\u201d \u201cthe FCC,\u201d and",
  "83:13": "so on.",
  "83:15": "This analysis was done by Jeff Kao who's now a computational journalist at ProPublica doing",
  "83:22": "great work, and he did this analysis discovering this campaign, in which these comments were",
  "83:32": "designed to look unique but had been created through some mail-merge-style, kind of putting",
  "83:39": "together, Mad Libs.",
  "83:43": "Yes?",
  "83:45": "So this was great work by Jeff.",
  "83:48": "He found that..",
  "83:50": "So while the FCC received over 22 million comments, less than 4% of them were truly",
  "83:57": "unique.",
  "83:58": "This is not all malicious activity, there are many ways where you get a template to",
  "84:04": "contact your legislator about something.",
  "84:07": "But in the example shown previously, these were designed to look like they were unique",
  "84:12": "when they weren't.",
  "84:16": "More than 99% of the truly unique comments wanted to keep net neutrality; however, that",
  "84:21": "was not not the case if you looked at the full 22 million comments.",
  "84:26": "However, this was in 2017, which may not sound that long ago, but in in the field of natural",
  "84:32": "language processing we've had an entire revolution since then--there's just been so much progress",
  "84:38": "made.",
  "84:39": "And this would be (I think) virtually impossible to catch today, if someone was using a sophisticated",
  "84:45": "language model to generate comments.",
  "84:48": "So Jess asks a question, which I'm gonna treat as a two-part question I think it's not necessarily.",
  "84:53": "What happens when there's so much AI trolling that most of what gets straight from the web",
  "84:59": "is AI generated text?",
  "85:01": "And then the second part: And then what happens when you use that to generate more AI generated",
  "85:05": "text?",
  "85:06": "Yes, for the first part...",
  "85:08": "Yeah, this is a real risk, or not \u201crisk,\u201d but kind of challenge we're facing of real",
  "85:14": "humans can get drowned out when so much text is gonna be AI trolling.",
  "85:23": "We're already seeing, and I (in the interest of time I can talk about disinformation for",
  "85:29": "hours and I had to cut a lot of stuff out) but many people have talked about how the",
  "85:35": "new form of censorship is about drowning people out.",
  "85:39": "So it's not necessarily forbidding someone from saying something but just totally, totally",
  "85:44": "just drowning them out with the massive quantity of text and information and comments.",
  "85:53": "And AI can really facilitate that, and so I do not have a good solution to that.",
  "85:58": "In terms of AI learning from AI text, I mean, I think you're gonna get systems that are",
  "86:07": "potentially less and less relevant to humans, and may have harmful effects if they're being",
  "86:15": "used to create software that is interacting with or impacting humans, so that's a concern.",
  "86:23": "I mean one of the things I find fascinating about this is: we could get to a point where",
  "86:31": "99.99% of tweets and fastai forum posts, and whatever, are auto-generated.",
  "86:38": "Particularly on more like political-type places where a lot of it's pretty low content, pretty",
  "86:43": "basic.",
  "86:44": "The thing is, like if it was actually good: you wouldn't even know!",
  "86:50": "So what if I told you that 75% of the people you're talking to on the forum right now are",
  "86:55": "actually bots?",
  "86:56": "How can you tell which ones they are?",
  "86:58": "How would you prove whether I'm right or wrong?",
  "87:01": "Yeah, I think this is a real issue on Twitter.",
  "87:05": "Particularly people you don't know of, wondering like is this an actual person or a bot?",
  "87:11": "I think it's a common question people wonder about and can be hard to tell.",
  "87:19": "But, I think it has significance for- has a lot of significance for- kind of how human",
  "87:28": "government works, you know.",
  "87:29": "I think there's something about humans being in society and having norms and rules and",
  "87:35": "mechanisms that this can really undermine and make difficult.",
  "87:45": "So, when GPT2 came out, Jeremy Howard, co-founder of fastai, was quoted in the Verge article",
  "87:57": "on it, \u201cI've been trying to warn people about this for a while.",
  "88:00": "We have the technology to totally fill twitter, email, and the web up with reasonable sounding,",
  "88:05": "context appropriate prose, which would drown out all other speech and be impossible to",
  "88:09": "filter.\u201d",
  "88:10": "So, one kind of step towards addressing this is the need for digital signatures.",
  "88:24": "Oren Etzioni, the head of the Allen Institute on AI, wrote about this in HBR.",
  "88:30": "He wrote, \u201cRecent developments in AI point to an age where forgery of documents, pictures,",
  "88:36": "audio recordings, videos, and online identities will occur with unprecedented ease.",
  "88:40": "AI is poised to make high fidelity forgery inexpensive and automated, leading to potentially",
  "88:46": "disastrous consequences for democracy, security, and society.\u201d",
  "88:50": "and proposes kind of digital signatures as a means for authentication.",
  "88:55": "And, I will say here kind of one of the additional risks of kind of all this forgery and fakes",
  "89:05": "is that it also undermines people speaking the truth.",
  "89:09": "And, Zeynep Tufekci, who does a lot of research on protests around the world and in different",
  "89:16": "social movements, has said that she's often approached by kind of whistleblowers and dissidents",
  "89:21": "who in many cases will risk their lives to try to publicize like a wrongdoing or human",
  "89:27": "rights violation only to have kind of bad actors say, \u201coh, that picture was photoshopped-",
  "89:34": "that was faked\u201d and that it's kind of now this big issue for for whistleblowers and",
  "89:38": "dissidents of how can they verify what they are saying and that kind of need for verification.",
  "89:47": "And then someone you should definitely be following on this topic is Renee DiResta and",
  "89:54": "she wrote a great article with Mike Godwin last year framing that we really need to think",
  "89:58": "disinformation as a cybersecurity problem, you know.",
  "90:01": "It sees kind of coordinated campaigns of manipulation and bad actors and there's, I think, some",
  "90:09": "important work happening at Stanford, as well on this.",
  "90:14": "Alright, questions on disinformation?",
  "90:19": "Okay, so our next step Ethical Foundations.",
  "90:24": "So, now the fastai approach we always like to kind of ground everything in what are the",
  "90:29": "real-world case studies before we get to kind of the theory underpinning it- and I'm not",
  "90:34": "going to go too deep on this at all.",
  "90:37": "And, so there is a fun article, \u201cwhat would an Avenger do?\u201d",
  "90:41": "And, hat tip to Casey Fiesler for suggesting this.",
  "90:46": "And, it goes through kind of three common ethical philosophies: Utilitarianism and gives",
  "90:55": "the example of Iron Man.",
  "90:57": "I'm trying to match good Deontological Ethics of Captain America being an example of this",
  "91:03": "adhering to the right.",
  "91:04": "And, then Virtue Ethics, Thor living by a code of honor and so I thought that was a",
  "91:09": "nice reading.",
  "91:10": "Rachel: Yes?",
  "91:12": "Question: Where do you stand on the argument that social media companies are just neutral",
  "91:20": "platforms and that problematic content is the entire responsibility of the users just",
  "91:25": "the same way that phone companies aren't held responsible when phones are used for scams",
  "91:29": "or car companies held responsible when vehicles are used for, say, terrorist attacks?",
  "91:34": "Rachel: So, I do not think that the platforms are neutral because they make a number of",
  "91:44": "design decisions and enforcement decisions around even kind of what their Terms of Service",
  "91:49": "are and how those are enforced.",
  "91:52": "And, keeping in mind harassment can drive many people off of platforms and so kind of",
  "92:02": "many of those decisions is not that \u201cOh, everybody gets to keep free speech when there's",
  "92:07": "no enforcement\u201d.",
  "92:08": "It's just changing kind of who is silenced.",
  "92:13": "I do think that there are a lot of really difficult questions that are raised about",
  "92:19": "this because I also think that the platforms, you know, they're not publishers.",
  "92:27": "But, they are in this I think kind of a intermediate area where they are performing many of the",
  "92:36": "functions that publishers used to perform.",
  "92:38": "So, you know like a newspaper would be, which is curating which articles are in it, which",
  "92:43": "is not what platforms are doing, but they are getting closer closer to that.",
  "92:51": "I mean, something I come back to is that it is an uncomfortable amount of power for private",
  "92:58": "companies to have.",
  "92:59": "Yeah, and so it does raise a lot of difficult decisions",
  "93:03": "But I, I do not believe that they are they are neutral.",
  "93:06": "So, for for this part I mentioned the Markkula Center earlier, definitely check out their",
  "93:12": "site, Ethics in Technology Practice.",
  "93:14": "They have a lot of useful, useful resources.",
  "93:19": "And I'm gonna go through these relatively quickly as just kind of examples.",
  "93:23": "So they give some kind of deontological questions that technologists could ask.",
  "93:28": "And so deontological ethics or where you kind of have various kind of rights or duties that",
  "93:36": "you might want to respect.",
  "93:37": "And this can include principles like privacy or autonomy.",
  "93:45": "How might the dignity and autonomy of each stakeholder be impacted by this project?",
  "93:50": "What considerations of trust and of justice are relevant?",
  "93:54": "Does this project involve any conflicting moral duties to others?",
  "93:59": "In some cases, you know, there'll be a kind of conflict between different, different rights",
  "94:05": "or duties you're considering.",
  "94:06": "And so this is, this is kind of an example, and they have more, more in the reading of",
  "94:10": "the types of questions you could be asking, kind of when evaluating, of just, even how",
  "94:15": "do you evaluate, kind of whether, whether a project is ethical.",
  "94:22": "Consequentialist questions -- who will be directly affected, who will be indirectly",
  "94:26": "affected?",
  "94:28": "Will the (and consequentialist includes utilitarianism as well as common good), will the effects",
  "94:35": "in aggregate create more good than harm, and what types of good and harm?",
  "94:40": "Are you thinking about all the relevant types of harm and benefits including psychological,",
  "94:44": "political, environmental, moral, cognitive, emotional, institutional, cultural?",
  "94:49": "Also looking at long term, long term benefits and harms, and then who experiences them?",
  "94:55": "Is this something where the risk of the harm are going to fall disproportionately on the",
  "94:59": "least powerful?",
  "95:01": "Who's going to be the ones to accrue the benefits?",
  "95:04": "Have you considered dual use?",
  "95:06": "And so these are, these are again kind of questions you could use when trying to, trying",
  "95:10": "to evaluate a project.",
  "95:12": "And I think, and the recommendation of the Markkula Center is that this is a great activity",
  "95:16": "to kind of to be doing as a team and as a group.",
  "95:20": "Yes?",
  "95:21": "I was gonna say like I can't, I can't overstate how useful this tool is.",
  "95:32": "Like it, you know, I think, \u201cOh, it's just a list of questions.\u201d",
  "95:35": "Yeah, you know, but like this is kind of to me, this is that this is the big gun tool",
  "95:40": "for for how you, how you handle this.",
  "95:43": "It\u2019s like if somebody is helping you think about the right set of questions and then",
  "95:46": "you let go through them with a diverse group of people and discuss the questions.",
  "95:51": "I mean that\u2019s, this is, this is gold.",
  "95:54": "Like don't, you know, go back and reread these.",
  "95:56": "And don't, don't just skip over them.",
  "95:57": "Take them to work.",
  "96:00": "Use them next time you're talking about a project.",
  "96:01": "They're a really great, great set of questions to use.",
  "96:05": "A great tool in your toolbox.",
  "96:07": "And go to the original reading, has even kind of more detail and more elaboration on the",
  "96:15": "questions.",
  "96:17": "And then they kind of give a summary of five potential ethical lenses.",
  "96:21": "The, the rights approach -- which option in best respects the rights of all who have a",
  "96:27": "stake?",
  "96:28": "The justice approach -- which option treats people equally or proportionally?",
  "96:31": "And so these two are both deontological.",
  "96:34": "The utilitarian approach -- which option will produce the most good and do the least harm?",
  "96:39": "The common good approach -- which option best serves the community as a whole, not just",
  "96:42": "some members?",
  "96:44": "And so here three and four are both a consequentialist.",
  "96:49": "And then -- virtue approach, which option leads me to act as the sort of person I want",
  "96:53": "to be and that can involve, you know particular virtues of you know, do you value trustworthiness",
  "96:59": "or truth or courage?",
  "97:02": "And so I mean a great activity if this is something that you're studying or talking",
  "97:07": "about at work with your teammates, the, the Markkula Center has a number of case studies",
  "97:12": "that you can talk through and will even ask you to kind of evaluate them, you know evaluate",
  "97:16": "them through these five lenses.",
  "97:17": "And how does that kind of impact your, your take on what the what the right thing to do",
  "97:22": "is.",
  "97:26": "It's kind of weird for a programmer or a computer programmer or data scientist in some way in",
  "97:30": "some ways to like think of these as tools like fast.ai or pandas, or whatever, but I",
  "97:37": "mean they absolutely are.",
  "97:38": "This is like, these are like software tools for your brain, you know, to help you kind",
  "97:44": "of go through a program that might help you debug your thinking.",
  "97:51": "Great.",
  "97:52": "Thank you.",
  "97:55": "And then as someone brought up earlier, so that was a kind of very Western centric intro",
  "97:58": "to ethical philosophy there are other ethical lenses and other cultures.",
  "98:04": "And I've been doing some reading particularly on the, the Maori worldview.",
  "98:09": "I don't feel confident enough in my understanding that I could represent it, but it is very",
  "98:15": "good to be mindful that there are other other ethical lenses out there",
  "98:18": "and I do very much think that, you know, the people being impacted by a technology like,",
  "98:24": "their, their ethical lens is kind of what matters.",
  "98:26": "And that, this is, is a particular issue and we have so many kind of multinational corporations.",
  "98:33": "There's an interesting project going on in New Zealand now where the New Zealand government",
  "98:38": "is kind of considering its AI approach and is at least ostensibly kind of wanting to,",
  "98:43": "wanting to include the Maori view on that.",
  "98:48": "So that's a that's kind of a little, a little bit of theory.",
  "98:50": "But now I want to talk about some kind of practices you can implement in the workplace.",
  "98:55": "Again, this is from the Markkula Center.",
  "98:57": "So this is their ethics toolkit, which I particularly like.",
  "99:02": "And I'm just, I'm not going to go through all of them, I'm just going to tell you a",
  "99:04": "few of my favorites.",
  "99:06": "So, Tool 1 is Ethical Risk Sweeping and this I think is similar to the idea of, kind of",
  "99:11": "pen testing (that Jeremy mentioned earlier from security), but to have regularly-scheduled",
  "99:18": "ethical risk sweeps.",
  "99:21": "And while no vulnerability, vulnerabilities found is generally good news, that doesn't",
  "99:27": "mean that it was a wasted effort.",
  "99:29": "And you keep doing it, keep looking for, for ethical risk.",
  "99:34": "One moment.",
  "99:35": "And then assume that you missed some risk in the initial project development.",
  "99:39": "Also, you have to set up the incentives properly where you're rewarding team members for spotting",
  "99:43": "new ethical risk.",
  "99:45": "All right.",
  "99:46": "So got some comments here.",
  "99:50": "So my comment here is about the learning rate finder, and I'm not going to bother with the",
  "99:54": "exact mathematical definition (partly because I'm a terrible mathematician and partly because",
  "99:58": "it doesn't matter), but if you just remember, oh, sorry, that's actually not me, I am just",
  "100:02": "reading something that Patty Hendrix has trained a language model of me.",
  "100:08": "So that was me greeting the language model of me, not the real me.",
  "100:15": "Thank you.",
  "100:17": "This is a Tool 1.",
  "100:18": "I would say another kind of example of this, I think it's like red teaming of, you know,",
  "100:22": "having a team within your org that\u2019s kind of trying to find your vulnerabilities.",
  "100:28": "Tool 3, another one I really like, Expanding the Ethical Circle.",
  "100:34": "So whose interests, desires, skills, experiences and values have we just assumed rather than",
  "100:40": "actually consulted?",
  "100:41": "Who are all the stakeholders who will be directly affected?",
  "100:45": "And have we actually asked them what their interests are?",
  "100:49": "Who might use this product that we didn't expect to use it, or for purposes that we",
  "100:54": "didn't initially intend?",
  "100:56": "And so then a great implementation of this comes from the University of Washington's",
  "101:03": "Tech Policy Lab did a project called Diverse Voices.",
  "101:09": "And it's neat, they have both a academic paper on it and then they also kind of have like",
  "101:15": "a guide, lengthy guide on how you would implement this.",
  "101:18": "But the idea is how to kind of organize expert panels around, around new technology, and",
  "101:28": "so they, they did a few samples.",
  "101:30": "One was they were considering augmented reality and they held expert panels with people with",
  "101:36": "disabilities, people who are formerly are currently incarcerated, and with women to",
  "101:40": "get their get their input and make sure that that was included.",
  "101:43": "They did a second one on an autonomous vehicle strategy document and organized expert panels",
  "101:48": "with youth, with people that don't drive cars, and with extremely low-income people.",
  "101:53": "And so I think this is a great guide if you're kind of unsure of how do you even go about",
  "101:59": "setting something like this up to expand your circle, include, include more people and get,",
  "102:06": "get perspectives that may be underrepresented by your employees.",
  "102:11": "So I just want to let you know that this resource is out there.",
  "102:18": "Tool 6 is Think About the Terrible People.",
  "102:21": "And, and this can be hard because I think we're often, you know, thinking kind of positively,",
  "102:29": "or thinking about people like ourselves who don't have terrible intentions.",
  "102:33": "But really, think about who might want to abuse, steal, misinterpret, hack, destroy,",
  "102:39": "or weaponize what we build?",
  "102:41": "Who will use it with alarming stupidity or irrationality?",
  "102:47": "What rewards, incentives, openings, has our design inadvertently created for those people?",
  "102:51": "And so kind of remembering back to the section on metrics, you know, how are people going",
  "102:55": "to be trying to game or manipulate this?",
  "102:59": "And how can how can we then remove those rewards or incentives?",
  "103:02": "And so this is, this is an important kind of important step to take.",
  "103:09": "And then, Tool 7 is Closing the Loop, Ethical",
  "103:12": "Feedback and Iteration, remembering this is never a finished task and identifying feedback",
  "103:18": "channels that will give you kind of reliable data and integrating this process with quality",
  "103:24": "management and user support and developing formal procedures and chains of responsibility",
  "103:29": "for ethical iteration.",
  "103:31": "And this tool reminded me of a blog post by Alex Feerst that I really like.",
  "103:36": "Alex Feerst was previously the Chief Legal Officer at Medium.",
  "103:40": "And, I guess this was a year ago, he interviewed or something like 15 or 20 people that have",
  "103:47": "worked in trust and safety.",
  "103:49": "And trust and safety includes content moderation, although it's not not solely content moderation.",
  "103:55": "And kind of one of the ideas that came up that I really liked was one of, one of the",
  "104:00": "people .. and so many of these people have worked in Trust and Safety for years at big-name",
  "104:04": "companies and one of them said, \u201cThe separation of product people and trust people worries",
  "104:09": "me, because in a world where product managers and engineers and visionaries cared about",
  "104:14": "the stuff, it would be baked into how things get built.",
  "104:17": "If things stay this way -- that product and engineering are Mozart and everyone else is",
  "104:20": "Alfred the butler -- the big stuff is not going to change.\u201d",
  "104:24": "And so, I think at least two people in this kind of talk about this idea of needing to",
  "104:29": "better integrate trust and safety, which are often kind of on the front lines of seeing",
  "104:33": "abuse and misuse of a technology product.",
  "104:36": "Integrating that more closely with product and engs so that it can kind of be more directly",
  "104:41": "incorporated and you can have a tighter feedback loop there -- about what's going wrong, and",
  "104:47": "and how, how that can be designed against.",
  "104:50": "Okay, so those were, these were, well, I link to a few blog posts and research I thought",
  "104:57": "relevant, but inspired by the Markkula Center\u2019s tools for tech ethics and hopefully those",
  "105:03": "are practices you could think about potentially implementing at your, at your company.",
  "105:07": "So next I want to get into diversity, which I know came up earlier.",
  "105:16": "And so only 12% of Machine Learning researchers are women.",
  "105:22": "This is kind of a very, very dire statistic.",
  "105:26": "There's also a kind of extreme lack of racial diversity and age diversity and other factors,",
  "105:33": "and this is, this is significant.",
  "105:37": "A kind of positive example of what diversity can help with, and a post Tracy Chou, who",
  "105:43": "was an early, early an engineer at Quora and later at Pinterest, wrote that the first feature",
  "105:50": "(and so I think she was like one of the first five employees at Quora), \u201cThe first feature",
  "105:54": "I built when I worked at Quora was the block button.",
  "105:57": "I was eager to work on the feature because I personally felt antagonized and abused on",
  "106:00": "the site,\u201d and she goes on to say that If she hadn't been there, you know they might",
  "106:05": "not have added the the block button as soon.",
  "106:07": "And so that's kind of like a direct example of how, how having a diverse team can help.",
  "106:12": "So my kind of key, key advice for anyone wanting to increase diversity is to start at the opposite",
  "106:20": "end of the pipeline from, from where people talk about the, the workplace.",
  "106:25": "I wrote a blog post five years ago, \u201cIf you think women in tech is just a pipeline",
  "106:30": "problem, you haven't been paying attention.\u201d",
  "106:32": "And this was the most popular thing I had ever written until Jeremy and I wrote the,",
  "106:35": "the Covid 19 post last month, so the second most, most popular thing I've written.",
  "106:41": "But I linked to a ton of, ton of research in there.",
  "106:45": "A key statistic to understand is that 41% of women working in tech end up leaving the",
  "106:52": "field, compared to 17% of men.",
  "106:54": "And so this is something that recruiting more girls into, into coding or tech is not going",
  "107:00": "to address this problem, if they keep leaving at very high rates.",
  "107:03": "I just had a little peek at the YouTube chat, and I see people are asking questions there.",
  "107:10": "I just want to remind people that we are not, that Rachel and I do not look at that.",
  "107:15": "If you want to ask, ask questions, you should use the forum thread and, and if you see questions",
  "107:21": "that you like, then please put them up, such as this one, \u201cHow about an ethical issue",
  "107:29": "bounty program just like the bug bounty programs that some companies have?\u201d",
  "107:34": "You know, I think that's a neat idea you have, rewarding people for, for finding ethical",
  "107:39": "issues.",
  "107:43": "And so the, the reason that women are more likely to leave tech Is and this was found",
  "107:49": "in a meta-analysis of over 200 books, white papers articles -- women leave the tech industry",
  "107:55": "because they're treated unfairly, underpaid, less likely to be fast-tracked than their",
  "107:59": "male colleagues and unable to advance.",
  "108:03": "And, and too often, diversity efforts end up just focusing on white women, which is",
  "108:10": "wrong.",
  "108:11": "Interviews with 60 women of color who work in stem research found that 100 percent had",
  "108:15": "experienced discrimination and their particular stereotypes varied by race.",
  "108:19": "And so it's very important to focus on women of color in, in diversity efforts as a, kind",
  "108:25": "of the top priority.",
  "108:29": "A study found that men's voices are perceived as more persuasive, fact-based, and logical",
  "108:36": "than women's voices, even when reading identical scripts Researchers found that women receive",
  "108:43": "more of a feedback and personality criticism and performance evaluations, whereas men are",
  "108:47": "more likely to receive actionable advice tied to concrete business outcomes.",
  "108:53": "When women receive mentorship, it\u2019s often advice on how they should change and gain",
  "108:57": "more self-knowledge.",
  "108:58": "When men receive mentorship, it's public endorsement of their authority.",
  "109:01": "Only one of these has been statistically linked to getting promoted, it's the public endorsement",
  "109:06": "of authority.",
  "109:08": "And all these studies are linked to another post I wrote called, \u201cThe Real Reason Women",
  "109:11": "Quit Tech\u201d, and how to address it.",
  "109:13": "Is that a question, Jeremy?",
  "109:17": "Yeah, so if you're interested, kind of, these two blog posts I link to a ton of ton of relevant",
  "109:23": "research on this.",
  "109:24": "And I think this is kind of the the workplace, is the the place to start in addressing these",
  "109:28": "things.",
  "109:32": "So another issue is tech interviews are terrible for everyone.",
  "109:38": "So now kind of working one step back from people that are already in your in your workplace,",
  "109:42": "but thinking about the interview process.",
  "109:44": "And they wrote a post on how to make tech interviews a little less awful, and went through",
  "109:50": "a ton of research.",
  "109:51": "And I will, I will say that the the interview problem, I think is a hard one.",
  "109:53": "I think it's very time consuming and hard to to interview people well.",
  "109:59": "But kind of the two most interesting pieces of research I came across -- one was from",
  "110:05": "Triplebyte, which is a recruiting company that interviews, kind of does this first-round",
  "110:14": "technical interview for people and then they interview at Y Combinator (it's a Y Combinator",
  "110:20": "company), and they interview at Y Combinator companies.",
  "110:23": "And they have this very interesting data set where they've kind of given everybody the",
  "110:26": "same technical interview and then they can see which companies people got offers from",
  "110:31": "when they were, you know interviewing at many of the same companies.",
  "110:34": "And the number one finding from their research is that the types of programmers that each",
  "110:38": "company looks for often have little to do with what the company needs or does, rather",
  "110:44": "they reflect company culture and the backgrounds of the founders And this is something where",
  "110:48": "they even, they even gave the advice that if you're job hunting, look for, try to look",
  "110:54": "for companies where the founders have a similar background to you.",
  "110:58": "And that's something that while I, that makes sense that's going to be much easier for certain",
  "111:05": "people to do than others.",
  "111:06": "And particularly, given the, the gender and racial disparities in VC funding, that's gonna",
  "111:11": "make a big difference.",
  "111:12": "Yes.",
  "111:13": "Actually, I would say that was the most common advice I heard from VCs when I became a founder",
  "111:20": "in the Bay Area was when recruiting focus on getting people from your network, and people",
  "111:28": "that are as like-minded and similar as possible.",
  "111:31": "That was by far the most common advice that I heard.",
  "111:34": "Yeah, I mean this is one of my controversial opinions -- I do feel like ultimately, like",
  "111:42": "I get why people hire from their network, and I think that long term, we all need to",
  "111:47": "be developed.",
  "111:48": "Well particularly white people need to be developing more diverse networks.",
  "111:50": "And that's like a, you know, like ten-year project.",
  "111:54": "That's not something you can do right when you're hiring, but really kind of developing",
  "111:59": "a diverse network of friends and trusted acquaintances, a kind of over time.",
  "112:06": "But, yeah, thank you for that perspective to Jeremy.",
  "112:10": "And then kind of the other study.",
  "112:12": "I found really interesting, was one where they, they gave people resumes.",
  "112:18": "And in one case, so one resume had more academic qualifications, and then one had more practical",
  "112:24": "experience.",
  "112:25": "And then they switched the gender -- one was a woman, one was a man (or you know male name",
  "112:30": "and a female name).",
  "112:31": "And basically people were more likely to hire the male and then they would use a post-hoc",
  "112:35": "justification of, \u201cOh, well, I chose him because he had more academic experience, or",
  "112:39": "I chose him because he had more practical experience.\u201d",
  "112:43": "And that's something, I think it's very human to use post hoc justifications, but it's a,",
  "112:48": "it's a real risk that definitely shows up in hiring.",
  "112:53": "Ultimately, AI, or any other technology I developed or implemented by companies for",
  "113:00": "financial advantage, i.e., more profit, maybe the best way to incentivize ethical behavior",
  "113:05": "is to tie financial or reputational risk to good behavior.",
  "113:09": "In some ways, similar to how companies are now investing in cybersecurity because they",
  "113:13": "don't want to be the next Equifax.",
  "113:15": "Can grassroots campaigns help in better ethical behavior with regards to the use of AI.",
  "113:21": "Oh, that's a good question.",
  "113:22": "Yeah, and I think there are a lot of analogies with cybersecurity and I know that for a long",
  "113:27": "time I think was hard for people to make, or people had trouble making the case to their",
  "113:33": "bosses of why they should be investing in cybersecurity.",
  "113:35": "Particularly because cybersecurity is you know, something like when it's working, well,",
  "113:40": "you don't notice it.",
  "113:42": "And so that can be, can be hard to build the case.",
  "113:45": "So I think that there, there is a place for grassroots campaigns.",
  "113:50": "And I'm gonna talk more, I'm gonna talk about policy in a bit.",
  "113:54": "It can be hard in in some of these cases where there are not necessarily meaningful alternatives",
  "114:01": "So I do think like monopolies can kind of, kind of make that harder.",
  "114:09": "That's it, yeah, a good good question.",
  "114:15": "All right, so next step...",
  "114:20": "Actually on this slide is the, the need for policy.",
  "114:25": "And so I'm gonna start with a case study of what, what's one thing that gets companies",
  "114:30": "to take action?",
  "114:32": "And so as I mentioned earlier, an investigator for the UN found that Facebook played a determining",
  "114:38": "role in the Rohingya genocide.",
  "114:40": "I think the best article I've read on this was by Timothy McLaughlin, who did a super",
  "114:46": "super in-depth dive on Facebook's role in Myanmar.",
  "114:52": "And people people warned Facebook executives in 2013, and in 2014, and in 2015, how the",
  "115:00": "platform was being used to spread hate speech and to incite violence.",
  "115:05": "One person in 2015 even told Facebook executives that Facebook could play the same role in",
  "115:12": "Myanmar that the radio broadcast played during the Rwandan genocide, and radio broadcasts",
  "115:17": "played a very terrible and kind of pivotal role in the Rwandan genocide.",
  "115:22": "Somebody close to it, this said that's not 20/20 hindsight, the scale of this problem",
  "115:27": "was significant and it was already apparent.",
  "115:29": "And despite this in 2015, I believe Facebook only had four contractors who even spoke Burmese,",
  "115:37": "the language of the of Myanmar.",
  "115:39": "Question?",
  "115:40": "That's an interesting one.",
  "115:42": "How do you think about our opportunity to correct biases in artificial systems, versus",
  "115:48": "the behaviors we see in humans?",
  "115:50": "For example, a sentencing algorithm can be monitored and adjusted, versus a specific",
  "115:55": "biased judge who remains in their role for a long time.",
  "116:03": "I mean well, theoretically, though \u2026 I think I feel a bit hesitant about the it's, it'll",
  "116:10": "be easier to correct bias in algorithms because I feel like the \u2026 you still need people",
  "116:21": "kind of making the decisions to prioritize that.",
  "116:25": "Like, it requires kind of an overhaul of the systems priorities, I think.",
  "116:33": "It also starts with the premise that there are people who can't be fired, or disciplined,",
  "116:40": "or whatever.",
  "116:41": "Which I guess, maybe for some judges that's true, but tha kind of maybe suggests that",
  "116:45": "judges shouldn't be lifetime appointments.",
  "116:50": "Yeah, even then I think you kind of need the, the change of heart of the people advocating",
  "116:56": "for the new system, which I think can, would be necessary in either case, kind of.",
  "117:02": "And that, that's kind of the critical piece of getting the, the people that are wanting",
  "117:07": "to overhaul the values of a system.",
  "117:10": "So returning to, to this issue of the Rohinga genocide.",
  "117:16": "And this is a kind of continuing, continuing issue.",
  "117:22": "Yeah, this is something that's just kind of really stunning to me that, that there were",
  "117:29": "so many warnings, and that so many people tried to raise an alarm on this, and that",
  "117:33": "so little action was taken.",
  "117:36": "And even, this was last year, Zuckerberg finally said that Facebook would add, or maybe, maybe",
  "117:44": "this was actually, this was probably two years ago, said that Facebook would add, but this",
  "117:47": "is you know after genocides already happening, Facebook would add dozens of Burmese language",
  "117:54": "content reviewers.",
  "117:57": "So in contrast, wo we have this, this is how Facebook really failed to respond in any any",
  "118:03": "significant way in Myanmar, Germany passed a much stricter law about hate speech and",
  "118:11": "NetzDG, and the the potential penalty would be up to like 50 million euros.",
  "118:22": "Facebook hired 1200 people in under a year because they were so worried about this penalty.",
  "118:28": "And so, and I'm not saying like this is a law we want to replicate here.",
  "118:32": "I'm just illustrating the difference between being told that you're contributing, or playing",
  "118:37": "a determining role in a genocide versus a significant financial penalty.",
  "118:43": "We have seen what the one thing that makes Facebook take action is.",
  "118:48": "And so I think that that is really significant in remembering what the, what the power of",
  "118:54": "a credible threat of a significant fine is.",
  "118:58": "And it has to be a lot more than you know, just like a cost of doing business",
  "119:01": "So, I really believe that we need both policy and ethical behavior within industry.",
  "119:12": "I think that policy is the appropriate tool for addressing negative externalities, misaligned",
  "119:17": "economic incentives, race to the bottom situations, and enforcing accountability.",
  "119:23": "However, ethical behavior of Individuals, and of data scientists, and software engineers",
  "119:28": "working in industry, is very much necessary as well.",
  "119:31": "Because, the law is not always going to keep up.",
  "119:33": "It's not going to cover all the edge cases.",
  "119:35": "We really need the people in industry to be making kind of ethical ethical decisions as",
  "119:40": "well.",
  "119:41": "And so I believe both are significant and important.",
  "119:47": "And then, something to note here is that, many, many examples of kind of AI ethics issues,",
  "119:55": "and I haven't talked about all of these, but there was Amazon's facial recognition the",
  "120:01": "ACLU did a study finding that it incorrectly matched 28 members of Congress to criminal",
  "120:06": "mug shots and this disproportionately included Congress people of color.",
  "120:12": "And there's also, this was a terrible article.",
  "120:15": "The article was good, but the story is terrible, of a City that's using this IBM dashboard",
  "120:22": "for predictive policing and a city official said, oh like whenever you have machine learning",
  "120:26": "it's always 99% accurate, which is false, and quite concerning.",
  "120:33": "We had we had the issue in, so in 2016, ProPublica discovered that you could place a housing",
  "120:41": "ad on Facebook and say \u201cI don't want Latino or black people\u201d, or \u201cI don't want wheelchair",
  "120:46": "users to see this housing ad\u201d, which seems like a violation of the the Fair Housing Act.",
  "120:52": "And so there's this article, and Facebook was like we're so sorry, and then over a year",
  "120:57": "later It was still going and ProPublica went back and wrote another article about it.",
  "121:03": "There's also this issue of dozens of companies were Placing ads on facebook, job ads and",
  "121:08": "saying, \u201cWe only want young people to see this\u201d.",
  "121:14": "There's Amazon creating the recruiting tool that penalized resumes that had the word \u201cwomen's\u201d",
  "121:21": "in it.",
  "121:22": "And so something to note about these examples, and many of the examples we've talked about",
  "121:26": "today, is that many of these are about human rights and civil rights.",
  "121:31": "It's a good article by Dominique Harrison of the Aspen Institute on this.",
  "121:37": "And I kind of agree with Anil Dash\u2019s framing.",
  "121:41": "I mean, he wrote, \u201cThere is no technology industry anymore, Tech is being used in every",
  "121:47": "industry\u201d.",
  "121:48": "And so I think in particular, we need to consider human rights and civil rights such as housing,",
  "121:54": "education, employment, criminal justice, voting, and medical care, and think about what rights",
  "122:00": "we want to safeguard and I do think policy is the appropriate way to do that.",
  "122:06": "And I think, I mean, it's very easy to be discouraged about regulation, but I think",
  "122:13": "sometimes we overlook the positive, or the cases where it's worked well.",
  "122:20": "And so something I really liked about datasheets for data sets by Timnit Gebru et al., is that",
  "122:26": "they go through three case studies of how standardization and regulation came to different",
  "122:32": "industries.",
  "122:35": "And so the electronics industry, around circuits and resistors, and so there that's kind of",
  "122:39": "around the standardization, of you know, what the specs are and what you write down about",
  "122:42": "them.",
  "122:43": "And the pharmaceutical industry, and car safety, and none of these are perfect, but it's still,",
  "122:48": "it was a kind of, very illuminating, the case studies there.",
  "122:51": "I mean, in particular I got very interested in the car safety one, and there's also a",
  "122:56": "great 99% invisible episode, this is a design podcast about it.",
  "123:00": "And so some things I learned is that early cars had sharp metal knobs on the dashboard",
  "123:09": "that could lodge in people's skulls in a crash.",
  "123:12": "Non-collapsible steering columns would frequently impale drivers and then even after the collapsible",
  "123:17": "steering column was invented, it wasn't actually implemented because there was no economic",
  "123:21": "incentive to do so.",
  "123:23": "But it's the collapsible steering column that has saved more lives than anything other than",
  "123:29": "the seatbelt, when it comes to car safety.",
  "123:32": "And there was also this widespread belief that cars were dangerous because of the people",
  "123:37": "driving them and it took, it took consumer safety advocates decades to just even change",
  "123:42": "the culture of discussion around this and to start kind of gathering and tracking the",
  "123:46": "data and to put more of an onus on car companies around safety.",
  "123:53": "GM hired a private detective to trail Ralph Nader and try to dig up dirt on him.",
  "123:58": "And so this was really a battle that we kind of, I take for granted now, and and so kind",
  "124:06": "of shows how much how much it can take to to change, change the needle there.",
  "124:12": "And then, kind of a more recent Issue is that it wasn't until I believe 2011 that it was",
  "124:19": "required that crash test dummies start representing the average female anatomy, in addition to",
  "124:25": "\u2026. previously was kind of just crash test dummies were just like men.",
  "124:30": "And that in a crash with the same impact, women were 40% more likely to be injured than",
  "124:37": "men, because that's kind of who the cars were being designed for.",
  "124:40": "So I thought, I thought all this was very interesting and it can be helpful to kind",
  "124:42": "of remember, and remember some of the successes we've had.",
  "124:47": "And another area that's very relevant is environmental protections.",
  "124:53": "And kind of looking back and Maciej Ceglowski has a great article on this.",
  "125:00": "But you know just remembering like in the US, we used to have rivers that would catch",
  "125:04": "on fire, and London had terrible terrible smog, and that these are things that were,",
  "125:08": "you know were very, would not have been possible to kind of solve as an individual.",
  "125:12": "We really needed kind of coordinated, coordinated regulation.",
  "125:14": "All right, is then on a kind of closing note.",
  "125:21": "So I think a lot of the problems I've touched on tonight are really huge, huge and difficult",
  "125:29": "problems and they're often kind of very complicated and I .. Well, I go into more detail on this",
  "125:35": "in the course, so please, please check out the course once it's released.",
  "125:39": "I always try to offer some like steps towards solutions, but I realize they're not they're",
  "125:44": "not always.",
  "125:45": "you know.",
  "125:46": "as satisfying as I would like of like this is gonna solve it and that's cuz these are",
  "125:50": "really really difficult problems.",
  "125:52": "And Julia Angwin, a former journalist from ProPublica, and now the editor in chief of",
  "125:59": "The Markup, gave a really great interview on privacy last year that I liked and found",
  "126:06": "very encouraging.",
  "126:07": "She said, \u201cI strongly believe that in order to solve a problem, you have to diagnose it",
  "126:12": "and that we're still in the diagnosis phase of this.",
  "126:16": "If you think about the turn of the century and industrialization, we had, I don't know,",
  "126:21": "30 years of child labor, unlimited work hours, terrible working conditions, and it took a",
  "126:26": "lot of journalists muckraking and advocacy to diagnose the problem, and have some understanding",
  "126:31": "of what it was and then the activism to get laws changed.",
  "126:38": "I see my role as trying to make as clear as possible what the downsides are and diagnosing",
  "126:43": "them really accurately so that they can be solvable.",
  "126:46": "That's hard work and lots more people need to be doing it.\u201d",
  "126:48": "I found that really encouraging and that I do, I do think we should be working towards",
  "126:54": "solutions.",
  "126:55": "But I think just at this point, even better, diagnosing and understanding kind of the complex",
  "126:58": "problems we're facing is valuable work.",
  "127:02": "A couple of people are very keen to see your full course on ethics.",
  "127:08": "Is that something that they might be able to attend or buy or something?",
  "127:13": "So it will be released for free at some point this summer.",
  "127:18": "And it was, there was a paid in-person version and offered at the Data Institute as a certificate",
  "127:24": "kind of.",
  "127:26": "Similar to how this, this course was supposed to be offered, you know, in person.",
  "127:31": "The data ethics one was in-person, and that took place in January and February.",
  "127:34": "And then I'm currently teaching a version, version for the Masters of Data Science students",
  "127:38": "at USF, and I will be releasing the free online version and later, sometime before July.",
  "127:45": "Thank you.",
  "127:46": "I will see you next time."
}